\documentclass{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{authblk}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{booktabs}

\newcommand{\bbeta}{\boldsymbol\beta}
\newcommand{\bt}{\boldsymbol\tau}
\newcommand{\bx}{\boldsymbol x}
\newcommand{\bta}{\boldsymbol\ta}
\newcommand{\bOmega}{\boldsymbol\Omega}
\newcommand{\bY}{\mathbf{Y}}
\newcommand{\cC}{\mathcal{C}}
\newcommand{\cG}{\mathcal{G}}
\newcommand{\st}{v_{\ta}}
\newcommand{\ta}{\theta}
\newcommand{\lla}{\longleftarrow}
\newcommand{\G}{\mathcal{G}}
\newcommand{\I}{\mathbb{I}}
\newcommand{\Normal}{\mathcal{N}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\bX}{\mathbf{X}}

\title{AMG+: General Principles of Algebraic Multigrid Coarsening, Illustrated on the 1D Helmholtz Equation}
\author{}

% Change ordering to something that makes sense.
\author[1]{Achi Brandt}
\author[2]{Oren Livne}
\affil[1]{The Weizmann Institute of Science,Department of Applied Mathematics \& Computer Science, 76100 Rehovot, Israel. Email: achibr@gmail.com}
\affil[2]{Educational Testing Service, 660 Rosedale Road, Attn: MS-12, T-197, Princeton, NJ 08540. Email: olivne@ets.org}
\author[3]{James Brannick} % Add affiliation
\author[4]{Karsten Khal} % Add affiliation
\date{\today}

\begin{document}
\maketitle

\begin{abstract}
TBA
% Multigrid solvers have been successful in solving numerous scientific and and engineering systems. Typically, a hierarchy of increasingly coarser levels are constructed from the original fine equation, such that each level reduces a certain range of error components, and combined in a multigrid cycle that has an optimal convergence rate. Notwithstanding, optimal multigrid design is still very difficult to design for indefinite problems, highly nonlinear problems, and huge systems where even linear complexity is not good enough. Algebraic multigrid solvers automate the coarsening process, but are still limited in application.

% In this work we develop AMG+, a new Bootstrap Algebraic Multigrid (BAMG) algorithm for constructing an efficient multilevel hierarchy. We focus on developing automatic criteria for deriving the coarse variables (based on local singular value decompositions) and interpolation operators (via regularized least-squares fitting) with as little assumptions as possible about the input system. The approach is demonstrated on the one-dimensional Helmholtz equation on a periodic domain. This model is an example where standard multigrid solvers has been notoriously difficult to develop, as the coarse variables have a completely different character (rays) than fine-level variables (waves). AMG+ produces both near-null space functions, and a multilevel hierarchy of highly accurate coarse levels.

% We discuss future generalizations to the multi-dimensional Helmholtz equation, and  systematic upscaling of highly repetitive non-linear systems arising in fluid flows and molecular dynamics.
\end{abstract}

\section{Introduction}
\label{intro}
Multigrid (MG) solvers have been very successful in solving a wide variety of scientific and engineering problems with optimal efficiency \cite{review}. Typically, a hierarchy of coarse levels is constructed from the original system, and combined in a multigrid cycle to obtain a fast iterative solver. There are several reasons for further generalizing these multigrid methods: (a) it takes expertise and a tedious manual development process to tailor the methods to different \emph{systems} of Partial Differential Equations (PDEs) and \emph{indefinite problems}; (b) for many systems, even attaining linear complexity is not good enough, since the number of variables is huge. Such systems on the other hand are typically \emph{highly repetitive}, in the sense that the same small set of governing equations (or Hamiltonian terms) keep repeating itself throughout the physical domain. (c) The methods do not perform well for \emph{highly nonlinear cases}, where configurations cannot be decomposed into weakly-interacting local and non-local parts.

{\bf Define AMG+ as as AMG with new set of coarsening principles. Discuss in particular AMG+ coarsening = new coarse variable type):
\begin{itemize}
\item Current AMG: coarse variable = a representative/average of several ``strongly-connected'' fine variables.
\item In many problems (non-elliptic, NN, etc.) there are no particularly strong connections.
\item Can coarsen only larger aggregates of moderately connected variables, with several coarse variables per aggregate.
\item Coarse variables are of different type than fine variables.
\end{itemize}
}


\subsection{Example: The Helmholtz Equation}
The Helmholtz equation 
\begin{equation}
	\Delta u(\bx) + k(\bx)^2 u(\bx) = F(\bx)\,,\qquad \bx \in \Omega \subset \R^d\,,
\end{equation}
arises in optics and as a simplified version of the Kohn-Sham equation in electronic structure calculations (with a fixed energy $\lambda$ and potential $V(\bx)$, where $k^2 = \lambda - V$ and $F \equiv 0$). Helmholtz is a good model problem for our purposes, because. the required coarse-level variables have a completely different character (rays/geometric optics) than fine-level variables (waves/electromagnetics). For instance, for constant $k$, slow-to-converge errors in relaxation are approximately a sum of rays in different directions, namely,
\begin{equation}
    \sum_{\alpha_1^2 + \dots \alpha_d^2 = k^2} A_{\alpha_1,\dots,\alpha_d}(\bx) e^{i (\alpha_1 x_1 + \dots \alpha_d x_d)}\,,
    \label{ray_sum}
\end{equation}
where $A_{\alpha_1,\dots,\alpha_d}$ are smooth functions. Creating a multigrid solver that properly approximates Almost Zero Modes (AZMs) and automatically detects the transition from waves to rays, has thus been a notoriously difficult task \cite{wave_ray, later_papers}. Multi-coarsening (multiple coarse grids per fine grid) has been known to be required, but designing the coarsening ``by hand'' using the specific exponential form of the basis functions in (\ref{ray_sum}) basically failed.

In this work we solve the 1D Helmholtz equation, where there are only two rays (the basis functions $e^{i k x}$ and $e^{-i k x}$, as opposed to infinite number of directions in higher dimensions), so we can focus on the task of automatically finding the right type of coarse variables accurate good coarse-level equations without needing to also develop multi-coarsening, which is required in the e2D problem.

Our coarsening rules lead to an algorithm that finds both low-residual ``test functions'' that contain linear combinations of those AZMs, and a multilevel hierarchy that accurately represents them at all levels and can be used for an efficient 1D Helmholtz solver. This setting factors out other difficulties that a numerical solver of the PDE must treat (radiation boundary conditions are necessary; boundary effects; and diffraction effects of domain features of size comparable with the wavelength; phase error), to focus on the derivation of the interior-domain coarsening.

\subsection{Our Contribution}
Our contribution is two-fold. First, we develop general principles multilevel coarsening (Sec.~\ref{rules}). For each multigrid component, we develop a quality measure and a construction method, which allows designing each component separately before they are combined, simplifying the solver design. These rules should help extend AMG to many new problems, including the fast iterative solution of linear algebraic systems; discretized nonlinear problems and systems of PDEs (such as the Navier-Stokes equations); systematic upscaling (coarsening of a repetitive equation on a large domain using local windows); and numerical homogenization, in which the fine levels are never or rarely visited during the simulation.

Second, we develop AMG+, a multilevel algorithm that finds low-residual functions, an accurate multilevel hierarchy, and a fast linear solver, for the 1D Helmholtz equation on a periodic domain. AMG+ is a completely automatic algebraic multigrid construction procedure that does not exploit specific properties of the Helmholtz operator, such as the exponential form the AZMs. It is much simpler than a specifically tailored approach  \cite{wave_ray, later_papers}. In particular, it provides a stepping stone towards a multilevel solver of the multi-dimensional Helmholtz equation. We also discuss how to enhance the method's performance in case of repetitive equations (e.g., constant $k$), where the entire coarsening construction can be performed locally.

\subsection{Related Work}
{\bf
\begin{itemize}
    \item Multilevel Helmholtz solvers \cite{wave_ray, later_papers}.
    \item Bootstrap papers.
    \item Neural networks (with Eran) where the SVD was applied to network compression and inspired the current work.
\end{itemize}
}

The paper is organized as follows. In Sec.~\ref{rules} we summarize the general rules of multilevel coarsening. We then apply them to solving the 1D Helmholtz problem on a fixed periodic domain in in Sec.~\ref{helm_linear}. Each rule's quality measure and construction method are illustrated in a separate sub-section. Finally, we discuss further generalizations and future research directions in Sec.~\ref{future}.

\section{General Rules of Multilevel Coarsening}
\label{rules}
The goal of these rules is to simplify the task of developing multilevel solvers for new problems. For each multigrid component, we develop a quality measure and a construction method, which allows designing each component separately.

%%%% Add a table of principle, quality metric, construction. Predicts arrow.

\begin{enumerate}
    \item {\bf Relaxation and shrinkage.} Relaxation should exhibit a fasts initial residual reduction, starting from a random initial error. This means that relaxation quickly shrinks the information content of the error, allowing it to be approximated by a coarser level after few sweeps. In Sec.~\ref{sec:relaxation} we we introduce the residual {\it shrinkage factor} to measure relaxation efficiency, which generalizes the smoothing factor \cite[Sec.~3.1]{guide} in geometric multigrid.
    \item {\bf Using Test Functions.} Test Functions (TFs) are examples of slowly-converging errors in the current solver. The idea of using examples has arisen in other fields such as neural networks (where they are used during stochastic gradient decent training \cite{sgd}) and in bootstrap multigrid methods \cite{lamg, bamg}, i.e., examples of algebraically smooth errors. During the coarsening process, the set of TFs is gradually improved and enriched via bootstrapping (see the next item). The coarsening is derived {\it locally} from these functions (Sec.~\ref{sec:svd_coarsening}--\ref{sec:interpolation}).
    \item {\bf Local SVD coarsening.} The coarse variables (both their {\it type} and {\it number}) should be derived using a Singular Value Decomposition (SVD) of TF values on small aggregates of fine-level variables. Aggregates can be chosen geometrically, in geometric problems; more generally, they should be sets of fine variables that are highly correlated across all TFs. See Sec.~\ref{svd_coarsening}. The SVD yields {\it tentative} aggregate sizes and coarsening ratios; the actual coarsening is determined by the mock cycle rate (see next item).
    \item {\bf Numerical quantitative performance predictors.} The potential multilevel efficiency can be estimated before the interpolation and coarse equations are derived, using direct numerical predictors that vastly generalize Local Fourier Analysis (LFA) for PDEs. The {\it mock cycle} (also known as habituated compatible relaxation \cite{mg_guide, cr_oren}) asymptotic convergence rate is the ultimate metric of coarsening quality; cf.~\ref{sec:mock_cycle}.
    \item {\bf Interpolation and Coarse-level Operator.} An initial interpolation is simply the transpose of the SVD coarsening operator; in some cases, it may be good enough. More generally, it can be derived {\it locally} by least-squares fitting to TF values; cf. Sec.~\ref{sec:interpolation}. While the fitting accuracy on TFs gives tentative interpolatory sets, the ultimate interpolation quality metric is the convergence rate of the two-level cycle. Only good initial convergence (shrinkage) is required at this stage (Sec.~\ref{sec:two_level}). The coarse-level operator may be post-processed to reduce fill-in and preserve important properties such as symmetry; cf. Sec.~\ref{sec:coarse_operator}.
    \item {\bf Bootstrapping.} During the multilevel setup process, initial test functions are generated by fine-level relaxation sweeps, from which a coarse level can be derived. Using the coarse level, a multilevel relaxation cycle can smooth the TFs much faster. As TFs become smoother, more levels are added to the hierarchy. Slow-to-converge errors that emerge from the current multilevel cycle are added to the TF set. In extreme cases, global Almost Zero Modes (AZMs), i.e., near-null-space eigenvectors, should be computed, and the coarsening adapted to fit them, to obtain an efficient multilevel solver. Bootstrapping is not strictly necessary in the 1D Helmholtz case, although it may be used to reduce the AMG setup cost; see Sec.~\ref{sec:bootstrap}.
   \item {\bf Coarse-level construction is local.} The relaxation, test functions, coarsening, interpolation and coarse operator can all be derived on a small sub-domain (different parts of the domain can be coarsened independently and in parallel). This is especially useful in case of repetitive equations. In that case, instead of multiple TFs, many {\it local windows} can be sampled from a single TF and used for the SVD coarsening and interpolation construction. \end{enumerate}

\section{AMG+ for the 1D Helmholtz Equation}
\label{helm_linear}
{\bf Comment on our computational goal and that it's a synthetic goal to solve $A x = 0$, but illustrates our techniques in the simplest setting.}

Our model problem is the 1D Helmholtz equation
\begin{equation}
	\label{helm1d}
	(A u)(x) := \Delta u(x) + K(x)^2 u(x) = F(x)\,,\qquad x \in [0,L]\,,
\end{equation}
with periodic boundary conditions $u(x) = u(x + L)$. 
The problem is discretized on a uniform grid with meshsize $h$ into the algebraic system by
\begin{equation}
	\label{level0}
	A^h u^h = f^h\,.
\end{equation}
Our goals are (a) find $M$ \emph{Test Functions (TFs)}: $u_1\,\dots,u_M$ of unit $l_2$ norm with small $\|A^h u^h\|$; i.e., near-null-space components of $A^h$; (b) construct a multilevel hierarchy that accurately approximates the test functions, and gives rise to an optimal linear solver of (\ref{level0}), i.e., it attains $\varepsilon$-residual accuracy $\|A^h u^h - f^h \| \leq\varepsilon$ in $O(n \log 1/\varepsilon)$ operations.

$h$ is assumed to be small enough to resolve the wavelength and provide good local pointwise discretization accuracy. (We do not worry at this point about decreasing $h$ further to obtain pointwise accuracy on a large domain that does not have a phase error, as in such resolutions simple geometric coarsening can be employed. Thus they are irrelevant to the challenge of coarsening the indefinite system and automatically transitioning from wave-type variables to ray-type coarse variables in the multilevel hierarchy.)

The test functions are an approximation to the eigenvectors of $A$ corresponding to the $M$ smallest eigenvalues. We do not insist on finding the exact orthogonal basis.

\subsection{Discretization}
\label{discretization}
Eq.~(\ref{helm_1d}) is discretized on a uniform grid with meshize $h$ and $n$ points $x_j = j h$, $j= 0,\dots,n-1$; references to indices larger than $n - 1$ or negative are understood as periodic, i.e., $j \text{ mod } n$), and meshsize $h = L/n$. We denote the discrete approximation by $u^h := (u^h_0,\dots,u^h_{n-1})^T$.

To ensure that special properties of tridiagonal matrices are not being exploited (as in total reduction), 
do not mask the true efficiency of the algorithm, we test both the $3$-point second-order finite difference discretization 
$$ (A^h u^h)_j = u_{j-1} + (-2 + k_j^2) u_j + u_{j+1} = f_j$$
and $5$-point  fourth-order discretization
$$ (A^h u^h)_j = \frac{1}{12} \left( -u_{j-2} + 16 u_{j-1} + (-30 + 12 k_j^2) u_j + 16 u_{j+1} - u_{j+2} \right) = f_j\,, $$
for all $j = 0,\dots,n-1$, where $f_j := h^2 f(x_j)$ and $k_j := h K(x_j)$.

\subsection{Relaxation; Shrinkage Factor}
\label{sec:relaxation}

\subsection{Local SVD Coarsening}
\label{svd_coarsening}

%\subsection{Test Functions}
%\label{sec:test_functions}
Since $A$ is indefinite, we use a robust smoother: Kaczmarz relaxation in lexicographic ordering. While for special cases (e.g. $k h = 0$), better smoothers are available (e.g., red-black Gauss-Seidel), Kaczmarz relaxation is guaranteed to smooth the error \cite{amg_theory}, so we keep the approach general.

Initial vectors are obtained by $\nu$ Kacmzarz relaxations on $A x = 0$, starting from $\text{rand}[-1, 1]$. $x$ is normalized to unit $l_2$ norm at the end. There is no need to normalize after each relaxation sweep, since we are solving a homogeneous equation (and anyway,the norm doesn't change much unless a huge number of relaxations is performed). 

{\bf What $\nu$ should we use to get a good enough initial guess for two-level boostrapping?}

For simplicity, we omit the $h$ superscripts in this section and use quantities without superscripts (e.g., $A$, $x$) to denote fine-level quantities and $c$-superscripted quantities (e.g., $A^c, x^c$) to denote coarse-level quantities.

Given a fixed-size discrete domain with $n$ and test functions $x_1,\dots,x_M$ on this domain, we derive the coarsening matrix $R_w: w_c \times w$ separately in each \emph{aggregate} (consisting of $w = 2-4$ consecutive gridpoints). The coarsening matrix over the entire domain is the block diagonal matrix $R: n_c \times n$
\begin{equation}
	R := \text{diag} \left\{ \underbrace{R_w, R_w, \dots, ... R_w }_{\frac{n}{w}}  \right\} \,.
\end{equation}

$R_w$ depends on an accuracy threshold $\varepsilon$ and minimum coarsening ratio $0 < \rho < 1$. Start with $w = 2$. The test functions provide $M$ samples of low-residual functions over the aggregate, $u^m_j= (u_{m,j},\dots,u_{k,j+w})$ for $m = 1,\dots,M$. We compute the SVD decomposition $X = U \Sigma V^T$ of the $m \times w$ matrix $X$ whose rows are the windows, where $\Sigma = \text{diag}\left\{\sigma_1, \dots, \sigma_m \right\}$ and $\sigma_1 \geq \sigma_2 \geq \cdots$. Let $w_c$ be the smallest number such that
\begin{equation}
	\left(\frac{\sum_{j=n_c+1}^m \sigma_j^2}{\sum_{j=1}^m \sigma_j^2 } \right)^{\frac12} < \varepsilon.
	\label{r_accuracy}
\end{equation}
Then $R_w := V(1:w_c,:)$ is the sub-matrix of $V$ consisting of the first $w_c$ rows of $V$. (\ref{r_accuracy}) guarantees an $\varepsilon$-relative reconstruction error of $X$ in the Frobenius norm \cite{svd}. $M$ must be larger than $w$, say $M \geq 4 w$, to avoid SVD over-fitting.

If $w_c/w > \rho$, we double the window size $w$ and recalculate $R$ until a large enough window is reached such that $w_c/w \leq \rho$. If the relaxation method is a good smoother that quickly reduces the error (and thus our test functions, since we relax on $A x = 0$) into a lower-dimensional space, such a value of $w$ can be found.  {\bf How can we determine $M$ in advance if we don't know the window size $w$ in advance?}

The SVD yields coarse variables that are less sensitive to relaxation than pointwise coarsening (i.e., selecting one or several fine gridpoints as coarse variables, which corresponds to $R_w = [1,\underbrace{0,\dots,0}_{w-1}]$), as shown by comparing mock cycle rates Sec.~\ref{mock_cycle}. It also automatically selects the number of coarse variables $w_c$ per aggregate.

{\bf Add explanation on the particular nature of the coarse vars for the 1D Helmholtz: two waves, i.e., combination of sines and cosines. Add figure.}

\subsection{Mock Cycle Performance Predictor}
\label{sec:mock_cycle}
{\bf
\begin{enumerate}
\item Define it so that it also works in the non-linear case.
\item Numerical example for Poisson (2/1), Helmholtz (4/2, 6/2, 6/3).
\end{enumerate}
}

\subsection{Least-Squares Interpolation}
\label{sec:interpolation}
As for $R$, we derive an interpolation matrix $P_w$ on an aggregate of size $w$, and then create a block diagonal$P_{n \times n_c}$ interpolation matrix over the entire domain from the aggregate interpolation matrices.

The \emph{interpolatory set} (the set of coarse variables used to interpolate to a target fine variable) can be the set of coarse variables of its aggregate. In this case, the optimal interpolation that minimizes the interpolation error of the test functions is $P_w = R_w^T$ (as the truncated SVD is the best low-rank approximation in the Frobenius norm).

While this choice may be sufficient in some problems, even in the Laplace case ($k=0$), $R_w^T$ is a piecewise-constant geometric interpolation (with the window size $w = 2, w_c = 1$, where only one principal component already explains most of the energy -- the constant vector), which only yields marginal multilevel efficiency (two-level, but but not multilevel $h$-independent convergence). Linear interpolation is required to obtain the full multilevel efficiency.

Thus, we increase $P_w$'s accuracy by including in the interpolatory set as many coarse variables as possible without increasing the coarse operator's density. A general rule is to use the union of all coarse variables in all aggregates of the fine gridpoints appearing in the target gridpoint's stencil.

The \emph{interpolation weights} are determined by least-square fitting to test samples.

{\bf include a description of the LS process: NO NEED TO WEIGHT functions at this stage until there are AZMs, and then we use a different process (Kaczmarz) to modify interpolation. No need to apply Ritz projection to TFs - per Steve McCormick's comment/SA work.
Test set: test the interpolation accuracy on samples we haven't used to ensure no overfitting.
}

\begin{equation}
	\min_p \sum_m \left( x_{m,i} - \sum_j p_{ij} x_{m,j} \right)^2 + \alpha \left( \sum_m (x_{m,i})^2 \right) \left(\sum_j p_{ij}^2\right) 
\end{equation}

\subsection{Coarse-level Operator}
\label{sec:coarse_operator}
Sparsification, symmetrization.

{\bf This is the first time where we use $R$ in the algorithm, not just in a criterion, like CR was used.}

\subsection{Two-level Cycle}
\label{sec:two_level}

{\bf
\begin{enumerate}
\item all we need is good shrinkage. Do not solve exactly at coarse level here!
\item Bootstrapping can be used to reduce cost.
\item Show numerical example for Poisson (2/1), Helmholtz (4/2, 6/2, 6/3). Mock cycle vs. 2-level cycle.
\end{enumerate}
}

\subsection{Multilevel Cycle}
{\bf
\begin{enumerate}
\item Show asymptotic convergence for full cycle, when coarsest is small enough.
\item Wave-Ray needed a W-cycle at a transitional level; we can gee away with V at all level.
\end{enumerate}
}

\label{sec:multilevel}

\subsubsection{Emerging Type of Coarse Variables}
{\bf Wave to Ray Transition -- plot coarse variables. Interpretation of coarse stencils as rotations after diagonalizing the center block? are coarse equations elliptic / Ray?
}

\section{Setup Optimization}
\label{sec:reduce_cost}

{\bf
\begin{enumerate}
\item All we need is good shrinkage so no need to run many cycles.
\item Bootstrapping can be used to reduce cost.
\item performance vs. number of windows, samples. No need for samples $> 2 *$ aggregate size.
\end{enumerate}
}

\subsection{Exploiting Repetitiveness}
\label{sec:repetitive}
{\bf
\begin{enumerate}
\item Sampling; tiling $R,P,Q$. Locality of mock cycle, two-level cycle rates.
\item Show numerical example for Poisson (2/1), Helmholtz (4/2, 6/2, 6/3). Mock cycle vs. 2-level cycle.
\end{enumerate}
}

%\section{Numerical Results}
%We test two discretizations of the Helmholtz operator $A = -\Delta + k^2 I$: a $3$-point second order stencil , and a $5$-point $4th$ order finite-differences $A^h = [-1, 16, -30 + 12 (kh)^2, 16, -1]$. The $5$-point stencil avoids any properties particular to 1D tridiagonal systems that lead to easier coarsening (e.g., as in total reduction).
%
%\subsection{General Framework}
%\subsection{Repetitive Framework}
%
%\subsubsection{Quantitative Performance Prediction}
%%\label{mock_cycle}
%
%{\bf Smoothing Factor:} The Fourier mode analysis is done for a fixed $k = K h$.
%
%The traditional smoothing factor \cite[Sec. ??]{guide} assumes that the coarse level perfectly reduces all scaled frequencies $\theta$ with $|\theta| leq \frac{\pi}{\rho}$ and leaves intact all high frequencies $|\theta| > \frac{\pi}{\rho}$, where $\rho$ is the coarsening factor. However, since our coarsening is designed to represent near null-space components of the Helmholtz operator instead of smooth functions, the smoothing factor definition should be modified to
%\begin{equation}
%	\overline{\mu} := \max_{\theta \in HF} \left| R(\theta) \right|\,,
%\end{equation}
%where $R$ is the relaxation amplification factor. HF is the set of relevant high-frequency components: the set of $[n/\rho]$ frequencies $\theta$ with smallest $|A(\theta)|$ value.
%
%{\bf TODO: James and Karsten calculated the traditional Kaczmarz smoothing factor to be $0.75$, but for which $k$ and coarsening factor? Add a table here of $\overline{\mu}$ vs. $k$ and $\rho$. Also add a column with the smoothing factor to the mock cycle table below so we can compare elevant numbers. Also note: can calculate $\overline{\mu}$ by running Fourier components through Kaczmarz to numerically calculate its amplification matrix.}
%
%{\bf Mock Cycle}
%To gauge the quality of the coarse variable set, we compared the mock cycle convergence factors with SVD coarsening, $1:2$ pointwise coarsening ($R_w = [1, 0]$) and averaging coarsening with $1:2$ coarsening ratio ($R_w = [1, 1]$). for different $kh$ values and $\nu$ relaxations per cycle. Kaczmarz relaxation was used in all cases; for $kh=0$ only, we also tested Gauss-Seidel relaxation for comparison. The SVD coarsening was based on test vectors obtained by $100$ relaxations of $\text{random}[-1,1]$ starting vectors, a $w=4$-point aggregate to calculate the SVD, and accuracy threshold of  $\varepsilon=0.1$. Two principal components were selected as coarse variables for all $kh$, so all three coarsening schemes have coarsening ratio $\rho = 0.5$.
% 
%\begin{table}
%{\small
%\begin{tabular}{l|cccc|cccc|cccc}
%\toprule
%& \multicolumn{4}{c|}{SVD Coarsening} & \multicolumn{4}{|c|}{Pointwise Coarsening} & \multicolumn{4}{|c}{Averaging Coarsening} \\
%$k$ &  $\nu=1$ &  $2$ &  $3$ &  $4$ &  $1$ &  $2$ &  $3$ &  $4$ &  $1$ &  $2$ &  $3$ &  $4$ \\
%\midrule
%GS &       0.30 &       0.11 &       0.07 &       0.02 &      0.65 &      0.60 &      0.57 &      0.54 &       0.32 &       0.12 &       0.07 &       0.02 \\
%0.0   &       0.42 &       0.22 &       0.15 &       0.12 &      0.49 &      0.55 &      0.49 &      0.50 &       0.45 &       0.21 &       0.19 &       0.15 \\
%0.1   &       0.46 &       0.22 &       0.15 &       0.13 &      0.50 &      0.55 &      0.48 &      0.48 &       0.42 &       0.20 &       0.19 &       0.15 \\
%0.2   &       0.45 &       0.24 &       0.15 &       0.12 &      0.51 &      0.58 &      0.50 &      0.48 &       0.45 &       0.21 &       0.19 &       0.16 \\
%0.3   &       0.46 &       0.23 &       0.15 &       0.12 &      0.49 &      0.56 &      0.50 &      0.49 &       0.42 &       0.20 &       0.20 &       0.15 \\
%0.4   &       0.49 &       0.24 &       0.16 &       0.12 &      0.50 &      0.58 &      0.53 &      0.49 &       0.49 &       0.20 &       0.20 &       0.18 \\
%0.5   &       0.51 &       0.25 &       0.16 &       0.13 &      0.53 &      0.55 &      0.49 &      0.47 &       0.47 &       0.22 &       0.21 &       0.20 \\
%0.6   &       0.52 &       0.30 &       0.13 &       0.14 &      0.56 &      0.52 &      0.53 &      0.48 &       0.52 &       0.22 &       0.20 &       0.21 \\
%0.7   &       0.57 &       0.25 &       0.15 &       0.13 &      0.61 &      0.51 &      0.53 &      0.46 &       0.54 &       0.25 &       0.22 &       0.24 \\
%0.8   &       0.60 &       0.30 &       0.15 &       0.11 &      0.64 &      0.44 &      0.50 &      0.52 &       0.49 &       0.24 &       0.23 &       0.26 \\
%0.9   &       0.63 &       0.35 &       0.17 &       0.15 &      0.67 &      0.39 &      0.49 &      0.51 &       0.63 &       0.34 &       0.20 &       0.27 \\
%1.0   &       0.60 &       0.29 &       0.14 &       0.10 &      0.74 &      0.46 &      0.43 &      0.50 &       0.69 &       0.48 &       0.25 &       0.25 \\
%\bottomrule
%\end{tabular}
%}
%\caption{Mock cycle convergence factors for a grid of size $n=32$. The first row corresponds to the Laplace operator and Gauss-Seidel. All other rows refer to Kaczmarz relaxation. $\nu$ is the number of relaxations per cycle.}
%\end{table}
%
%The pointwise coarsening convergence factors is bounded by $0.5$ as the error function $1,0,1,\dots$ is reduced only by a meager $0.5$ factor by relaxation. In practice, a multigrid cycle with pointwise coarsening converges faster. Generally, mock cycle predictions can be pessimistic, and cases where their prediction is quantitatively accurate should be identified based on numerical experimentation. Here, it is more accurate when the coarse variables are averages. As expected, the SVD coarsening is equivalent to averaging for the Laplace case, but trumps simple averaging for $k = O(1)$, which is substantiates its use for upscaling. The convergence improves with $\nu$ up to $3$ relaxation sweeps.
%
%Finally, in line with geometric multigrid theory, Gauss-Seidel is a better smoother for the Laplace case, but Kaczmarz relaxation provides adequate convergence as well in that case, while its efficiency is maintained for all values of $0 \leq k \leq 1$. All results are independent of $n$, for $n \geq 16$.
%
%\subsection{2-level Shrinkage}
%{\bf 
%\begin{itemize}
%\item Start with $n=96$ at level 1. 
%\item Show 2-level rates of solving $A x = 0$ for $n=96$ as a function of number of bootstrap cycles. Test function residuals.
%\item  Then show convergence for increasingly larger $n$ with the same coarsening.
%\item How do the SVD components look like.
%\item Performance vs. window size. Number of components.
%\end{itemize}
%}

\section{Future Directions}
\label{future}

\subsection{The Multi-dimensional Helmholtz Equation}
\label{multidim}

\subsection{Systematic Upscaling}
\label{upscaling}
Many physical systems involve a huge number of variables, such that even attaining linear complexity is not good enough, since the number of variables is huge. Such systems on the other hand are typically \emph{highly repetitive}, in the sense that the same small set of governing equations (or Hamiltonian terms) keep repeating itself throughout the physical domain. 

Consider for instance the 1D Helmholtz equation with constant $K(x) \equiv K$ on a large domain of size $L$. AMG+ can be adapted to take advantage of the periodicity to derive the coarse equations locally, and use the 

\begin{itemize}
\item {\bf Step 1: domain growing; still use the entire fine level for relaxation.} We then double $L$ and tile the test vectors and $P$, $R$ and $A^c$ over the twice-larger domain. For each domain doubling, we add another coarse level to the hierarchy. 

\item {\bf Step 2: use only local windows on the fine levels and FAS} to obtain sub-linear complexity.
\end{itemize}

{\bf When do we grow the domain? Do we add more levels on a fixed-size domain of size $n$ gridpoints until we get to $O(\log n)$ levels and a coarsest grid with only a few points, before growing the domain, so that we get a full multilevel cycle that reduces the error by a fixed factor in $O(n)$ operations; or, do we start with level 1 on $n$ points, create level $2$ with $n/2$ points, improve the hierarchy with $2$-level cycles, then consider level $2$ on $n$ points as our finest level, and coarsen it using $2$-level cycles, and so on, i.e., recursive homogenization with $2$ level-cycles only, but increasingly larger domains?}

\subsection{Eigensolver}
{\bf Solving for an eigenvalue $A x = \lambda x$ near a given value of $\lambda$ in the middle of the spectrum. This is always a well-defined goal. Application to quantum chemistry.} Use Rayleigh-quotient/inverse iteration at coarse level + FAS transfer of $A x = \lambda B x$ + normalization constraint.

\subsection{$\tau$-Homogenization}
\label{hom}
The FAS coarse-level equations depend on the fine level only through the fine-to-coarse $\tau$ correction:
\begin{equation}
	A^c x^c = P^T b + \tau\,,\qquad \tau :=  A^c \overline{x}^c - P^T A x = (A^c R - P^T A) x\,,
	\label{fas}
\end{equation}
where $\overline{x}^c := R x$ is the restricted fine-level approximate solution (and initial coarse level approximation). Thus, $\tau_I$ depends on a local neighborhood of fine-level values near the coarse gridpoint $I$, and thus in turn on neighboring coarse-level values $x^c_J$. We can thus attempt to \emph{homogenize} (\ref{fas}) to only depend on $x^c$  by fitting a rule $\tau \approx Q x^c$ with some interpolation matrix $Q$. $Q$ can be found using a least-squares fit, just like for the BAMG interpolation $P$, namely,
\begin{equation}
	\min_p \sum_m \left( \tau_{m,I} - \sum_j q_{IJ} x^c_{m,J} \right)^2 + \alpha \left( \sum_m (\tau_{mI})^2 \right) \left(\sum_j q_{IJ}^2\right) \,.
\end{equation}

\begin{itemize}
    \item Local relaxation.
	\item Non-linear problems (e.g., fluid flows): use FAS; fine-level windows for different coarse solution values.
	\item Molecular dynamics: $\tau$ homogenization may lead to discovering different physical laws at different scales.
	\item Non-geometric and non-local problems (e.g., NNs): use correlations instead of locality graph to define aggregates.
\end{itemize}

\bibliographystyle{plain}
\bibliography{mg.bib}

\end{document}
