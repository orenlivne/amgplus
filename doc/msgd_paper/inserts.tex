%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Achi's revisions, organized in snippets. Eventually to be integrated
% into the the main multiscale optimization paper.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass{article} % For LaTeX2e
%\documentstyle[nips14submit_09,times,art10]{article} % For LaTeX 2.09

%--------------------------------------------------------
% Packages.\dfrac{•}{•}
%--------------------------------------------------------
%\usepackage{nips15submit_e,times}
\usepackage[toc,page]{appendix}
\usepackage{hyperref}
\usepackage{url}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{subcaption} 
\usepackage{mathtools}
%\usepackage{xcolor}
%\usepackage{soul}
\usepackage{graphicx}
\usepackage[disable]{todonotes}
\usepackage{changebar}
%\graphicspath{ {figs/} }
\newtheorem{theorem}{Theorem}
\include{aliases}
\newcommand{\ins}{INSERT}

% Number equations A.1, A.2, etc.
\counterwithin*{equation}{section}
\renewcommand{\theequation}{\thesection.\arabic{equation}}
%--------------------------------------------------------
% Title.
%--------------------------------------------------------
\title{Multi-level Stochastic Optimization and Neural Networks - Snippets}
% \author{
% Achi Brandt\\
% Oren E. Livne \\
% }
\date{\today}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Paper starts here.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}

% \maketitle
% \tableofcontents
% \todototoc
% \listoftodos

%\begin{abstract}
%\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage
\section{U}
instead of coarsening and interpolation being inexpensively performed at the activation yard\footnote{grid?}. But at this stage of coarsening it may be OK, since we may assume that the number of parameters is much smaller than their number at previous coarsening stages.

\section{V1}
\subsection{10.3 The Hebbian Neuroscience Rule}
Suppose two input activations $a_1$ and $a_2$ are highly correlated. According to the coarsening algorithm in Sec.~\ref{coarsening:8} they will form a two-activation neighborhood at the coarse level ($a^c = Q a$; see Fig.~{2}) will include a linear combination of them $a^c_k = q_1 a_1 + q_2 a_2$. This is exactly the well-known {\it Hebbian rule} in neuroscience: ``cells that fire together wire together``.

``Highly correlated $a_1$ and $a_2$`` means that there exists a normalized linear combination
$$ r_1 a^m_1 + r_2 a^m_2\,, \quad r_1^2 + r_2^2 = 1 $$
that is small for all samples $m$\footnote{The superscript $c$ for coarse-level quantity collides with the superscript $m$ for sample. Clarify notation.} The multi-level algorithm extends this rule to larger activation sets (termed ``neighborhoods``), each containing one or multiple\footnote{Rephrased this sentence a little since it was unclear. Please check.} normalized linear combinations that are small (compared with other normalized combinations in the same neighborhood).

Further interesting analogies should be investigated between the multilevel algorithms and system neuroscience. For example, the search for hidden mini-clusters (see Apps.~\ref{G.4, G.5}) is akin to ``recent technological advances that enable large-scale recording of neural activity, [enabling] the discovery of new behavioral variables hidden within neural codes, and even allows using one animal's data to decode another animal's behavior`` \cite{18}. This finding may indicate that the association matrices $T$ can be initialized by knowledge transfers within a continuation process (see App.~\ref{H}).

\section{V2}
\subsection{10.4 Low Rank Adaptation (LoRA)}
There is an obvious similarity between a two-level coarsening scheme and a recent technique called Low Rank Adaptation (LoRA) \cite{lora}.

Generally, as discussed in Sec.~\ref{3}, following a proper relaxation (e.g., SGD), the main remaining error (hence the needed correction) resides in a much lower dimensional space. The LoRA approach (without recognizing the real reason for the low dimension) is to {\it learn} the projection of the correction to lower dimension (our $Q$ operator) as part of learning the optimal adaptation of a pre-trained network to any downstream task. A great advantage of this approach compared to our multi-level coarsening method is that it is simpler to understand and implement. On the other hand, the system has the following potential advantages:

\begin{enumerate}
    \item Our coarsening operator $Q$ is potentially much sparser than LoRA's projection operator $A$, since it coarsens one local neighborhood at a time, while $A$ is a full matrix.\footnote{However, critics will say that modern GPUs are optimized to perform matrix-vector multiplications a lot more efficiently than a sparse matrix multiplication. One might argue that that means we should change GPU architectures, but this might take time. How can we efficiently implement the multilevel algorithm so it gains not just in FLOPs but also in runtime?}
    \item The locality also implies that $Q$ can be derived based on a much smaller number of examples.
    \item Our coarse level correction is designed to accelerate the pre-training too, not just the fine tuning.
    \item Equiped with FAS, our coarse level can itself be corrected by a still coarser level (a correction of still-lower dimension).
    \item The FAS coarsening may lead to upscaling (see Sec.~\ref{14}), whereupon the heavy pretrained network is discarded, leaving only the much lighter coarse levels, both for post-training and for inference.
\end{enumerate}

An interesting hybrid possibility is to start with the local-SVD-based $Q$, then improve it as part of the overall optimization by backprojection. This should yield faster learning than starting from a random $Q$.

\section{V4}
\subsection{10.5 Transformer}
There are fundamental similarities between the transformer algorithm \cite{transformer} and the multilevel solver.

The multilevel solver's alternation between relaxation (fine-level processing) and coarse-level iterations is similar to the transformer's alternation between attention layers and simple feed-forward networks. An attention layer is similar to a coarse-level correction since it learns and performs {\it collective} changes of correlated activations. The key and query attention matrices are analogous to the transformation $T$ that brings out hidden neighborhoods (see App.~\ref{G4}).

These similarities may actually suggest some useful improvements. The mini-clustering production of neighborhoods (see App.~\ref{G1}) hints to a fast way of summarizing attention over very long context windows. The multi-level construction of the matrix $T$ (see App.~\ref{G5}) suggests faster ways to {\it train} the attention matrices.

\subsection{W}
$\gamma^m_j(p) = \partial L/\partial \tilde{a}_j(p)$, i.e. correlations between vectors
$$
\gamma_j = \left\{ \gamma^m_j(p) \right\}_{m \in \text{batch}, p \in \text{output grid}}\,.
$$
Briefly: using mini-clustering (see App.~\ref{sec:miniclustering}) we subdivide the space of vectors $\gamma_j$ into strongly correlated neighborhoods, then using SVD (or economical SVD for large neighborhoods, see Sec.~\ref{?}) we divide each neighborhood into two complementary subspaces: a space of low-noise combinations and one of higher-noise. The low-noise combinations are used for low-noise relaxation, while the higher-noise ones define the coarsening operator $\tilde{Q}$. The coarse level of the layer then read as
\begin{equation}
    \tilde{a}^c = W^c a\,,\quad \tilde{a} = \tilde{Q}^T \tilde{a}^c\,.
    \label{11.1}
\end{equation}
The coarse-level processing optimizes $W^c$ by backpropagation, starting with $W^c = 0$ (or with $\hat{W}^c$ approximating $W$ when using FAS). $W^c$ (or $\delta W^c = W^c - \hat{W}^c$ when using FAS) is then interpolated and added to $W$, where the interpolation $\tilde{P}^*$ is defined as corresponding to interpolation designed in the space of vectors $\gamma_j$.

\section{Appendix X: Dependence of Gradient Combination Statistics on Input Activation Statistics}
\label{sec:x}

{\bf Notation.} In a given layer, at each output point $p$ and for each sample $m$, denote
\begin{itemize}
    \item $a^m_i(p)$ = the $i${\it th} post-ReLU {\it input} activation.
    \item $\gamma^m_j(p)$ = the loss derivative with respect to the $j${\it th} pre-ReLU
    {\it output} activation.
    \item $g^m_{ij} = \sum_p a^m_i(p) \gamma^m_j(p)$ = gradient of the weight $w_{ij}$.
    \item $\overline{[\cdot]^m}$ = averaging the quantity $[\cdot]^m$ over all samples $m$.
\end{itemize}

{\bf Combinations.} For any normalized vector $\alpha = (\alpha_1, \alpha_2, ...)$, $\sum_i \alpha_i^2 = 1$, we define a normalized combination of the input activations $A^m_{\alpha} = \sum_i \alpha_i a^m_i(p)$ and a corresponding gradient combination
$$ G^m_{\alpha j} = \sum_i \alpha_i \alpha_i g^m_{ij} = \sum_i \alpha_i \sum_p a^m_i(p) \gamma^m_j(p)\,.$$

{\bf Assumption 1:} during training gradient averages should tend to zero, so we assume $|\overline{\gamma^m_j(p)}| \ll \overline{|\gamma^m_j(p)|}$.

{\bf Averages.} $\overline{G^m_{\alpha j}} = \sum_p \overline{A^m_{\alpha}(p) \gamma^m_j(p)}\,.$
Since $\gamma^m_j(p)$, and even its sign, may vary arbitrarily, $\overline{G^m_{\alpha j}}$ cannot be estimated from input activation statistics. In contrast, the relative sizes of $\overline{(G^m_{\alpha j})^2}$ and variances of $G^m_{\alpha j}$ can generally be estimated from the input activation statistics, as follows.

{\bf Variance.}
\begin{equation}
    \begin{split}
    \overline{\left(G^m_{\alpha j} - \overline{G^m_{\alpha j}}\right)^2} \approx \overline{\left(G^m_{\alpha j}\right)^2} \\
     = & \sum_{p, q} \overline{A^m_{\alpha}(p) \gamma^m_j(p) A^m_{\alpha}(q) \gamma^m_j(q)} \\
     \leq & \frac14 \sum_{p, q} \overline{\left(A^m_{\alpha}(p)^2 + A^m_{\alpha}(q)^2\right)(\left(\gamma^m_j(p)^2 + \gamma^m_j(q)^2\right)}\footnote{Follows from the arithmetic-geometric mean inequality and Lagrange's identity: $abcd \leq ((ac + bd)/2)^2 \leq \frac14 ((ac + bd)^2 + (a d - b c)^2) = \frac14 (a^2 + b^2)(c^2 + d^2).$}\\
     = & \sum_p \overline{A^m_{\alpha}(p)^2 \cdot \frac12 \sum_q (\left(\gamma^m_j(p)^2 + \gamma^m_j(q)^2\right)}\footnote{I need further explanation on why this inequality and the next one hold.}\\
     \approx & N_{\text{out}} \sum_p \overline{A^m_{\alpha}(p)^2} \sum_p \overline{\gamma^m_j(p)^2}\,,
    \end{split}
    \label{g_var}
\end{equation}
where $\approx$ denotes likely comparable sizes. Indeed, dividing both sides by $N_{\text{out}} \sum_p \overline{\gamma^m_j(p)^2}$, they both express some weighted averages of $\sum_p A^m_{\alpha}(p)^2$, only with different non-negative weights.

{\bf Conclusion I.} A gradient combination can generally have large variance (compared with the variance of some other normalized combinations in the same output sheaf) only if the corresponding combination of input activations has a large size (compared to the size of some other activation combinations).

{\bf Conclusion II.} A small error in an interpolation between input activations implies a small error in the corresponding parameter interpolation in every sheaf. To see this, apply the above inequality to the combination expressing the interpolation {\it error}, which is the difference between the interpolant, which is a linear combination of coarse variables, and the target fine-level variable, where each coarse variable is itself a linear combination (the operator $Q$) of fine-level variables.

{\bf Fully-connected layer.} In the special case of a fully connected layer, where there is no out-point ($p$) dependence , the relations are simpler (approximate equality, not {\it in}equality) and extend to {\it co}variances (not just variances) of combinations, as follows.

{\bf Covariance.}
\begin{equation}
    \begin{split}
    \overline{\left(G^m_{\alpha j} - \overline{G^m_{\alpha j}}\right) \left(G^m_{\beta j} - \overline{G^m_{\beta j}}\right)} & \\
     & \approx \overline{G^m_{\alpha j} G^m_{\beta j}} \\
     & = \overline{A^m_{\alpha} A^m_{\beta} \left(\gamma^m_j\right)^2} \\
     & \approx \overline{A^m_{\alpha} A^m_{\beta}} \overline{\left(\gamma^m_j\right)^2}\,,
    \end{split}
    \label{g_cov}
\end{equation}
because, dividing both sides by $\overline{\left(\gamma^m_j\right)^2}$, both express some weighted average of $A^m_{\alpha} A^m_{\beta}$, only with different non-negative weights.

\section{H. Continuation: Network Initialization and Growth}
\label{sec:h}
The present paper is mainly concerned with {\it accelerating the convergence} of iterative solvers using multilevel methods. The minimization problems under consideration (NN optimization in particular) are highly nonlinear and often have many local minima. The iterations' convergence speed and the solution they approach heavily depend on where (or how) they are {\it initialized}.

A popular initialization approach, and the simplest to implement, is to start {\it random}. Usually such a start is far from a solution and requires many iterations to converge. Moreover, a random start for a highly nonlinear problem, even when by chance successful, would often lead to wrong or unhealthy solution, e.g., the obtained network would not be generalizable enough and would show high sensitivity to slight data perturbations.

Generally, at high nonlinearity, a desired solution can be reliably obtained only if we start from a close-enough first approximation. In many areas of scientific computation, a common way to obtain a good first approximation (and trace sequences of solutions) is by the method of {\it continuation} (also called ``embedding`` or ``Davidenko method''; see for example \cite{keller77}). Namely, a sequence of auxiliary problems is formulated, starting from an easily solvable (e.g., linear) problem and gradually increasing its complexity all the way to the desired problem, such that the (approximate) solution of each problem in the sequence is a good initial guess for the next problem\footnote{reworded this sentence since it was a bit cumbersome; see if you like it.}. The sequence of auxiliary problems in fact {\it defines} which solution we are interested in, {\it even when it is not the lowest minimum} of the formal objective functional. A {\it multilevel} solver integrated with a continuation process may in fact be essential for a good {\it definition} of the desired solution (see example in \cite{SafRon}).

The continuation process need not be expensive, since for dragging the system to the desired attraction basic it may be sufficient so only solve each auxiliary problem very crude, leaving the bulk of computation to the high-accuracy final stage.

The continuation approach is in fact already largely implemented in Neural Network computations, e.g., under the name ``knowledge transfer'' or ``downstream tasks``, initialized by a pre-trained foundation model. But continuation can of course be applied to the pre-training itself.

Choosing for example the objective of pre-training to be the identification when two given images are two different views of the same object or scene, one can start with the easy case that the two images are adjacent video frames, and gradually develop the network to deal with increasingly longer time intervals between the frames \footnote{Simplified sentence, see what you think.}. One can start with only few different objects in the database, perhaps even just one object (like a baby starting by recognizing just his/her mother's face), and gradually increase the number of objects. Similarly, one can gradually increase the number of moving parts of objects, or the noise level, occlusion, distortions, etc. and then introduce object classes and other distinctions of interest.

As the sequence of problems unfolds, the NN architecture can, and should, adapt itself, typically adding more channels, more layers, denser resolutions, wider kernels (see Sec.~\ref{sec:13}), or wider attention context windows, more speciality sub-networks (see Sec.~\ref{sec:15.7}), or stronger localization procedures (see Sec.~\ref{sec:5.4}) -- where and when needed.

A multilevel structure can be particularly useful here, as many continuation steps can be confined to coarse levels, with rare visits to finer ones to update the fine-to-coarse defect correction (see Sec.~\ref{sec:?}). Parameter {\it relations} (like those involved in coarsening) can be more suitable for knowledge transfer than the parameters themselves.

Furthermore, one organic mode of architecture adaptation during continuation can emerge from the multilevel solver: the latter typically introduces {\it fixed} coarsening layers (like $Q$ in Fig.~\ref{fig:2}); then, with more incoming data later in the continuation sequence, these layers can be {\it ``unfrozen''} to become regular {\it learned} layers.

\section{Z}
For a {\it deterministic} {\it linear} system of equations, $Ax = b$, producing low-GER test functions is quite straightforward: just apply your relaxation iterations to the homogeneous equation $Ax = 0$, starting from a random initial approximation $x$. When the iteration slows down the current approximation (which is also the current error $e$, since the solution is $0$) is an example of a {\it relaxed} error, a lower-GER error. Starting with another random error, another low-GER example is produced; and so on. To produce examples with still-lower GER (if needed), apply the to homogeneous equation not just your relaxation, but your current multilevel cycles.

For the deterministic {\it nonlinear} system $A(x) = b$ one can apply the same procedure to the system $A(y) = A(x_0)$, where $x_0$ is the current approximate solution and $y = x_0 + e$, $e$ being initially a random perturbation. The initial as well as all subsequent-iteration residuals $A(x_0) - A(y)$  throughout this procedure should be large compared with $b - A(x_0)$, the current residual, for the procedure to produce a good example of a relaxed error. Also, the iterations should converge $y$ to $x_0$, which is not always guaranteed in nonlinear systems. So the procedure is quite limited in producing high-accuracy examples of relaxed errors. Still, it should be adequate for producing reasonable first approximations to $P$ and $Q$ \footnote{Why?}. And in the stochastic case, good initial coarsening is provided by the sequence of samples described in Sec.~\ref{sec:5.5}. Then, to get more or higher-accuracy examples, one can generally use the following steps.

\section{E}
We show below that some popular training acceleration techniques can be interpreted as vastly simplified special cases of collective-motion coarsening steps. They attest to the fact that even a very partial coarsening can significantly improve performance. We also point out that at least some simple coarsening steps are known to occur biologically.

\section{E1}
(Generally,correlations between variables (or between gradients) converge much sooner than the variables (or gradients) themselves.)

\section{E2}
Similarly, the fine-tuning of a pre-trained model to a new task may be mostly confined to the coarse levels of the pre-trained model.

\section{E3}
\begin{itemize}
    \item By systematically separating in each level at each layer the higher-noise from the low-noise and the lower-RER from the high-RER components, the multilevel cycles can eliminate the main resources of slowness inherent in current SVD optimizers. The cycles are highly parallelizable. Even though the gain in wall clock speed may be partly offset due to hardware considerations, the energy savings are likely to still fully reflect the bug reduction in operation count.
\end{itemize}

\section{E4}
\subsection{15.6 Memory Efficiency}
The organization of input activations and weights in mini-clusters allow storing only their differences from cluster centers, which can be done in a relatively lower accuracy, resulting in large store savings via quantization and low-dimensional projections (see App.~\ref{G.3}).

\section{F1}
\subsection{6.1 Non-uniform Noise}
In the case that the sampling noise is nearly uniform (i.e., the variance $\sigma^2$ has comparable values for all components), a conventional multigrid-like training algorithm can treat the two problematic aspects of the stochastic ill-conditioning: first, it can apply only a small number of updates at the fine level (using $O(\mu^2/sigma^2)$ minibatch sizes, where $\mu$ is the current target accuracy),

\section{F2}
Often, however, the real problem is the non-uniformity of the noise, where some high-$\lambda$ components may be very noisy (having much larger $\sigma^2$ than others), forcing small $\varepsilon$ in (\ref{eqn:A.7)}. Therefore, the stochastic multilevel algorithm starts with separating low noise from higher noise (see Sec.~\ref{sec:5.6}).

\section{Q-1}
\subsection{13. Increased Convolutional Kernel Width}
In convolutional layers, the input kernel (for 2D inputs) has dimension $N_c \times k \times k$ (see Sec.~\ref{sec:8}). Most often the smallest reasonable width $k = 3$ is used. Wider (larger-$k$) kernels, when optimized, should always give better results (sine a narrow kernel is a special case of a wider one), but they are much more expensive to both train and use. In addition  to increased complexity due to the $O(k^2)$ number of parameters, training with larger $k$ also suffers from more severe slowness due to the larger-scale motions that need to be performed.

In contrast, with a multilevel optimizer the use of a larger $k$ seems more attractive, for the following reasons.

First, the slowness due to required larger-scale motions is eliminated by performing them at coarser levels.

\section{Q-2}
Secondly, it is exactly those larger-scale motions that make for higher accuracy. To obtain the same accuracy with a narrower kernel, many more layers need to be employed to account for the larger scale corrections, which introduces many more parameters as well as inter-level slowness (see Sec.~\ref{sec:12}).

A wider kernel would allow taking sooner into account wider and/or more remote correlations. It

\section{Q-3}
The wider/remote correlations can be detected as soon as they occur by applying an efficient multi-level mini-clustering (like RMC, App.~\ref{sec:x1} to the entire input of a wide kernel.

The coarsening with a wider kernel should yield much better coarsening ratios\footnote{Why?}. This, together with running most of the training at coarse levels, and using corresponding upscaling (see Sec.~\ref{sec:upscaling}) at inference time, may compensate for the increased number of parameters in a wider kernel network.

\section{R}
In fact, one or more levels of upscaling can often naturally occur as part of coarsening. That should happen when the optimization problem is overparameterized. In such cases, by performing the CGD relaxation (see Sec.~\ref{sec:7.1}) or the low-noise relaxation (see Sec.~\ref{sec:8.3}) one would find that the relaxed combinations are already converged (so the relaxation has negligible effect), so one can switch to the next coarser level and never return to the current fine level.

Another case should happen upon fine-tuning for a downstream task, which should normally depend on much fewer degrees of freedom (compared to the huge number of pretrained parameters). One should then only update some coarse levels at each layer. Using FAS, the fine level should then only be visited infrequently, to update the fine-to-coarse defect correction. Using upscaling (i.e., learning th edefect correction's dependence on the coarse variables) one may avoid further fine-level visits altogether.

\section{S1}
\subsection{8.1 Coarsening}
\section{sec:coarsening}
Following Sec.~\ref{sec:relaxation_rules}, a coarse variable for the sheaf $\tilde{S}_j$ is a linear combination $W_{\alpha j} = \sum_i \alpha_i w_{ij}$ of its variables, whose gradient at sample $m$ is
$$ G^m_{\alpha j} = \sum_i \alpha_i g^m_{ij}\,, \\quad \sum_i \alpha_i^2 = 1 $$
and whose average size
\begin{equation}
    \frac{1}{M} \sum_m \left(G^m_{\alpha j} \right)^2 \text{ is large }
    \label{parameter_large_avg_size}
\end{equation}
compared to other normalized combinations in the same neighborhood. (The neighborhoods will be discussed below.) By conclusion I in App.~\ref{sec:x}, this generally implies that the corresponding linear combination of input activations
$$A^m_{\alpha} = \sum_i \alpha_i a^m_i$$
has a larger average size, i.e.,
\begin{equation}
    \frac{1}{M} \sum_m \left(A^m_{\alpha} \right)^2 \text{ is large }
    \label{activation_large_avg_size}
\end{equation}
compared to other normalized combinations (in the corresponding neighborhood of input activations).

\section{S4}
to produce sparse $Q$, and then employ SVD to calculate $Q$ in each neighborhood. In these calculations each input activation $a_i$ is represented by the vector $\left\{ a^m_i \right\}_{m \in B}$. The batch $B$ need not be very large, because all we need at this point is to capture the strongest correlations. (But see Sec.~\ref{sec:5.4}) for the case that the neighborhoods cannot be sufficiently small, hence $B$ cannot be smaller either).

\section{S3}
Note that App.~\ref{sec:x} shows that (\ref{parameter_large_avg_size}) implies (\ref{activation_large_avg_size}), but not that (\ref{activation_large_avg_size}) implies (\ref{parameter_large_avg_size}) for every $j$. This means that some parameter combinations may be added to the coarse level that do not {\it have to} be there. This should cause no problem.

\section{S2}
The coarse level will have the same general layer structure as the fine level, so solving its equations can be done with the same procedures (relaxation and further coarsening).

\section{S5--S6}
\subsection{8.2 Multilevel Cycle}
\label{sec:multilevel_cycle}
{\bf a multilevel cycle} for approximately solving the NN stochastic optimization is composed of the following four steps.
\begin{enumerate}
    \item Several {\it relaxation} iterations (see Sec.~\ref{sec:8.3} below).
    \item {\it Coarsening:} calculating the fine-to-coarse projection $Q$ as described above. (The same $Q$ can actually be reused by several successive cycles, so this calculation is done only one per several cycles). This, together with (\ref{8.5}) defines {\it the coarse level network} (depicted in Fig.~\ref{fig:fig2}). Note that the input activation to the coarse layer is the entire intermediate vector $a^c$, {\it not} divided into subsets according to the fine-level neighborhood used to create $a^c$; the coarse level will create its own neighborhoods.
    \item {\it Recursion:} executing one or more multilevel cycles for approximately optimizing the coarse network; except that the {\it coarsest} level, employ relaxation only. Note that for more than two elvels the recursion should use FAS (see Sec.~\ref{sec:fas}).
    \item {\it Interpolation:} as in the case of the fine-to-coarse projection $Q$, a great simplification and efficiency (supported by Conclusion II in App.~\ref{sec:x}) can be gained by choosing the coarse-to-fine interpolation ($P^*$ in (\ref{8.6})) to be the same at all output sheaves, determined by a corresponding interpolation derived for the corresponding input activations; namely,
    \begin{equation}
        P^* = \argmin \sum_p \sum_{m \in B} \left\{ a^m_i(p) - \sum_{k \in Z_i} P^*_{ik} a^{cm}_k(p)e\right\}^2\,.
        \label{pstar_fit}
    \end{equation}
    As explained in Sec.~\ref{sec:5.3}, the stencil $Z_i$ could generally include activations not just from the neighborhood of $a_i$, but also from neighboring neighborhoods.
\end{enumerate}

\section{S7}
\subsection{Relaxation: SGD for Low-Noise Combinations}
\label{sec:sgd_relaxation}
As explained in Sec.~\ref{sec:5.6} relaxation should be designed separately in each neighborhood, and should consist of SGD in terms of the low-noise normalized combinations of parameters belonging to the neighborhood. It is important that the relaxed combinations (nearly) span the space orthogonal to the high-noise combinations used for in coarsening described above (Sec.~\ref{sec:8.1}). For this purpose, the same SVD of each input-activation neighborhood is used for designing both coarsening and relaxation, the higher combinations (the high-noise ones, i.e. those corresponding to large SVD singular values) defined $Q$ and the rest -- the lower combinations, are used in relaxation; thus the same relaxation is used in all output sheaves. In the case of larger neighborhoods, economical approximate SVD should be used, as in Sec.~\ref{sec:5.4} So, the picture of relaxation is the same as Fig.~\ref{fig:2}, except that $a^c$, the vector of low-noise combinations, denoted $a^f$, and the steps of calculating the relaxation is as discussed for (\ref{8.5}), with $f$ replacing $c$,

\section{S8}
yielding the relaxation correction $\delta w^f$. This correction is added to the current weights $W$ using (\ref{8.6}) with $\delta w^f$ instead of $\delta w^c$, and (\ref{8.7}) with $a^{fm}_k$ replacing $a^{cm}_k$.

Since all the combinations participating in the relaxation have uniformly-bounded low noise, a uniform averaging length can be used which yields a sufficiently good approximation to all their gradients (see App.~\ref{sec:B}). Then this low-noise relaxation becomes similar to relaxing a deterministic system. So (as generally described in Sec.~\ref{sec:5.5}), in case relaxation slows down before reaching small gradients for all the relaxed combinations (small compared with the current high-noise gradients), the low-noise processing can e,ploy multilevel acceleration similar to a deterministic acceleration.

\section{T1}
Bottom-up agglomerative algorithms (see \cite{17} for a comprehensive study) can be used when $n$ is not too large, but for large $n$ their quadratic $O(n^2)$ complexity makes them inefficient. Moreover, bottom-up approaches build on strong {\it binary} correlations, but in may cases no such correlations exist while there still exist larger mini-clusters that can be coarsened (having small CR for small REE). A top-down algorithm, such as RMC described below, should then be used to construct suitable miniclusters. With its nearly linear complexity it can in particular be used to {\it find} the strong binary correlations (in the case that they exist by not apriori known), if needed.

\section{T2}
\subsection{G.1 Recursive Mini Clustering: $S^1 = \text{RMC}(S^0)$}
Given a set $S^0$ of vectors in $\Real^m$, our aim is to construct a set $S^1$ of many disjoint miniclusters whose union is $S^0$. By ``minicluster`` we mean a small subset of highly correlated vectors.

RMC first divides $S^0$ into $k$ (a small number) subsets $(S^0_1,S^0_2,\dots,S^0_k)$ using a standard k-means algorithm \cite{kmeans}, and defines the first approximation to $S^1$ as the union of miniclusters of all subsets. \footnote{Unclear. Miniclusters of $S^0_i$ have not yet been defined here.} This $S^1$ may not be good enough because the k-means division may have placed many close neighbors (highly correlated vectors) in different subsets. An improvement to $S^1$ is obtained by redefining the miniclusters using k-means within each super-minicluster, where the super-miniclusters are obtained by applying RMC to the set $S^1$. (Each member of $S^1$ is not a vector but a mini cluster of vectors, so for RMC it is represented by its center, i.e., the average of its vectors). Indeed, additional improvement steps of this type can be further applied.

{\bf Notation.}
\begin{itemize}
    \item $S^0 = \left\{x_i\right\}_i$: the given set of vectors, $x_i \in \Real^m$.
    \item $S^1 = \left\{x_i^1\right\}_i$: set of miniclusters.
    \item $S^2 = \left\{x_i^2\right\}_i$: set of miniclusters of miniclusters. \footnote{Also denoted super-miniclusters above, which is confusing to a reader.}
    \item $(S^0_1,S^0_2,\dots,S^0_k)$ : subsets of $S^0$.
\end{itemize}

% TODO: replace by algorithmic environment.
{\bf Function $S^1 = \text{RMC}(S^0)$.}
\begin{enumerate}
    \item If $|S^0| < k_1$ or $\text{radius}(S^0) < R_1$, then assign $S^0$ as  a mini-cluster. Otherwise:
    \item $k = \min\left\{k_2, \ceil*{\frac{|S^0|}{k_1}} \right\}\,.$
    \item $\left\{S^0_1,S^0_2,\dots,S^0_k\right\} \longleftarrow \text{k-means}\left(S^0\right)\,.$\footnote{The k-means algorithm here is missing some descriptive details. What is the initial guess for] the centers? How many iterations do we perform?}
    \item $S^1 \longleftarrow \bigcup_{i=1}^k \text{RMC}\left(S^0_i\right)\,.$
    \item $S^2 \longleftarrow \text{RMC}\left(\text{centers}\left(S^1\right)\right)\,.$
    \item $S^1 \longleftarrow \bigcup_{x_j^2 \in S^2} \text{k-means}\left(\bigcup_{x_i^1) \in x^2_j} \text{center}x_i^1 \right)\,.$
    \item Possibly repeat steps 5--6 several times.
    \item Return $S^1$\,.
\end{enumerate}

\section{T5}
\subsection{G.2 Operation Count}
Let $\mu$ be the number of $S^1$ improvement iterations (steps 5--6) applied. Assume for simplicity that $k_1 = k_2 = k$, and that $k$ and $\mu$ are constant at all RMC levels. Let $a k n$ and $b k n$ be the number of operations in the k-means of Steps 3 and 6, respectively. Let $f(n)$ be the operation count of $\text{RMC}(S^0)$. The RMC recursion implies
$$ f(n) = k f\left \frac{n}{k} \right) + a k n + \mu \left(f\left( \frac{n}{k} \right) + b k n\right)\,, \quad f(k^2) = b k ^3\,. $$
Defining $g(n) = f(n)/n$, one gets
\begin{equation}
    g(n) = \left(1 + \frac{\mu}{k}\right) g\left( \frac{n}{k} \right) + (a + \mu b) k\,,
    \quad g(k^2) = b k\,.
    \label{eqn:G.1}
\end{equation}
Assuming $n = k^{\nu}$ and denoting $h_{\nu} = g(n) = g(k^{\nu})$, $r = 1 + \mu/k$ and $A = (a + \mu b)k$, we get
\begin{equation}
    h_{\nu} = r h_{\nu-1} + A\,,\quad h_2 = b k\,,
    \quad g(k^2) = b k\,,
    \label{eqn:G.2}
\end{equation}
which for $\mu > 0$ implies
\begin{equation}
    h_{\nu} = r^{\nu-2} \left( b k + \frac{A}{r-1} \right) - \frac{A}{r-1}\,, \quad \nu \ geq 2.
    \quad g(k^2) = b k\,,
    \label{eqn:G.3}
\end{equation}
Since $\nu = \log n / \log k$, choosing $k$ such that
\begin{equation}
    k \log k \geq \mu \left( \log n - 2 \log k\right),,
    \label{eqn:G.4}
\end{equation}
it follows that $r \leq 1 + \frac{1}{\nu-2}$ and hence $r^{nu-2} \leq e$. Then
\begin{equation}
    g(n) = h_{\nu} \leq e b k + \left( \frac{a}{\mu} + b \right) (e-1) k^2\,.
    \label{eqn:G.5}
\end{equation}
For example, for $\mu = 1$ and $n = 10^3, 10^6, 10^9$ and $10^{12}$, (\ref{G.4}) is satisfied by $k = 4,6,8$ and $10$, respectively. So (\ref{G.5}) yields for $\mu = 1$
\begin{align}
    g(10^3) &\leq& 27.5 a + 38.4 b \\
    g(10^6) &\leq& 62 a + 79 b \\
    g(10^9) &\leq& 110 a + 132 b \\
    g(10^12) &\leq& 172 a + 205 b\,.
    \label{eqn:G.6}
\end{align}
For $\mu = 0$, if we choose $k = 10$, (\ref{G.2}) entails
\begin{equation}
    g(10^{\nu}) \leq 10 (\nu - 2) a + 10 (\nu - 1) b\,.
    \label{eqn:G.7}
\end{equation}
To emphasize the smallness of $a$ and $b$, note that there is no need to fully converge each of the k-means calculations. We are only interested in the smallness of the clusters' average radius, which does not significantly change after just few k-means iterations.

\subsection{G.3 Further Efficiency Notes}
In many (perhaps inmost practical) problems, although the overall dimension $m$ may be high, at each scale of the problem the data is {\it locally low dimensional}. For such problems, even though the local low dimensional space may not explicitly be given, the RMC algorithm should work well even with $\mu = 1$. This is because at low dimension the k-means clusters would have many more points in their interior than close to their boundary, so only few close neighbors will end up belonging to different k-means clusters.

Note that to be successful, the algorithm need not mini-cluster together all pairs of strongly coupled vectors. {\it Particularly} strongly coupled pairs will be miniclustered together with high probability, but many other strongly coupled vectors may not be miniclustered together; they will however belong to strongly coupled miniclusters that will subsequently be clustered together at a higher coarsening level.

In RMC applications that aim at larger miniclusters (more than just a couple of vectors per minicluster) the correction (steps 5--6 of the algorithm) is not likely to significantly change the average cluster radius, so $\mu=0$ can safely be used.

Also, each k-means procedure, especially when larger clusters are sought, can be much accelerated by a {\bf multilevel} approach which first finds cluster centers using only a small random subset instead of the full set of vectors, then improve the center positions using a larger random subset, etc. Only one iteration needs to be performed with the full set of vectors, so the overall work need not be larger than the work of two full k-means iterations.\footnote{Provided that the random subset is doubled at each level of this multilevel procedure.}

Finally, much further acceleration can be gained by {\bf random-projection hashing and quantizations.} Indeed, at the first RMC level, where k-means subdivides the entire set of vectors into few large subsets, low accuracy is sufficient in measuring distances. One can then use a random projection that represents each vector by a much shorter vector, and that short vector can be quantized to a much lower precision. As one proceeds to progressively finer clusters higher accuracy is required, but each vector can then be represented by its difference from the parent-cluster center. Relative to that difference, low accuracy is again sufficient, again allowing representation by shorter and quantized vectors.

Importantly, note that this approach can also yield a {\bf very compact storage}, enabling fitting very long inputs into memory. Each vector is represented by its hashed-and-quantized distance from its minicluster center; tje centers are similarly represented by their hashed-and-quantized differences from the center of the parent cluster; etc.

Note that the RMC algorithm is {\bf highly parallelizable}.

\subsection{G.4 Hidden Miniclusters}
Let again $S^0 = \left\{x_i\right\}_i$ be the set of input activations, $x_i \in \Real^m$. We search for an {\it ``association matrix''} $T \in \Real^{\bar{m} \times m}$ such that the set $\left\{T x_i\right\}_i$ has as good as possible aggregates (miniclusters), by which we mean as small as possible Lagrangian $L_a = E + \lambda n$, where $E$ is a chosen error metric, $n$ is the number of miniclusters, and $\lambda$ is a chosen Lagrange multiplier.

$E$ and $\lambda$ depend on the use case Denoting $u_i = T x_i$, a typical example of $E$ is the ratio
\begin{equation}
    E = \sum_i \left\| u_i - C_{I(i)} \right\| / \sum_i \left\| u_i \right\|
    \label{clustering_error_metric}
\end{equation}
with some chosen norms, where $C_{I(i)}$ is the center of the $I${\it th} minicluster to which $u_i$ belongs. (In calculating derivatives of $E$ one can consider the denominator of (\ref{clustering_error_metric}) and the center $C_I$ to be frozen.) \footnote{Wouldn't it make sense then, in order to define a consistent optimization problem, to define $E$ with a frozen denominator and centers, not just the derivatives of $E$?} The Lagrange multiplier $\lambda$ can be used as a control hyperparameter: choosing lower $\lambda$ would yield smaller \footnote{For smaller $E$, or for the same value of $E$, do we obtain a larger $n$?} $E$ for larger $n$.

Another hyperparameter that depends on the use case is $\bar{m}$, the dimension of the vectors $u_i$. Smaller $\bar{m}$ is desired for compute, storage and data efficiencies, but too small $\bar{m}$ may not give coarsenable aggregates (i.e., small $\text{CR}_{\text{local}}$ for small $\text{REE}_{\text{local}}$; see Sec.~\ref{sec:5.2}).

We this get a combination of two minimization problems, each having its own parameters: minimizing the overall loss functional $L(w)$ and minimizing the Lagrangian $L_a(T)$. We can iteratively solve both of them using the same forward feeding of samples, but with separate backprojections of gradients. We expect $T$ to converge faster and be better initialized by continuation (see App.~\ref{sec:continuation}), since it is less dependent on the specific task, and its backpropagation is only within one layer. Then, as soon as $T$ starts to converge, the revealed hidden aggregates with start helping to produce the right coarse variables for accelerating the convergence of $W$.

\subsection{G.5 Multiple Associations}
Different initializations may yield different association matrices. One should accept all those that yield low values of $L_a$. Each of them would contribute to accelerating the minimization of the loss $L$. Each of them may also lead upon upscaling to the emergence of different high-level concepts. They may be trained and applied simultaneously, in parallel to each other.

A relatively fast multilevel hunt for multiple good (having small $L_a$) association matrices can proceed as follows. Begin by producing many association matrices with small width $\bar{m}$, each starting from a random $T$ followed by relaxation (possibly multilevel-accelerated) with relatively large $\lambda$ (producing partitions into few large miniclusters). Throw out all cases except those that have ended up with a small $L_a$.

For the next level, {\it double} the matrix width $\bar{m}$. Instead of starting from random, each $T$ calculation will start by concatenating the rows of a random pair of association matrices of the previous level. Relax (possibly with multilevel acceleration) now with a reduced value of $\lambda$ (creating finer miniclustering). Throwing out again all the matrices that do not converge to a clustering with small $L_a$, proceed to the next level, again doubling the width $\bar{m}$; and so on. Stop at the first level that yields good coarsenable aggregates (small CR for small REE).

(This is just a preliminary outline of multiple association algorithms. Furthermore, an empirical study is required.)
































%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Bibliography.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\bibliography{msgd}
\end{document}































