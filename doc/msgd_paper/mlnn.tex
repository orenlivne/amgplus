%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Multiscale optimization paper.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass{article} % For LaTeX2e
%\documentstyle[nips14submit_09,times,art10]{article} % For LaTeX 2.09

%--------------------------------------------------------
% Packages.\dfrac{•}{•}
%--------------------------------------------------------
%\usepackage{nips15submit_e,times}
\usepackage[toc,page]{appendix}
\usepackage{hyperref}
\usepackage{url}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{subcaption} 
\usepackage{mathtools}
%\usepackage{xcolor}
%\usepackage{soul}
\usepackage{graphicx}
\usepackage[disable]{todonotes}
\usepackage{changebar}
\include{aliases}

% Number equations A.1, A.2, etc.
\counterwithin*{equation}{section}
\renewcommand{\theequation}{\thesection.\arabic{equation}}
%--------------------------------------------------------
% Title.
%--------------------------------------------------------
\title{Multilevel Extensions and Multilevel Neural Networks}
%\author{
%Achi Brandt\\
%Oren E. Livne \\
%}
\date{\today}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Paper starts here.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}

\maketitle
%\todototoc
%\listoftodos

%\begin{abstract}
%\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}
\subsection{Motivation}
The current approach to design, train and deploy Neural Networks (NNs) involves several challenges:

\begin{itemize}
    \item \textbf{Collecting} and annotating enough training data is hard and costly.
    \item The networks contain many millions, often even billions, of parameters and operations, resulting in a high \textbf{inference cost} that limits their deployment on limited-resource devices.
    \item The larger the model (having more parameters) the slower {\it per parameter} is its {\it training} (having many more iterations, each going through all the parameters).
    \item Large models require much more computational resources that generate very significant carbon footprint.
    \item The \textbf{prediction accuracy} can be very limited. Training algorithms often fail to converge to some important solution components. When converged, the solution can often be remarkably brittle and sensitive to tiny input perturbations.
 \end{itemize}
 
All these difficulties are related in part, directly or indirectly, to one basic problem: the flatness of the optimization algorithms, the fact that all operations are done at the lowest level of separately updating individual parameters. Instead, an efficient treatment of any system with such a large number of parameters must include hierarchical operations based on information summarized from a hierarchy of increasingly larger aggregates of parameters. This is a principle whose existing realizations for various problems in different areas of computational science and engineering are many and diverse, documented in thousands of papers and dozens of international conference proceedings. 

The prime examples of this principle are {\it multigrid (MG) algorithms} for solving Partial Differential Equations (PDEs) discretization on structured grids \cite{?} and their extension to {\it Algebraic Mutigrid (AMG) algorithms} for for solving similar problems on unstructured graphs (see for example the biennial Copper Mountain Conferences on Multigrid Methods, the annual Algebraic Multigrid Summits, the European Conference on Multigrid Methods, and more.)

The slowness observed in most iterative solvers of large-scale scientific and industrial problems has much in common with the one typically removed by multigrid and AMG algorithms. But for multiple reasons a lot of these problems cannot be treated by traditional MG/AMG methods. (See in particular in Sec.~\ref{sec:6.1} below a list of such reasons in case of the NN optimization problem.) This has motivated new extensions of the AMG methodology, enabling potential applications to many new areas.

The general principles of these extensions are described in {\it Part I} of this article. Their possible application to NN optimization are then outlined in {\it Part II}.

\subsection{Main Concepts}
Part I of this article can first of all serve as a simple introduction to the multigrid and AMG underlying ideas. The presentation is from a more general point of view than the traditional multigrid presentation in which the coarsening (approximating the error on a coarse grid) has always been based on the {\it smoothness} of the error, rapidly obtained by few relaxation (gradient descent) passes. Instead of smoothness, more general correlations between neighboring parameters are used to construct a lower dimensional approximation to the post-relaxation error function. This allows extending the methodology

to more general classes of linear and nonlinear problems, including problems with non-local equations, optimization problems with no explicit system of equations, indefinite problems, particle and macro-molecular problems, neural networks, and more.

Relative to the established multigrid and AMG algorithms, the main general-principle innovations (presented mostly in Part I, and also in Sec.~\ref{sec:16} and some appendices) are the following:
\begin{enumerate}
    \item[(1)] Defining parameters as {\it neighbors} is based on their strong correlations over a set of examples (test functions obtained by relaxation) rather than on their geometric or graph-based proximity. A new general {\it ``mini-clustering''} algorithm can be more generally used to partition the set of parameters into many (usually small)\footnote{Omitted close as small already implies small radius.} {\it neighborhoods}. The neighborhoods can then similarly be mini-clustered\footnote{Why do we need to keep mini-clustering neighborhoods? Doesn't the first mini-clustering algorithm already give us all levels of neighborhoods as it's working its way top-down?} into neighborhoods of neighborhoods, and so on, creating hierarchical {\it multilevel neighborhoods}.
    \item{[2]} Defining the coarse-level variables in terms of local Singular Value Decomposition ({\it local SVD}), with possibly several (instead of just one) coarse variables being combined to represent each neighborhood. For large neighborhoods, a particularly {\it economical\footnote{Seems more on-point than cheap here.} SVD} variant is introduced.
    \item{[3]} In case of {\it stochastic} optimization, each SVD is applied to a matrix of the gradients of a neighborhood for a batch of independent samples. It provides a transformation of the neighborhood parameters into a new set of variables that can be split into two subsets: a subset of high-variance variables that will serve as the next-coarser-level variables, and a complementary subset of lower-variance variables that allows fast relaxation (needing only short averaging).
    \item{[4]} {\it Data efficiency.} Since at each scale the multiscale (deterministic or stochastic) solver the coarsening can be designed to be {\it essential local} (at that scale), it can be guided by relatively few training examples. Each global example supplies {\it many} local examples.
    \item{[5]} {\it Memory efficiency.} In the case that the input data is a long sequence of vectors (scalars being a special case), it may be enough to store the difference of each vector from the center of its neighborhood, then similarly store the difference of each center from the center of the higher-level neighborhood to which it belongs, etc. These differences can be represented at much lower relative accuracy, using quantization and random projections to lower-dimensional vectors (see App.~\ref{sec:G}).
    \item{[6]} {\it Hidden neighborhoods} can be discovered based on transformed sets of variables. They are as important as the plain neighborhoods for the purposes of solver acceleration through coarsening, data efficiency, memory efficiency and upscaling. 
    \item{[7]} {\it Upscaling} is a process in which the fine level of a multilevel solver is used to train a coarser level to be an independent representation of the system, no longer depending on interacting with the fine-level details by retaining the fine-level accuracy in features of interest (see Sec.~\ref{sec:upscaling}). Sequentially employing several levels of upscaling leads to very economic high-level representations and processing of systems.
    \item{[8]} A {\it mock cycle} is a general approach for {\it quantitative performance prediction}\footnote{missing end of sentence, perhaps add ``of solver efficiency''}, which can be very helpful in building and debugging the solver. It is much more general and more accurate than the traditional multigrid performance prediction by smoothing-rate analysis (see Sec.~\ref{sec:7}). 
\end{enumerate}
We call this general extended methodology AMG+, since it can be regarded as an extension of AMG. Due to its generality it can be applied to many important scientific and industrial optimization and stochastic optimization, with possible extensions to macro-molecular dynamics and more. Each new type of application would require its own understanding of locality, clustering, coarsening, levels/scales and high-level concepts.

In Part II we outline in detail how this methodology (needing all the innovations listed above) can be applied to the training of {\it Neural Networks}. A leading concept here is the organization of neighborhoods of the parameters (weights) in each layer in correspondence to neighborhoods of the input activations. This significantly simplifies the implementation of coarsening and interpolations, and yields a coarse-level network similar in structure to the fine level, allowing self-similar derivations of still-coarser levels. Some other types intra-layer coarsening are also briefly discussed, as well as inter-layer coarsening in which the number of layers is roughly halves at each coarsening level.

In Sec.~\ref{sec:15} we summarize the {\it potential benefits} of developing AMG+ for neural networks. Note that all the major steps of the algorithms are {\it highly parallelizable}.

In Sec.~\ref{sec:10} we note an existence of an element of coarsening in several popular NN techniques (batch normalization, dropouts, LoRA and transformer) and in neuroscience finding (the Hebbian rule), which can help explain their effectiveness. We discuss existing relations with our approach, and point out some possible improvement to those techniques suggested by these relations.

\subsection{Contents}
\tableofcontents

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage
\part{Multi-level Stochastic Optimization}
\label{sec:stochastic_optimization}

\section{Multilevel Solvers: Overview}
\section{A1}
{\bf Local and non-local equations.} Very efficient multilevel solvers (multigrid (MG) and Algebraic Multigrid (AMG) solvers) were traditionally developed for problems whose equations are {\it local} (except possibly for some ``global conditions''). A local equation $\phi_i(x)=0$ is one which involves few unknowns $_j$, all located in a small neighorhood in a low dimensional space (usually the physical space). Discretized linear and nonlinear differential equations (in particular Partial Differential Equations (PDEs)) are the prime examples of such problems. Particle systems with only local interactions is another example. Such problems are typically solved by MG or AMG in just few {\it ``work units''}, where a work unit is the amount of computer operations required to express all the equations. The hierarchy of levels employed by such solvers is related to a hierarchy of spatial scales: the higher the level, the latarget its spatial scale (or the average distance between variables) in each of its equations, while the equations are still local {\it at that scale} (each involving only few neighboring variables of that level).

Many problems defined in the physical (or other low-dimensional) space, even when their equations are {\it not} local (such as discretized {\it integral} equations, or particle systems with long-range (e.g., electrostatic) forces) can still enjoy fast multilevel solvers by using at each level {\it salient variables} (e.g., differences of the original variables) in terms of which the equations are {\it essentially local}, i.e. dependence on distant variables decays fast with their distance. For such problems, the great efficiency of the multilevel solver is the product of two different components:
\begin{enumerate}
    \item[(1)] {\bf Fast residual evaluation:} even though the original matrix of $n$ equations (the Hessian in nonlinear systems) is a full $n \times n$ matrix, approximately evaluating all $n$ residuals (gradients of the optimization objective) can cost only $O(n)$ computer operations when done in terms of the salient variables.
    \item[(2)] {\bf Rapid convergence} of the multilevel solvers, like those applicable to local equations.
\end{enumerate}
To extend the multilevel methodology to problems {\it not} defined in a low dimensional space (including the Neural Network optimization problems) an extended definition of locality\footnote{This is a much more common term than ``localness''.} is introduced, based on certain correlations between variables, as explained below (see Secs.~\ref{sec:5.2}, \ref{sec:?}).

For the sake of clarity, we introduce the basics of multilevel for deterministic as well as stochastic optimization problems, using a description that strictly applies to problems with local equations, deferring modifications (salient variables, generalized locality) to where they are needed.

{\bf General idea.} In solving an optimization problem, at each stage of the processing, the convergence rate of {\it deterministic} Gradient Descent (GD) methods is proportional to the Gradient-to-Error Ratio (GER). As observed in practically all large-scale optimization problems, in the first few GD iterations the GER may be large, yielding fast initial convergence. But then the GER progressively diminishes, resulting in a dramatic convergence slowdown. Furthermore, in the {\it stochastic case}, using Stochastic Gradient Descent (SGD) iterations, the required number of gradient samples one should average over to obtain the equivalent of just {\it one} GD iteration increases quadratically with the inverse of the size of the gradient. (Making updates after each fixed-size minibatch, this is achieved by diminishing the learning rates at least as fast as $O(\ger^2)$).

All low-GER error vectors approximately belong to a much lower dimensional subspace, which satisfies many linear relations that in the stochastic case translate into correlations. The GD slowness is due to disregarding these relations, and processing gradient terms independently of each other: if several error terms are strongly correlated, changing one of them without taking into account the changes in the others is very inefficient. To overcome this inefficiency, as soon as the convergence starts slowing down, the solver should start using \textbf{collective updates}, each simultaneously updating a cluster of highly correlated parameters taking their correlations into account. Including such updates in the iterations indeed yields fast convergence at first, but then (as the GER becomes even smaller) the collective updates also become increasingly correlated (to each other approximately residing in a still-lower dimensional subspace), so the algorithm should start including collective updates of collective updates. And so on.
	
The multi-level solver iteratively cycles between all the levels of a correlation-based hierarchy of collective updates. This hierarchy is usually constructed as a sequence of increasingly \textbf{“coarser”} (lower dimensional) spaces. The variables of each coarser level stand for certain averages of the next finer-level variables, and their gradients are correspondingly averages of the next-finer-level gradients. An \textbf{interpolation operator} from each coarse space to the next finer space spells out how each update of a coarse-level variable will translate into a collective fine-level update.

The intra-level processing at each of the levels of this hierarchy is called {\it relaxation} (GD or SGD is one possible choice of relaxation). A {\it cycle} for approximately solving the equations at each level (except the coarsest) is composed of relaxation steps, followed by transferring the current gradients to the next-coarser level; then the coarse-level solution {\it changes} based on these transferred gradients and interpolated and {\it added} as a correction to the current fin-level approximation; whereas the coarse level solution itself is recursively calculated by one or two {\it coarse-level cycles}, except the {\it coarsest level}, which is approximately solved just by relaxation.

Even for non-local equations (like NN optimization), at least to a certain degree, the relaxation at each level should achieve only local convergence (local to that level), hence each of its gradients can be estimated based on relatively few samples. This fact can add another big efficiency factor to the multi-level training, while also drastically reducing the amount of required training samples.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Definitions}
\label{sec:definitions}
The general optimization problem is to calculate
$$
	w^{min} = \argmin L(w)\,,\qquad w = (w_1, w_2,\dots, w_n)\,.
$$
Given an iterative solver we will denote by $w^{\infty}$ the solution it converges to, disregarding in the current discussion the question whether $w^{\infty} = w^{min}$. (In fact, as argued in App.~\ref{sec:additional}, with a suitable continuation method, in practice $w^{\infty}$ may anyway be the solution we are actually interested in, not $w^{min}$).

At any iteration, denote the current approximate solution by $w$, the current error by
\begin{equation}
	e = e(w) = (e_1, e_2, \dots, e_n) = w - w^{\infty}
	\label{error}
\end{equation}
and the current (true, deterministic) gradient by
$$
	g(e) = \left(g_1(e), g_2(e), \dots, g_n(e)\right)\,,\qquad g_i(e) = \frac{\partial L(w)}{\partial w_i} = \frac{\partial L(w)}{\partial e_i}\,.
$$
We further denote the current \textbf{Gradient-to-Error Ratio} (GER) by
\begin{equation}
	\ger(e) = \frac{\left\| g(e) \right\|}{H \left\| e \right\|}
	\label{ger}
\end{equation}
where $\|\cdot\|$ is the $l_2$ norm and $H$ is a normalization constant chosen such that $\max_e \ger(e) = 1$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Gradient Descent (GD) and Stochastic GD (SGD)}
\label{sec:gd}

Each GD iteration is the update
\begin{equation}
	w \longleftarrow w - \rho g\left(e(w)\right)\,,
	\label{gd}
\end{equation}
where the ``learning rate'' $\rho$ is a positive hyper-parameter that generally needs to be smaller than $3 / H$ to avoid fast divergence. In practice only an approximation to $g(e)$ is calculated at each GD iteration. in many large-scale scientific and engineering problems, where the equations are essentially local, the gradient calculation is an inexpensive $O(n)$ highly-parallelizable calculation (but not in other problems, including the NN training problem; see Sec.~\ref{sec:gd_local} below). Since (\ref{gd}) implies that in each iteration
$$
	e_{NEW} = e_{OLD} - \rho g\left(e_{OLD}\right)\,,
$$
the convergence factor per iteration is
$$
	\frac{\left\|e_{NEW}\right\|}{\left\|e_{OLD}\right\|} = 1 - O\left(\ger\left(e_{OLD}\right)\right)\,.
$$
As observed in practically all problems with local equations, in the first few iterations the GER is $O(1)$, yielding fast initial convergence. But then the GER progressively diminishes, resulting in a dramatic convergence slowdown, requiring a huge number of GD iterations to converge.

Furthermore, in problems with non-local equations the actual slowness is usually much worse, because as $\|e\|$ decreases and $\|g(e)\|$ decreases even much more, the work to approximately calculate $g(w)$ at each GD iteration sharply increases, typically proportionally to $\|g(e)\|^{-2}$.

This is in particular the case in stochastic optimization problems, where $g(w)$ is the average of its values over many samples:
$$
	g(w) = \frac{1}{M} \sum_{m=1}^M g^m(w)\,,
$$
where $g^m(w)$ is the gradient of the $m$-th sample, which can typically be calculated in $O(n)$ operations (e.g., in NN, by back propagation). The number $M$ of relevant samples is huge, so in each GD iteration an approximation to $g(w)$ is generally calculated by averaging over just $M_a$ random samples, $M_a \ll M$.

During the iterations, as the error tends to $0$, the average of each component, $g_i(e) = \frac{1}{M} \sum_{i=1}^M g_i^m(e)$ should tend to $0$, while its variance
$$
	\sigma_i^2(e) =  \frac{1}{M} \sum_{m=1}^M \left(g_i^m(w)\right)^2 - g_i(e)^2
$$
does not generally decrease much. Hence averaging over $M_i = O(\sigma_i^2(e) / |g_i(e)|^2)$ samples is needed in order to get a reasonable approximation to $g_i(e)$, so the equivalent of one GD iteration requires $O(n\|g(e)\|^{-2})$ operations.

In practice, the popular Stochastic Gradient Descent (SGD) algorithm averages gradients in each of its iterations over a minibatch of fixed size $M_b$, where $M_b \ll M_i$ for most of the components $i$. This yields a stochastic approximation to each $g_i(e)$, such that one needs to average over $O(M_i/M_b)$ such minibatch values in order to average out enough noise in $g_i(e)$. This is effectively obtained by reducing the learning rate $\rho$ by at least a factor of $O(M_b/M_i)$. So effectively, still $O(\|g\|^{-2})$ samples are averaged over to get the equivalent of one GD iterations. (See a more precise analysis in terms of eigen components in App.~\ref{sec:sgd_eigenmode}).


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Multilevel Gradient Descent}
\label{sec:mlgd}
While GD is very inefficient at asymptotic convergence, it is initially very efficient not only at reducing the error \textbf{size}, but also at vastly reducing its effective \textbf{dimension}. This is most easily seen by looking at the expansion of the error in term of eigen-components of the Hessian. Indeed, in local-equation problems most error components have large GER, comparable with the GER of random errors. Low-GER error components are more special, approximately satisfying the homogeneous Hessian equations, hence their number is much smaller. They therefore approximately comprise a much lower dimensional subspace: the lower their GER, the lower the dimension. (In non-local-equation problems this is true only upon suitable modification of the GD steps and the GER measure. See Sec.~\ref{sec:gd_local}.)  The low-GER error obtained by few GD iterations belongs to such a lower-dimensional subspace, except for also containing remnants (small amplitudes) of high-GER eigen components.  

Thus, for any $\gamma \ll 1$ there exists a subspace $W_{\gamma} \subseteq W$ of dimension $n_{\gamma} \ll n$, such that

\begin{equation}
	{\mbox{If }} \ger(e) \leq \gamma {\mbox{ then }} e \approx e_{[\gamma]} \in W_{\gamma}\,,
	\label{ger_gamma}
\end{equation}
where the smaller the cutoff $\gamma$, the smaller $n_{\gamma}$ and/or $\|e -e_{[\gamma]}\|$.

{\mbox{}}\\ \noindent
\underline{Multilevel Cycle} is any process that for some decreasing sequence of cutoffs
$$
1 = \gamma_0 \gg \gamma_1 \gg \gamma_2 \gg \cdots\,,
$$
performs several relaxation step (e.g., GD iterations) at each of the levels (subspaces)
$$ W = W_{\gamma_0}, W_{\gamma_1}, \cdots, W_{\gamma_L} $$
with appropriate information transfers between the levels.

{\mbox{}}\\
\underline{Multilevel Solver} is an optimizer composed of a (usually short) sequence of multilevel cycles. Each multilevel cycle usually reduces all error components by a large factor (typically between $2$ and $20$), because each  GD step in $W_{\gamma}$ can use a learning rate that effectively reduces all error components with $\ger = O(\gamma)$ -- without amplifying higher-GER components since they are absent in $W_{\gamma}$. In fact, in some important cases, just {\it one} cycle is needed to {\it adequately} solve the equations, e.g., to solve a discretized PDE to error smaller than the discretization error (assuming the initial approximation is obtained by solving the discretized equations at a coarser/simpler grid; see Sec.~\ref{sec:7} in \cite{4}).

After a full cycle, the remaining error is usually again dominated by high-GER components (their sizes being much smaller than in the pre-cycle error) which can then be efficiently reduced by the next cycle. (The remnants of high-GER components could not have as efficiently been reduced by making additional GD step in the first cycle, because in those additional steps the gradients would no longer be dominated by the high-GER components).

It is important to note that, since subsequent cycles can further clean any left error, the determination of the subspaces $W_{\gamma_k}$ need not be exact. The construction of these subspaces (their variables and their equations) and the inter-level information transfers is the main subject of this whole article.

The general approach for constructing the coarse-level {\it variables} (approximating the fine-level {\it error}) and the coarse-to-fine interpolation (providing the correction {\it added} to the current fine-level solution) is described in the following subsections. The definition of the coarse-level equations is discussed in Sec.~\ref{sec:6}. These equations are solved recursively, i.e., by relaxation and corrections from the next-coarser level.

\subsection{Building the Coarse Subspaces: SVD}
\label{sec:w_gamma}

Eq.~(\ref{ger_gamma}) implies that there exists a \textbf{projection} matrix $Q_{n_{\gamma} \times n}$, and an \textbf{interpolation} matrix $P_{n \times n_{\gamma}}$, such that

\begin{equation}
	{\mbox{if $\ger(e) \leq \gamma$ then $e \approx P Q e$.}}
	\label{pqe}
\end{equation}
Our task is to build such matrices, and such that it is inexpensive to multiply by them.

Suppose we already have $K$ error samples $e^{(1)}, e^{(2)}, \dots, e^{(K)}$ that satisfy
\begin{equation}
	\ger\left(e^{(k)} \right) \leq \gamma\,, \qquad k = 1, 2, \dots, K\,.
	\label{ger_ek}
\end{equation}
(See App.~\ref{sec:low_ger} for a description of how such examples can be created and improved throughout the solution process.)

We can then build a matrix $E^K \in \Real {n \times K}$ whose $k$th column is $e^{(k)}$. If $K$ is sufficiently large ($K \gg n_{\gamma}$), we can replace (\ref{pqe}) with the requirement that $E^K \approx P Q E^K$. For a given value of $n_{\gamma}$, the matrices $P_{n \times n_{\gamma}}$ and $Q_{n_{\gamma} \times n}$ that give the best approximation are obtained as follows.

\begin{theorem}[Eckart-Young-Mirsky]
Let the Singular Value Decomposition (SVD) of $E^K$ be $E^K = V \Sigma U^T$, where $V_{n \times n}$ and $U_{K \times K}$ are unitary matrices and 
$$
	\Sigma = \diag(s_1, s_2, \dots, s_n)\,,\qquad s_1 \geq s_2 \geq \cdots \geq s_n \geq 0\,.
$$
Then for a given $n_{\gamma}$, the Frobenius norm $\|E^K - P_{n \times n_{\gamma}} Q_{n_{\gamma} \times n} E^K\|_F$ is minimized by $P = Q^T = (V^{(1)}, V^{(2)}, \cdots, V^{(n_{\gamma})})$\,, where $V^{(k)}$ is the $k$th column of $V$.
\end{theorem}

See \cite{eym} for a proof. (See also in Sec.~\ref{sec:5.6} another interpretation to this coarsening $Q^T$). The ratio
\begin{equation}
    \rho = \rho_{\gamma} = \frac{\|E^k - P Q E^k \|^2_F}{\|E^k\|^2_F} =
    \frac{\sum_{i=n_{\gamma}+1}^n e_i^2}{\sum_{i=1}^n e_i^2}
    \label{ree}
\end{equation}
is called the Relative Energy Error (REE). Unfortunately, these SVD-based optimal matrices are not suitable for our purpose. First, because they are very expensive to derive, requiring $O(n^3 + K^2 n)$ operations. More importantly, they are dense, making matrix multiplication expensive. Fortunately, $P$ and $Q$ need not be optimal in any sense; in fact, as noted above, quite crude approximations are sufficient and generally inexpensive to derive, as explained next.

\subsection{Localization: Local SVD}
\label{sec:neighborhoods}
\text{Coarsening} is the approximate representation of all $n$ parameters ($w_1, w_2, \dots, w_n$ -- the \textbf{"fine"} level) as interpolants from a smaller set of $n_{\gamma}$ parameters (the \textbf{"coarse"} level). The ratio $n_{\gamma}/n$ is called the Coarsening Ratio (CR). As explained above, a short relaxation process at the fine level makes it possible to coarsen the fine-level error function with a small CR and small ERR. For problems with local equations, this relaxation is local, so the coarsening operator $Q$ can also be local. So instead of the global SVD described above, we may rather use {\it local SVD}, done separately in subsets of neighboring parameters, one subset ({\it ``neighborhood''}) at a time.

For systems of local equations, such as discretized PDEs, proximity\footnote{``neighborness`` is unclear.} is usually defined by the system graph, i.e., neighbors are variables that appear in the same equation (possibly plus neighbors of neighbors, etc.). But even for discretized PDEs, this definition is not general enough and sometimes inadequate (defining as neighbors parameters that should not be coarsened together). For the purpose of coarsening, a more general definition that is adequate for local as well as non-local equations, should be tied to the relaxation process (specifically to the low-GER errors produced by it), as follows.

Suppose again we acquired a set of $K$ {\it test vectors}, i.e. post-relaxation error examples $e^{(1)}, e^{(2)}, \dots, e^{(K)}$ that satisfy (\ref{ger_ek}). For each parameter $w_i$ this set defines a vector
$$
	E_i = \left( e^{(1)}_i, e^{(2)}_i, \cdots, e^{(K)}_i \right)\,.
$$
Parameters $w_i$ and $w_j$ are considered \textbf{neighbors} to the extent that they are \textbf{correlated}, i.e., that the distance
\begin{equation}
	d_{ij} := 1 - \corr(E_i, E_j)
	\label{dij}
\end{equation}
is small, where correlation is defined by
\begin{equation}
    \corr(a, b) := \left[\frac{\cov(a, b)^2}{\var(a) \var(b)}\right]^{\frac12}\,.
\end{equation}
Consequently, we assume we can partition the set of $n$ parameters into small neighborhoods, i.e., many clusters called {\it aggregates}, each containing few highly correlated parameters.  (We discuss below the partition to neighborhoods and the case where the neighborhoods cannot be small.)

Let $P_{\nu}$ and $Q_{\nu} = P_{\nu}^T$ be the SVD-based interpolation and projection matrices of the $\nu$th neighborhood, respectively. Reordering the $n$ parameters into a sequence of neighborhoods, we can then construct the complete $P$ and $Q$ as block-diagonal matrices with blocks $P_1, P_2, \dots$ and $Q_1, Q_2, \dots$, respectively. The column dimension of each $Q$ (i.e., number of coarse variables in a neighborhood) may vary.

The partition into neighborhoods can be done in various ways, but should satisfy the following rule. The local SVD of nearly every neighborhood should yield a moderately small CR (e.g., $CR_{\text{local}} \leq 0.5$) for small REE (e.g., $\rho_{\text{local}} \leq 0.4$) so as to obtain a small overall coarsening ratio (e.g., $CR_{\text{global}} \leq 0.7$; the smaller the better of course) while ensuring reasonably good coarsening accuracy. $\rho_{\text{local}}$ need not be very small because, as soon as the overall set of coarse variables is sufficiently representative, high-accuracy interpolation can be obtained as described in Sec.~\ref{sec:5.3}.

Restricted by this rule (small $CR_{\text{local}}$ for small $REE_{\text{local}}$), we want every neighborhood to have as few members as possible. This is needed for the sake of coarsening sparsity (sparse $Q$). Even more important, the smaller the neighborhood the smaller the number $K$ of test vectors required to make its local SVD representative sufficient. (Notwithstanding, see Sec.~\ref{sec:5.4} regarding inexpensive SVD in case the neighborhood is not small enough.)

There is still a lot of freedom in constructing the neighborhoods, a process also called {\it localization} or {\it aggregation}. Usually one starts with partitions natural to the problem (like uniform partitions of uniform grids), possibly followed by subdividing them by other methods. One general top-down/bottom-up method is the Recursive Miniclustering (RMC) algorithm described in App.~\ref{sec:miniclustering}. In the case of NN optimization, much simplification and efficiency is obtained by showing that the aggregation of parameters can be performed in terms of aggregation of activations (see Sec.~\ref{sec:8}). The final quality of the coarse variables (chosen for each neighborhood by the local SVD) is judged by the Mock Cycle described in Sec.~\ref{sec:7.3}.

Situations are likely to arise where no good and small enough neighborhood can be found but a slowing down of the relaxation convergence indicates that such neighborhoods should exist and come into view upon suitable transformations (linear recombinations) of variables. It is important to seek out such transformations not only for the purpose of solver acceleration, data efficiency and memory savings (explained in Sec.~\ref{sec:8.3}); neighborhoods (hidden or not) also enable upscaling (see Sec.~\ref{sec:upscaling}), which can reveal higher ``concept`` of the system. A possible approach for hunting for the hidden neighborhood is outlined in App.~\ref{sec:G.4}.

\subsection{Improved Interpolation}
\label{sec:improved_coarsening}

For a given desired coarsening ratio CR the relative energy error of the local SVD, called $REE_{\text{local}}$, is usually much larger than the global REE with the same CR, mainly because the interpolation stencil is restricted to the local neighborhood (whose size we want to minimize, as explained above). This problem can be mitigated: while keeping the coarse variables defined by $SVD_{\text{local}}$, the interpolation can be replaced by a higher-accuracy interpolation $P_{HA}$ by allowing each of the fine variables to be interpolated not just from the coarse variables of its own neighborhood, but also from the the coarse variables of \textbf{neighboring neighborhoods}. $P_{HA}$ is built by Ridge-regularized least-squares fitting to $e^{(1)}, e^{(2)}, \dots, e^{(K)}$. (The regularization term is proportional to a weighted sum of squares of interpolation coefficients, to avoid over-fitting). Again, in the NN case, much simplification and efficiency can be gained by defining $P_{\text{HA}}$ in terms of activation statistics, then applying it to the parameters (see Sec.~\ref{sec:8}).

Since accuracy can be tuned by adjusting the interpolation $P$, the projection $Q$ may be simplified. For instance, in many cases $Q_{\text{local}}$ may be replaced by a one representative member of the neighborhood. The objective is often to sparsify $Q$ as much as possible. For example, in the NN applications discussed in Part~\ref{sec:nn}, $Q$'s sparsity is much more crucial than $P$'s sparsity.

\subsection{Economical Approximate Local SVD}
\label{sec:econonmical_svd}
If the obtainable neighborhood is not small enough (and one cannot reduce its size and still get a small CR for a small REE), then full calculation of the $\text{SVD}_{\text{local}}$ may become too costly. But full $\text{SVD}_{\text{local}}$ calculation is not really needed. For obtaining comparably efficient coarsening it is sufficient to derive a set of coarse variables that approximately span the same space spanned by the SVD coarsening.

As an example, for a neighborhood $B$ let $E^k_B \in \Real^{|B| \times k}$ be the matrix whose SVD we want to crudely approximate (i.e., $E^k_B$ is composed of the $|B|$ rows of $E^k$ corresponding to $B$). Then the rows of the local coarsening matrix $Q_B$ are the $l_B$ highest eigenvectors of the matrix $A_B = E^k_B (E^k_B)^T$, where $CR_B = l_b/|B|$ is the local coarsening ratio. The space spanned by these $l_B$ eigenvectors can be approximated by
$$ \text{span}\left\{u_1, u_2, \cdots, u_{l_B} \right\}\,,$$
where
\begin{equation}
    u_i = \left(lambda_1 I - A\right) \left(lambda_2 I - A\right) \cdot \left(lambda_q I - A\right) r_i\,,
    \label{ui}
\end{equation}
$(r_1, r_2, \cdots, r_{l_B})$ being independent random zero-mean vectors, and the set $(\lambda_1, \lambda_2, \cdots, \lambda_q)$ is spread over most of the interior of the range of {\it low} eigenvalues (corresponding to the eigenvectors that should {\it not} be included in the coarse level).\footnote{Why not just use $\lambda_i \equiv 0$, i.e., the power method?}\footnote{How do you estimate the range of low eigenvalues? I suppose using the mock cycle, checking which value gives good convergence} The number of multiplications $q$ need not be large ($q = 15$, say, should usually suffice). One can replace the set $(u_1, u_2, \cdots, u_{l_B})$ by an orthonormal set using the Gram-Schmidt process\footnote{The modified Gram-Schmidt (MGS) process is a small modification that can numerically stabilize the Gram-Schmidt process and produce smaller errors in finite-precision arithmetic.}

For the CGD relaxation (see Sec.~\ref{sec:5.5}) one can use the directions $(\bar{u}_1, \bar{u}_2, \cdots, \bar{u}_{|B|})$\footnote{Should this not be $l_B$ instead of $|B|$?} defined similarly to (\ref{ui}), except that the set $(\lambda_1, \lambda_2, \cdots, \lambda_q)$ should now be spread over most of the range of the $l_B$ {\it highest} vectors.

A good set of hyperparameters $(l_B, q, \left\{\lambda_i\right\}_i)$ can be identified using the Mock Cycle (see Sec.~\ref{sec:7.2})\footnote{That's a lot of parameters to optimize, especially if they are separate in every neighborhood and we optimize ALL of them together with the mock cycle. How can one run an efficient hyperparamter search?}. The cheap SVD calculations (one per each large neighborhood) should be done only once per many training iterations. It is particularly inexpensive in the case of Neural Networks, since it should be done there in terms of the {\it input activations}, not in terms of the {\it parameters} $w_i$ (as explained in Sec.~\ref{sec:8.1}).

\subsection{5.5 Relaxation Rules}
\label{sec:relaxation_rules}
% Top paragraph.
We continue to generally refer to the fine-level iteration (which in NN is normally GD; but we also explore alternative schemes) as \textbf{relaxation}, borrowing the jargon of multigrid PDE solvers \cite[Sec.~1]{guide}. We will use the term GD for a relaxation scheme that changes one parameter at a time in some proportion to its gradients. 

A first key question is whether it is possible to estimate how effectively GD, or an alternative relaxation scheme, \textbf{narrows the error}, i.e., reduces most of the error to a lower-dimensional space, even before a coarsening is chosen. 

This usually happens if the GD changes are \textbf{local}, i.e. only gradients of the strong neighbors of a parameter are significantly changed by the GD step of that parameter. Since such steps reduce components nearly independently in many local neighborhoods, this indirectly ensures that many error components are reduced and the remaining error is nearly low-dimensional. For instance, discretized PDEs \textbf{pointwise relaxation} \cite[Sec.~1]{guide} is local because of the equations' locality. In contrast, in discretized \textbf{integral equations}, updating a single parameter $w_i$ would introduce changes to the residuals at many $w_j$ far from $w_i$. To make the relaxation in this case still effectively local, so-called \textbf{distributive} relaxation is used instead \cite{lubrecht}; that is, simultaneously updating \textbf{several} neighboring parameters in a particular combination so that the effect on far parameters is negligible. Error narrowing is not equivalent to GER reduction in this case: in fact, integral equations are the inverse of differential equations, with {\it low-}GER components being ``local'' (in the sense that there is no correlation between their values at far points, almost like random errors), while high-GER errors are non-local (smooth). Generally, relaxation should narrow the error to reside in the lower-dimensional subspace of low \textbf{Response to Relaxation (RER)} errors. A simple general sufficient condition is for the relaxation to quickly reduce random components (whether they are high-GER or not).

Generally, if GD is not already local, locality can be achieved via the local neighborhoods and the coarse variables that represent each neighborhood described in Sec.~\ref{sec:neighborhoods}. Based on these neighborhoods, we can replace GD with \textbf{Compatible Gradient Descent (CGD)}: a modified GD scheme that keeps the coarse variables unchanged, and is therefore local. This can simply be done by gradient descent along the {\it lower} SVD directions, i.e. the SVD directions that will not be used as coarse variables. (See Sec.~\ref{sec:5.4} for the way to do it when the neighborhoods are not small enough.)

Similarly in {\it stochastic} optimization, if SGD is not local, it can be replaced by {\it Compatible SGD (CSGD)}, e.g., SGD along the lowest SVD directions. As mentioned above, this localization of relaxation (in as small neighborhoods as allowed by the CR-vs.-REE tradeoff) is important for enabling small $K$ in deterministic cases and small amount of averaging (hence data efficiency) in stochastic cases (see Sec.~\ref{sec:5.6}).

\subsection{5.6 Coarse Variables and Relaxation in the Stochastic Case}
While in the (linear or nonlinear) {\it deterministic} case the calculation of each test function $e^{(k)}$ can be quite involved (App.~\ref{sec:C} is an example), the {\it stochastic} case seems to have the advantage that each sample $m$ can serve as an independent example. But this is an example $g^m$ of the gradient $g$, {\it not} the error $e$, and it is {\it not} an example of the average $\frac{1}{M} \sum_{m=1}^M g^m$, i.e., not an example of the gradient of the true error.

However, correspondingly, in stochastic optimization the issue of relaxation slowness is different than in the deterministic case; it is dominated by the {\it stochastic slowness}, i.e. the long averaging of the gradient $g$ (over many samples $g^m$) needed to assess its true value (see Sec.~\ref{sec:4}). Fortunately, the SVD of a matrix $G^K = (g^{(1)},g^{(2)},\dots,g^{(K)})$ whose columns $g^{(1)}=g^{m_i}$ are gradients of independent samples (instead of $E^K$ above, whose columns $e^{(i)}$ are independent relaxed error vectors) is exactly what we need to treat this stochastic slowness. (And see below how in this framework the ill-posedness slowness is addressed.)

To see this, let's review in detail the role of SVD (either the global SVD or $\text{SVD}_{\text{local}}$ of one aggregation). In the {\it deterministic} case the SVD is
$$ E^k = V \Sigma U^T\,,\quad V, U \text{ are unitary}\,,\quad \Sigma = \diag\left\{s_1, s_2, \dots, s_n \right\}\,,$$
where $s_1 \geq s_2 \geq \cdot \geq s_n \geq 0$ are the ``singular values'' of $E^k$. The unitary matrix $V^T$ is used to rotate the set of parameters $w^T$ into a new set $\bar{w}^T = V^T w^T$; hence it rotates any error vector $e^T$ into a new vector $\bar{e}^T = V^T e^T$. So the $K$ error examples $E^K = (e^{(1)},e^{(2)},\dots,e^{(K)}$ are rotated to $\bar{E}^K = V^T E^K = \Sigma U^T$. Hence, since $U$ is unitary,\footnote{1) Missing $K$ superscript on $E_i$ in the equation below? 2) Missing vector norm inside the sum, i.e., $\left\|\bar{e}_i^{(k)}\right\|$?}
$$ \sum_{k=1}^K \left(\bar{e}_i^{(k)}\right)^2 = \bar{E}_i \bar{E}_i^T = s_i^2\,.$$
Thus, the $n$ rotated variables $(\bar{w}_1,\bar{w}_2,\dots,\bar{w}_n)$ are arranged in decreasing order of the $L_2$ norms of their errors in their $K$ examples, which is roughly a decreasing order of their convergence slowness in relaxation. So we choose the first $n_{\gamma}$ of them $(\bar{w}_1,\bar{w}_2,\dots,\bar{w}_{n_{\gamma}})$ to serve as our coarse variables.

In the {\it stochastic} case let the SVD be $G^K = V \Sigma U^T$, reusing the same notation Note that in this case, unlike the former, we should use $V$ (not $V^T$!) to rotate the set of parameters: $\bar{w}^T = V w$, because $V^T$ would rotate the corresponding gradient into $\bar{g}^T = V^T g$. Hence, similarly to the above deterministic derivation we get
$$ \sum_{k=1}^K \left(\bar{g}_i^{(k)}\right)^2 = s_i^2\,,$$
therefore the variance of $\bar{g}_i$ over the set of examples is
$$ \var(\bar{g}_i) = \frac{1}{K} \sum_{k=1}^K \left(\bar{g}_i^{(k)}\right)^2 - \left(\frac{1}{K} \sum_{k=1}^K \bar{g}_i^{(k)} \right) \approx s_i^2\,,$$
since the average of $\bar{g}_i$ should tend to $0$. Thus the rotated variables $(\bar{w}_1,\bar{w}_2,\dots,\bar{w}_n)$ are arranged in decreasing order of their variance, or {\it noise}. So the $n_{\gamma}$ leading variables $(\bar{w}_1,\bar{w}_2,\dots,\bar{w}_{n_{\gamma}})$ chosen for the coarse level are the higher-variance combinations, slower to converge in relaxation due to their higher noise. As explained in Sec.~\ref{sec:5.5}, the rest of the combinations $(\bar{w}_{n_{\gamma}+1},\bar{w}_{n_{\gamma}+2},\dots,\bar{w}_n)$ are those that should be used in the CSGD relaxation, to reduce its noise as much as possible.\footnote{But part of this logic does not make sense to me: the stochastic case should be an {\it extension} of the deterministic case, when the noise level is non-zero. So how come the algorithm for the stochastic case does not generalize the algorithm for the deterministic case? For instance, if we imagine that the batch size is gradually increased to infinity so we calculate the gradient more and more deterministically, what would happen in the algorithm? Or, is the stochastic case description referring to a different part of the overall algorithm that the local SVD is used for -- CSGD vs. coarsening, in which case the question is how do we coarsen in the stochastic case, not just how do we relax?}

This CSGD relaxation, even though restricted to low-noise combinations, may itself run into some slowness, since there may still be different RER sizes among the low-noise combinations (generalizing GER, see Sec.~\ref{sec:5.5}). The maximal RER ratio\footnote{It's unclear if the RER is already a ratio or what ratio is being referred to here.} among the low-noise combinations should be much smaller than the maximal ratio among the bare gradients, so CSGD slowness may still be mild and tolerable. {\it If not}, instead of applying relaxation to the low-noise combinations, one should apply to them multi-level cycles similar to the cycles applied in the deterministic case (as in App.~\ref{sec:C}). Indeed, relatively short minibatch averaging of the low-noise gradients should already yield a good approximation to their true gradients, thus making them amenable to the deterministic treatment.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Coarse-level Processing: Brief Survey}
\label{sec:coarse_overview}
Given the fine-to-coarse projection $Q$ and the coarse-to-fine interpolation $P_{\text{HA}}$, we still need to describe how the coarse-level solution is actually obtained. Usually we design the coarse level to be of the same kind and structure as the fine level, so its solution is recursively obtained in the same way (and the same code), i.e., by relaxation combined with corrections from a still coarser level. The coarsest level has few parameters and can usually be solved very efficiently just by relaxation.

The first multilevel algorithms were the multigrid solvers of discretized Partial Differential Equations (PDEs). In these solvers, the coarse-level equations are simply a coarse-grid discretization of the same PDE, except that their forcing terms (right-hand-sides) are composed of local averages of the fine-level \textbf{residuals} (gradients), so that the coarse-level solution is an approximation to the fine-level \textbf{error}. The coarse-level (approximate) solution is then interpolated to the fine level and added as a \textbf{correction} to the current fine-level approximate solution. The interpolation used was a simple multi-polynomial (e.g., linear) geometric interpolation, later modified by various ad-hoc rules to more accurately fit the given discrete equations \cite{mg_discontinous}). A suitable modification for \textbf{nonlinear} equations is the Full Approximation Scheme (FAS \cite{brandt77}, \cite[Sec.~8]{guide}; see NN example in \cite{sec:9} below).

Most optimization problems in science and engineering, even when they are not well-structured discretizations of PDEs, can still be cast as a sparse system of linear or nonlinear \textbf{explicit local} equations. The extension of the multigrid method to such schemes is called Algebraic Multigrid (AMG) \cite{amg_book}. Assuming the fine-level equations are linear (perhaps linearized equations around the current fine-level approximate solution $w_0^f$) they can be written as
$$A^f w^f = b^f\,,$$
where $A^f$ is a sparse matrix. Writing $w^f = w_0^f + e^f$, the error $e^f$ satisfies the equation
$$A^f e^f = b^f - A^f w_0^f := r^f\,,$$
where $r^f = -g(e^f)$ is called the {\it residual} vector. With the coarse-to-fine interpolation $e^f = P_{\text{HA}} w^c$, the coarse-level equations are chosen to be
$$A^c w^c = b^c\,,\text{where } A^c := P_{\text{HA}}^T A P_{\text{HA}}\,, b^c := P_{\text{HA}}^T r^f\,.$$
The interpolation $P_{\text{HA}}$ in such systems was first based on various ad-hoc rules similar to those mentioned above. The first systematic derivation of the AMG interpolation was called \textbf{Smoothed Aggregation} (SA \cite{sa}). It uses few GD steps over small overlapping neighborhoods, one neighborhood at a time, to find the approximate relations satisfied by a low-GER error in each neighborhood, when neighbors are defined by the system graph, i.e., neighbors are basically variables that appear in the same equation. SA uses one coarse variable $e_j^c$ per neighborhood; usually $e_j^c$ is the value of $e^f$ at a single variable of the neighborhood.

Another systematic approach, \textbf{Bootstrap AMG} (BAMG, \cite[Sec.~17.2]{review2000}), uses \textbf{global} GD iterations to create ``test functions'', i.e., examples that satisfy (\ref{ger_ek}), followed by the neighborhood-based derivation described above. The neighborhood may in many cases still be based on the system graph. But generally, a graph neighbor may sometimes be a false neighbor (being more strongly connected to a completely different neighborhood), and some neighbors may be much more correlated than others - which will all be suitably treated by the correlation-based neighborhoods described above. The BAMG approach also includes an optional iterative multilevel procedure to inexpensively construct ever more accurate (lower-GER) test functions (cf. App.~\ref{sec:low_ger}), as needed for some (e.g., nearly singular $A^f$) applications.

More fundamentally, the BAMG approach is applicable even when no explicit sparse system of equations, hence no system graph, is given. For example, when all that's available at the fine level is just the gradients $g_i = \partial L/\partial w_i$, we can still run relaxation and produce the examples $e^{(1)},\dots,e^{(K)}$ and then the coarsening operators $Q$, $P$ and $P_{HA}$. We can then calculate the coarse gradient components
$$
	g^c_j = \frac{\partial L}{\partial w^c_j} =
	\sum_i \frac{\partial L}{\partial w_i} \frac{\partial w_i}{\partial w_j^c} =
	\sum_i g_i P_{ij}, \quad {\mbox{i.e. }} g^c = P^T g\,.
$$
See Sec.~\ref{sec:direct_coarsening} for details of the steps in the stochastic case taken in this situation. For unlocking further benefits, it is important to create \textbf{autonomous} coarse-level iterations. This can be done, using the FAS approach, provided there is a way for \textbf{at least approximately} calculating $g^c$ independently of the fine level. See Sec.~\ref{sec:fas} for details in the NN case.

Sec.~\ref{sec:5} of the present article introduces new features to BAMG that have not appeared before. Most important is the {\it local SVD} procedure, which is a general way to define the coarse-level variables. It allows defining more than one coarse variable per neighborhood, as indeed necessary in many applications. So this opens up many new problems to multilevel solvers, including highly indefinite discretized PDEs (e.g., wave equations), systems of macromolecular dynamics and NN optimization.

\subsection{NN Training: THe Challenge}
The NN training problem is different in a number of fundamental ways from the optimization problems previously treated by multilevel approaches. In particular:
\begin{enumerate}
	\item The NN problem has \textbf{No explicit system of equations}; there is only a way to compute the residuals (the gradients of the objective function) of any given approximation. Thus there is no explicit graph of parameter proximity (neighborhoods).
	\item The residuals are measured \textbf{stochastically}: they are averaged over a random sequence of samples.
        \item The different layers of the NN systems have different properties that may require different coarsening at different training stages. The layers affect each other and cannot be studied independently, one layer at a time.
	\item The amount of averaging, hence  \textbf{cost} of measuring each residual, keeps sharply increasing upon approaching the solution.
	\item There are two types of coarsening for neural networks: \textbf{intra-layer and cross-layer} coarsening.
	\item The coarsening should be made for \textbf{two types of variables}: neuron activations and connection weights.
	\item \textbf{Non-uniqueness}: there are many, probably equally good, solutions, very different from each other.
	\item There are many local errors with low GER. (This is already the case in solving discretized {\it intergral} equations.)
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Two-level Convergence Quantitative Prediction}
\label{sec:mock_cycle}
We want to predict the potential efficiency of a given combination of a relaxation (e.g., GD or CGD) and a coarsening scheme, (i.e., the matrix $Q$, cf.~Sec.~\ref{sec:w_gamma}) \textbf{prior} to the actual construction of the interpolation matrix $P$ and all subsequent coarse levels. This allows us to \textbf{separate} the design of $Q$ from the design of $P$, and to obtain a \textbf{precise quantitative prediction} of the potential multi-level efficiency. This also \textbf{simplifies debugging:} if the predicted efficiency is not adequate, $Q$ should be replaced; if it is, but the actual cycle doesn't attain that prediction, $P$ is the culprit.

A general scheme to obtain this quantitative prediction is via a \textbf{mock cycle}: an iteration that mimics the true multi-level cycle, except that the coarse-level correction is replaced by the idealized step. {\bf A mock cycle run consists of the following steps:}
\begin{enumerate}
	\item Choose a particular problem that has a known solution $w_0$. For instance, $w_0$ can be the current solution at a certain optimization stage, and linear terms are added to the optimization objective $L$ (the loss) such that $g(w_0)=0$. Set $q_0 := Q w_0$.
	\item Choose an initial approximation $w = w_0 + \varepsilon$, where $\varepsilon$ is a suitable small perturbation vector. \footnote{In many systems, a random perturbation is good. In some cases including NN cases, a random perturbation is found to have a negligible effect on $L$'s value, so the two-level cycle becomes trivial.}
	\item Apply $\nu$ relaxation steps (typically $1 \leq \nu \leq 4$).
	\item Mock coarse-level correction: correct $w$ so that $Q w \approx q_0$. This can be implemented by a couple of Kaczmarz relaxation iterations \cite[Sec.~1.1]{guide} on the under-determined system of equations $Q w = q_0$. One sweep suffices for an exact solve in the case of no overlap between coarse neighborhoods. In any case, the iterations should converge very fast. Kaczmarz relaxation has the important advantage of producing the minimum-norm correction to $w$.
	\item Repeat steps 3 and 4, each repetition representing a mock cycle. Measure the rate of convergence per cycle of $w$ to $w_0$.
\end{enumerate}
This process has been called Habituated Compatible Relaxation (HCR) \cite[Sec.~14.3]{guide}, although this is a misnomer: we call it here mock cycle, so that the term "compatible" is reserved for the relaxation variant aimed at locality (see Sec.~\ref{sec:5.5}).

In the non-linear case, there may exist many solutions in the neighborhood of $w_0$, due to the presence of many Almost Zero Modes (AZMs) that may be added to it with little effect on the loss value. For instance, we have experimentally observed that in NNs, adding random components to a solution does not significantly change its accuracy. Thus, the mock cycle may not converge back to $w_0$, but to an alternate solution; the predicted multilevel efficiency should be the speed of convergence to \textbf{that} solution.

The mock cycle is easy to implement; compared with theory-driven predictions (e.g. LMA in geometric multigrid \cite[Sec.~2.1]{guide}), it is more generally applicable, and provides a more accurate prediction of the convergence rate obtainable in an actual two-level cycle that includes the same number $\nu$ of GD steps per coarse-level correction.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage
\part{Multi-level Neural Networks}
\label{sec:nn}

\section{First Coarsening Level of a Typical NN Layer}
\label{sec:nn_coarse1}

As discussed above, for the first coarsening we need to sub-divide the parameters (the weights $w$) of the network into small neighborhoods: mini-clusters of highly correlated parameters. Highly correlated parameters in neural networks must belong to the same layer, so the first NN coarsening levels will be done separately in each layer. (Independently, see Sec.~\ref{sec:skip_layers} about inter-layer coarsening.)

We will discuss a typical convolutional layer that has a 2D grid of input points and a corresponding 2D grid of out points. (A fully-connected layer is a particular case having just one out gridpoint). There are $N_{in}$ input channels, each having one activation at each input gridpoint, and $N_{out}$ output channels, each with one activation per output point. The set of input-to-output connection weights are the same at different output points; we denote them as follows.

At any given output point p=$(x_{out}, y_{out})$, for any given sample $m$, the $j$th channel activation ({\it before} the nonlinear activation gate, which is typically a ReLU \cite{Goodfellow-et-al-2016}), is denoted $\tilde{a}^m_j(p)$ is a linear combination
\begin{equation}
    \tilde{a}^m_j(p) = \sum_i w_{ij} a^m_i(p)\,,\qquad (j = 1, \dots, N_{out})\,,
    \label{tilde_aj}
\end{equation}
where $i = (i_0, i_1, i_2)$; $a_i = a_{i_0,i_1,i_2}$ is the $i_0$-th input channel activation at gridpoint $(x_{out}+i_1, y_{out}+i_2)$, and
$$
	i_0 = 1, \dots, N_{in}\,, \quad |i_1|\leq q\,,\quad |i_2| \leq q\,,
$$
so that $\sum_i$ is a sum over $I := N_{in} (2 q + 1)^2$ terms. $q$ is a non-negative integer; most often $q = 1$. The sets $S_i := \left\{w_{ij}\right\}_j$ and
$\tilde{S}_j := \left\{w_{ij}\right\}_i$ are called \textbf{input} and \textbf{output sheaves}, respectively (cf.~Fig.~\ref{fig:input_sheaf}--~\ref{fig:output_sheaf}).
\begin{figure}[htbp]
\centering
\begin{subfigure}[b]{0.55\textwidth}
  \includegraphics[width=3.3in]{output_sheaf.png}
   \caption{}
   \label{fig:input_sheaf} 
\end{subfigure}

\begin{subfigure}[b]{0.55\textwidth}
  \includegraphics[width=3in]{input_sheaf.png}
   \caption{}
   \label{fig:output_sheaf}
\end{subfigure}
\caption[Sheaves.]{(a) Output sheaf: the set of connections between input activations $\left\{a_i\right\}_i$ and a particular output activation $\tilde{a}_j$. (b) Input sheaf: the set of connections between a particular input activation $a_i$ and output activations $\left\{\tilde{a}_j\right\}_j$.}
\end{figure}
Let $g_{ij}$ be the gradient of $w_{ij}$, i.e.,
\begin{equation}
	g_{ij} := \frac{1}{M} \sum_{m=1}^M g^m_{ij} = 
	\frac{1}{M} \sum_{m=1}^M \frac{\partial L^m}{\partial w_{ij}} =
	\frac{1}{M} \sum_{m=1}^M a^m_i \sum_p a^m_i(p) \frac{\partial L^m}{\partial \tilde{a}_j(p)}\,,
	\label{gij}
\end{equation}
where $L^m$ is the loss of the $m$-th sample and $M$ is in principle the number of samples in the entire world. 

The relation (\ref{gij}) suggests that the strongest gradient correlations in a layer are intra-sheaf correlations and that the correlations within each $\tilde{S}_j$ approximate the correlations between the corresponding input activation \textbf{strengths}, while correlation within each $S_i$ similarly approximate correlations between output activation \textbf{gradients}. These observations (made more precise in App.~\ref{X}) motivate our coarsening approach.

%Below, when we refer to the correlation $\corr(a_i, a_{i'})$ between input \textbf{activations} $a_i$ and $a_j$, it is the correlation over \textbf{samples}, defined by
%\begin{equation}
%	\corr(x,y) := \frac{\lp x, y\rp}{\left(\lp x, x\rp \lp y, y \rp\right)^{\frac12}}\,,\quad
%	\lp x, y\rp := \sum_{m=1}^M x^m y^m\,;
%	\label{corr}
%\end{equation}
%(As (\ref{corr}) is defined for any vectors $x, y \in \Real^M$, so it applies to output activations $\tilde{a}_j$, $\tilde{a}_{j'}$ as well.) whereas the correlation between \textbf{parameters} $\corr(w_{ij}, w_{i'j'})$ refers to the correlation of the vector $e_{ij}$ of error values over different \textbf{low-GER examples} with the  vector $e_{i'j'}$ of values over the same examples (or equivalently, the correlation between the corresponding gradient vectors $g_{ij}, g_{i'j'}$).

We first describe here coarsening based on correlations within output sheaves $\tilde{S}_j$, because this coarsening is particularly inexpensive to create, store and manipulate, so it pays to use it in the first coarsening stage to produce much smaller networks that if needed can then be further coarsened by the more expensive types of coarsening. Coarsening based on {\it input} sheaves $S_i$, or simultaneously coarsening both input and output sheaves will be described next (Sec.~\ref{sec:11}).

\subsection{8.1 Coarsening}
\section{sec:coarsening}
Following Sec.~\ref{sec:relaxation_rules}, a coarse variable for the sheaf $\tilde{S}_j$ is a linear combination $W_{\alpha j} = \sum_i \alpha_i w_{ij}$ of its variables, whose gradient at sample $m$ is
$$ G^m_{\alpha j} = \sum_i \alpha_i g^m_{ij}\,, \\quad \sum_i \alpha_i^2 = 1 $$
and whose average size
\begin{equation}
    \frac{1}{M} \sum_m \left(G^m_{\alpha j} \right)^2 \text{ is large }
    \label{parameter_large_avg_size}
\end{equation}
compared to other normalized combinations in the same neighborhood. (The neighborhoods will be discussed below.) By conclusion I in App.~\ref{sec:x}, this generally implies that the corresponding linear combination of input activations
$$A^m_{\alpha} = \sum_i \alpha_i a^m_i$$
has a larger average size, i.e.,
\begin{equation}
    \frac{1}{M} \sum_m \left(A^m_{\alpha} \right)^2 \text{ is large }
    \label{activation_large_avg_size}
\end{equation}
compared to other normalized combinations (in the corresponding neighborhood of input activations). We therefore choose the fine-to-coarse projection $Q$ of every sheaf to be the same as the projection suitable for the input activation vector $\left\{a_i\right\}_i$, thus the \textbf{same} projection is used for all output sheaves.

Such a choice is very advantageous in several ways. It yields a simple and very inexpensive coarse level, in its creation, storage,its operation. The coarse level will have the same general layer structure as the fine level, so solving its equations can be done with the same procedures (relaxation and further coarsening).

Note that App.~\ref{sec:x} shows that (\ref{parameter_large_avg_size}) implies (\ref{activation_large_avg_size}), but not that (\ref{activation_large_avg_size}) implies (\ref{parameter_large_avg_size}) for every $j$. This means that some parameter combinations may be added to the coarse level that do not {\it have to} be there. This should cause no problem. Thus, to start the coarsening of a layer, a fine-to-coarse projection $Q$ is therefore first constructed for the input activation vector, yielding a \textbf{coarse activation} vector $a^c = Qa$. This is done by mini-clustering the input activations into highly-correlated neighborhoods, to produce sparse $Q$, and then employ SVD to calculate $Q$ in each neighborhood. In these calculations each input activation $a_i$ is represented by the vector $\left\{ a^m_i \right\}_{m \in B}$. The batch $B$ need not be very large, because all we need at this point is to capture the strongest correlations. (But see Sec.~\ref{sec:5.4}) for the case that the neighborhoods cannot be sufficiently small, hence $B$ cannot be smaller either). 

Note that it is assumed that the constant coarse activation $a_0 \equiv 1$ is included, i.e., the intercept term $a_0^m = 1$ is included in every vector $a^m$, its connection $w_{0j}$ to the coarse activation $\tilde{a}_j$ being the so-called bias. Correspondingly, the constant activation $a_0^c \equiv 1$ (i.e., $a_0^{cm} = 1$ for all $m$) is included in the vector $a^c = Q a$. We then define $Q$ so that the average of $a_i^{cm}$ vanishes:
$$
    \sum_{m \in B} a_i^{cm} = 0\,,\qquad (i \not = 0)\,.
$$

The first coarse level of our layer is then constructed simply by inserting the coarse activations $a^c$ as an intermediate activation layer between the input $a$ and output $\tilde{a}$. (This can change later. See App.~\ref{sec:Y}). The connections between $a$ and $a^c$ are given by $Q$ and are \textbf{fixed} during the coarse-level iterations. The connections between $a^{c}$ and $\tilde{a} = W^c a^c$ form a full matrix $W^c$ that is \textbf{learned} by the coarse-level forward/back-propagation iterations (cf.~Fig.~\ref{fig:layer_coarsening}). 
\begin{figure}[htbp]
  \centering
  \includegraphics[width=4in]{layer_coarsening.png}
  \caption{Coarsening the parameters of a layer via an intermediate layer of coarse activations. The connection between inputs and coarse inputs is \textbf{sparse}.}
  \label{fig:layer_coarsening}
\end{figure}

This in principle would mean calculating by back-propagation the gradients
\begin{equation}
	g^c_{i_c,j} = \frac{\partial L}{\partial w^c_{i_c,j}} =
	\frac{\partial L}{\partial \tilde{a}_j} a^c_{i_c}\,,
	\label{gc}
\end{equation}
averaging them over a suitable batch of samples, then multiplying the averages by a suitable coarse-level learning rate $\rho^c$, yielding the \textbf{coarse-level correction} $\delta W^c$, which should then be interpolated and added to the current fine-level weights $W$, namely,
\begin{equation}
	W \longleftarrow W + P \cdot \delta W^c\,.
	\label{w_correction}
\end{equation}

\subsection{8.2 Multilevel Cycle}
\label{sec:multilevel_cycle}
{\bf a multilevel cycle} for approximately solving the NN stochastic optimization is composed of the following four steps.
\begin{enumerate}
    \item Several {\it relaxation} iterations (see Sec.~\ref{sec:8.3} below).
    \item {\it Coarsening:} calculating the fine-to-coarse projection $Q$ as described above. (The same $Q$ can actually be reused by several successive cycles, so this calculation is done only one per several cycles). This, together with (\ref{8.5}) defines {\it the coarse level network} (depicted in Fig.~\ref{fig:fig2}). Note that the input activation to the coarse layer is the entire intermediate vector $a^c$, {\it not} divided into subsets according to the fine-level neighborhood used to create $a^c$; the coarse level will create its own neighborhoods.
    \item {\it Recursion:} executing one or more multilevel cycles for approximately optimizing the coarse network; except that the {\it coarsest} level, employ relaxation only. Note that for more than two elvels the recursion should use FAS (see Sec.~\ref{sec:fas}).
    \item {\it Interpolation:} as in the case of the fine-to-coarse projection $Q$, a great simplification and efficiency (supported by Conclusion II in App.~\ref{sec:x}) can be gained by choosing the coarse-to-fine interpolation ($P^*$ in (\ref{8.6})) to be the same at all output sheaves, determined by a corresponding interpolation derived for the corresponding input activations; namely,
    \begin{equation}
        P^* = \argmin \sum_p \sum_{m \in B} \left\{ a^m_i(p) - \sum_{k \in Z_i} P^*_{ik} a^{cm}_k(p)e\right\}^2\,.
        \label{pstar_fit}
    \end{equation}
    As explained in Sec.~\ref{sec:5.3}, the stencil $Z_i$ could generally include activations not just from the neighborhood of $a_i$, but also from neighboring neighborhoods.
\end{enumerate}

\subsection{Relaxation: SGD for Low-Noise Combinations}
\label{sec:sgd_relaxation}
As explained in Sec.~\ref{sec:5.6} relaxation should be designed separately in each neighborhood, and should consist of SGD in terms of the low-noise normalized combinations of parameters belonging to the neighborhood. It is important that the relaxed combinations (nearly) span the space orthogonal to the high-noise combinations used for in coarsening described above (Sec.~\ref{sec:8.1}). For this purpose, the same SVD of each input-activation neighborhood is used for designing both coarsening and relaxation, the higher combinations (the high-noise ones, i.e. those corresponding to large SVD singular values) defined $Q$ and the rest -- the lower combinations, are used in relaxation; thus the same relaxation is used in all output sheaves. In the case of larger neighborhoods, economical approximate SVD should be used, as in Sec.~\ref{sec:5.4} So, the picture of relaxation is the same as Fig.~\ref{fig:2}, except that $a^c$, the vector of low-noise combinations, denoted $a^f$, and the steps of calculating the relaxation is as discussed for (\ref{8.5}), with $f$ replacing $c$, yielding the relaxation correction $\delta w^f$. This correction is added to the current weights $W$ using (\ref{8.6}) with $\delta w^f$ instead of $\delta w^c$, and (\ref{8.7}) with $a^{fm}_k$ replacing $a^{cm}_k$.

Since all the combinations participating in the relaxation have uniformly-bounded low noise, a uniform averaging length can be used which yields a sufficiently good approximation to all their gradients (see App.~\ref{sec:B}). Then this low-noise relaxation becomes similar to relaxing a deterministic system. So (as generally described in Sec.~\ref{sec:5.5}), in case relaxation slows down before reaching small gradients for all the relaxed combinations (small compared with the current high-noise gradients), the low-noise processing can e,ploy multilevel acceleration similar to a deterministic acceleration.

\section{Full Approximation Scheme (FAS)}
\label{sec:fas}
For the sake of clarity, we have discussed above a process that makes \textbf{one} coarse-level iteration and then immediately transfers corrections $\delta w^c$ to the fine level. What we really need is a process that can make \textbf{many} iterations (generally even including corrections from still-coarser levels) before transferring the accumulated changes to the fine level. Note that in calculating the coarse gradients $g^c$ by (\ref{gc}), each sample $m$ should be fed-forward to obtain $a^m_i$ and $a^{c,m}_{i_c} = \sum_i Q_{i_c,i} a^m_i$, and then back-propagated to get $\partial L/\partial \tilde{a}^m_i$. These calculations are generally done on the current (pre-iteration) \textbf{combined} network, whose weights at each layer comprise of the sum $W + P^* W^c$. In the first iteration, the pre-iteration coarse level is $W^c = 0$, so only the uncorrected fine-level network is used in calculating $g^c$. Can we approximate several iterations of the combined network by processing only at the coarse level (i.e., accumulating $\delta W^c$ over several iterations) before making the correction (\ref{w_correction})?

The answer is yes; the way to do it is inspired by the Full Approximation Scheme (FAS; also called Full Approximation Storage) used in multigrid solvers for nonlinear problems (see \cite{3}). It is so named since the coarse-level variables are no longer directly the coarse-level \textbf{corrections} to the current fine-level solution, but redefined to approximate the \textbf{sum} of the corrections and the fine-level solution (represented at the coarse level). For our NN training, the suggested FAS steps are as follows:
\begin{enumerate}
	\item \textbf{Represent the current fine network $W$ on the coarse level} by setting the initial $W^c$ to
	$$ \hat{W}^c = W P^*\,. $$
	\item \textbf{Calculate the average bias} of this representation:
	$$ b^c = {\mbox{mean}}\left\{ \left( W - \hat{W}^c Q \right) a^m \right\}\,, $$
averaged over many samples $m$ (e.g. throughout the first coarse-level iteration, which is still using the fine-level system $W$ for feed forward and back propagation).
	\item \textbf{Train the FAS coarse level:} starting from $W^c = \hat{W}^c$ as the initial approximation (at all layers), perform iterations to improve $W^c$. In each iteration, we average $g^c$ over a suitable macrobatch of samples (whose length is determined similarly to App.~\ref{sec:control}). For each sample we feed forward and back propagate with $\tilde{a} = W^c Q a + b^c$, yielding $g^c$ as in (\ref{gc}). The batch average $\bar{g^c}$ is then used to update
	$$ W^c \longleftarrow W^c - \rho^c \bar{g^c}\,, $$
	simultaneously at all layers, where the learning rate $\rho^c$ is in fact adjusted for each parameter as in App.~\ref{sec:control}\,.
	\item \textbf{Interpolate the correction to the fine level:} recalling that the coarse level was implicitly calculating a sequence of \textbf{corrections}, what we interpolate back to the fine level is not $W^c$ but the change $W^c - \hat{W}^c$. Thus, instead of (\ref{w_correction}), the FAS correction is
	$$ W \longleftarrow W + P^* \left( W^c - \hat{W}^c \right)\,. $$
\end{enumerate}

The FAS algorithm enables advancing the training a long way at the coarse level, employing in the process accelerations from still coarser levels, and yielding overall a very efficient multilevel cycle. An SGD iteration at the coarse level will actually be much less expensive than a fine-level iteration, since $Q$ is sparse and $W^c$ is much smaller than $W$ (as $a^c$ is much smaller than $a$. This is of course on top of the multi-level advantages described in Sec.~\ref{sec:mlgd}, which are also shared by the non-FAS algorithm.)
\underline{Upgraded FAS.} If needed, we can make the initial coarse layer $\hat{W}^c Q$ to be a much closer approximation to the fine layer $W$ by comparative learning (see Sec.~\ref{sec:upscaling} and App.~\ref{sec:tunning}). Unlike the case of upscaling (Sec.~\ref{sec:upscaling}), for the current purpose (accelerated training) only a crude approximation is required, which is all we would want to invest in each coarsening step. Most often, just Step 2 above would probably do.

\section{Relation to Some Existing NN Algorithms}
\label{sec:relation_existing}
We show below that some popular training acceleration techniques can be interpreted as vastly simplified special cases of collective-motion coarsening steps. They attest to the fact that even a very partial coarsening can significantly improve performance. We also point out that at least some simple coarsening steps are known to occur biologically.

\subsection{One-Parameter Sheaf Coarsening. Batch Normalization}
An extreme simplification of the above input activation coarsening is to use only one coarse variable: $a^c \in \Real$. A further simplification is to define the weights of the interpolation $P$ to be proportional to the current values of the evolving solution (rather than matching examples). This would then basically be identical to performing the \textbf{batch normalization transform} of the popular Batch Normalization (BN) algorithm \cite{bn}. So the BN algorithm includes a crude form of coarse-level correction, promoting faster convergence of some components. This can explain the evident effectiveness of BN. This is perhaps the \textbf{main} explanation in the case of orderly slow convergence, when the \textbf{internal covariate shift} could be more accurately represented using \textbf{global} rather than \textbf{batch-}based means and variances. (The usual training delayed-practice tends to work on the brink of instability, as explained in App.~\ref{sec:sgd_eigenmode}, in which case the BN normalization of the current batch may have an important stabilization role, and due to the fast changes it may also give the best available approximation to the current global means and variances.)

\subsection{Sub-sampling Coarsening. Dropouts.}
As explained above (Sec.~\ref{sec:5.3}), the projection $Q$ can be chosen quite crudely. A particularly simple choice is \textbf{sub-sampling}, where $Q_{\nu}$ of each neighborhood $\nu$ is simply one member of the neighborhood (e.g., the one closest to the neighborhood mean). So in this case $a^c$ is just a subset of $a$.

Settling for a lower efficiency, one can also avoid the trouble of mini-clustering (building the neighborhoods) and choose $a^c$ to be a \textbf{random} subset of $a$, and frequently change the random choice. Then, one can also avoid the trouble of calculating the high-accuracy $P$, by averaging each gradient only over the cases in which it was not dropped out. This reproduces the popular \textbf{dropout} algorithm. This interpretation of dropouts as a vastly simplified coarsening algorithm explains its evident effectiveness (although likely much less effective than the full correlation-based coarsening described above). It also raises the interesting question of whether one can further profit from adding \textbf{coarser dropout} steps, or from introducing the dropouts at a coarser level of the multilevel solver.

\subsection{10.3 The Hebbian Neuroscience Rule}
Suppose two input activations $a_1$ and $a_2$ are highly correlated. According to the coarsening algorithm in Sec.~\ref{coarsening:8} they will form a two-activation neighborhood at the coarse level ($a^c = Q a$; see Fig.~{2}) will include a linear combination of them $a^c_k = q_1 a_1 + q_2 a_2$. This is exactly the well-known {\it Hebbian rule} in neuroscience: ``cells that fire together wire together``.

``Highly correlated $a_1$ and $a_2$`` means that there exists a normalized linear combination
$$ r_1 a^m_1 + r_2 a^m_2\,, \quad r_1^2 + r_2^2 = 1 $$
that is small for all samples $m$\footnote{The superscript $c$ for coarse-level quantity collides with the superscript $m$ for sample. Clarify notation.} The multi-level algorithm extends this rule to larger activation sets (termed ``neighborhoods``), each containing one or multiple\footnote{Rephrased this sentence a little since it was unclear. Please check.} normalized linear combinations that are small (compared with other normalized combinations in the same neighborhood).

Further interesting analogies should be investigated between the multilevel algorithms and system neuroscience. For example, the search for hidden mini-clusters (see Apps.~\ref{G.4, G.5}) is akin to ``recent technological advances that enable large-scale recording of neural activity, [enabling] the discovery of new behavioral variables hidden within neural codes, and even allows using one animal's data to decode another animal's behavior`` \cite{18}. This finding may indicate that the association matrices $T$ can be initialized by knowledge transfers within a continuation process (see App.~\ref{H}).

\subsection{10.4 Low Rank Adaptation (LoRA)}
There is an obvious similarity between a two-level coarsening scheme and a recent technique called Low Rank Adaptation (LoRA) \cite{lora}.

Generally, as discussed in Sec.~\ref{3}, following a proper relaxation (e.g., SGD), the main remaining error (hence the needed correction) resides in a much lower dimensional space. The LoRA approach (without recognizing the real reason for the low dimension) is to {\it learn} the projection of the correction to lower dimension (our $Q$ operator) as part of learning the optimal adaptation of a pre-trained network to any downstream task. A great advantage of this approach compared to our multi-level coarsening method is that it is simpler to understand and implement. On the other hand, the system has the following potential advantages:

\begin{enumerate}
    \item Our coarsening operator $Q$ is potentially much sparser than LoRA's projection operator $A$, since it coarsens one local neighborhood at a time, while $A$ is a full matrix.\footnote{However, critics will say that modern GPUs are optimized to perform matrix-vector multiplications a lot more efficiently than a sparse matrix multiplication. One might argue that that means we should change GPU architectures, but this might take time. How can we efficiently implement the multilevel algorithm so it gains not just in FLOPs but also in runtime?}
    \item The locality also implies that $Q$ can be derived based on a much smaller number of examples.
    \item Our coarse level correction is designed to accelerate the pre-training too, not just the fine tuning.
    \item Equiped with FAS, our coarse level can itself be corrected by a still coarser level (a correction of still-lower dimension).
    \item The FAS coarsening may lead to upscaling (see Sec.~\ref{14}), whereupon the heavy pretrained network is discarded, leaving only the much lighter coarse levels, both for post-training and for inference.
\end{enumerate}

An interesting hybrid possibility is to start with the local-SVD-based $Q$, then improve it as part of the overall optimization by backprojection. This should yield faster learning than starting from a random $Q$.

\subsection{10.5 Transformer}
There are fundamental similarities between the transformer algorithm \cite{transformer} and the multilevel solver.

The multilevel solver's alternation between relaxation (fine-level processing) and coarse-level iterations is similar to the transformer's alternation between attention layers and simple feed-forward networks. An attention layer is similar to a coarse-level correction since it learns and performs {\it collective} changes of correlated activations. The key and query attention matrices are analogous to the transformation $T$ that brings out hidden neighborhoods (see App.~\ref{G4}).

These similarities may actually suggest some useful improvements. The mini-clustering production of neighborhoods (see App.~\ref{G1}) hints to a fast way of summarizing attention over very long context windows. The multi-level construction of the matrix $T$ (see App.~\ref{G5}) suggests faster ways to {\it train} the attention matrices.

\section{Further Layer Coarsening Levels}
\label{sec:further_coarsening}
For full multilevel efficiency, more levels will be needed to be constructed on top of the first coarse level described above. Each \textbf{``level''} is a new network, constructed by coarsening a previously-constructed level. The latter serves as the \textbf{fine level} of this coarsening stage, while the new level is its \textbf{coarse level}.

The coarse level can be regarded as a tool to accelerate convergence of the fine-level iterations. In a typical cycle, training at each level would consist of a sequence of relaxation (e.g., GD or SGD) steps, intercepted by one or more visits to the \textbf{next-coarser} level, before the resulting accumulated changes at the present level are interpolation to correct the \textbf{next-finer} level. In some cases (problem types) each visit may employ a different type of coarsening.

Several types of coarsening will be discussed below. The choice of types and their order generally depends on the problem at hand.  Several coarsening stages of the same type may of course be successively used on top of each other. In particular, the same type of input activation coarsening as described above may well be repeatedly used several times before another type of coarsening is added on top of them.

\subsection{Input Sheaf Coarsening}
The first coarsening described above has coarsened each \textbf{output sheaf} $\tilde{S}_j$ based on correlations among \textbf{input activations} $\left\{a_i\right\}_i$. Similarly, one can coarsen each \textbf{input sheaf} $S_i$ based on the correlations between \textbf{output activation gradients}
$\gamma^m_j(p) = \partial L/\partial \tilde{a}_j(p)$, i.e. correlations between vectors
$$
\gamma_j = \left\{ \gamma^m_j(p) \right\}_{m \in \text{batch}, p \in \text{output grid}}\,.
$$
Briefly: using mini-clustering (see App.~\ref{sec:miniclustering}) we subdivide the space of vectors $\gamma_j$ into strongly correlated neighborhoods, then using SVD (or economical SVD for large neighborhoods, see Sec.~\ref{?}) we divide each neighborhood into two complementary subspaces: a space of low-noise combinations and one of higher-noise. The low-noise combinations are used for low-noise relaxation, while the higher-noise ones define the coarsening operator $\tilde{Q}$. The coarse level of the layer then read as
\begin{equation}
    \tilde{a}^c = W^c a\,,\quad \tilde{a} = \tilde{Q}^T \tilde{a}^c\,.
    \label{11.1}
\end{equation}
The coarse-level processing optimizes $W^c$ by backpropagation, starting with $W^c = 0$ (or with $\hat{W}^c$ approximating $W$ when using FAS). $W^c$ (or $\delta W^c = W^c - \hat{W}^c$ when using FAS) is then interpolated and added to $W$, where the interpolation $\tilde{P}^*$ is defined as corresponding to interpolation designed in the space of vectors $\gamma_j$.

\subsection{11.2 Direct Parameter Coarsening}
\label{sec:direct_coarsening}
Following several activation-based coarsening stages, a level may be reached at which more such stages would no longer be effective, so further acceleration may require a direct, cross-sheaf parameter coarsening. This means applying the aforementioned process of ``coarsening in neighborhoods'' (Sec.~\ref{sec:5.5}) to the full set $W$ of weights in the layer, instead of coarsening and interpolation being inexpensively performed at the activation yard\footnote{grid?}. But at this stage of coarsening it may be OK, since we may assume that the number of parameters is much smaller than their number at previous coarsening stages.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Inter-layer Coarsening: Skip Layers}
\label{sec:skip_layers}

Generally, there seems to be no particular correlation between individual parameters belonging to different layers, so cross-layer coarsening cannot be done in the way described above. However, with some resemblance to multigrid solvers, the slowness of information exchange across deep networks can be alleviated by hierarchical ``coarsening in layers'', achieved by several stages of \textbf{skip-layer coarsening}.

In each such coarsening stage, one adds a sequence of \textbf{skip layers} in parallel to the fine level. Each skip layer \textbf{adds} connections from all the input activations ($a$) of one fine-level layer to the output activations ($\tilde{a}$) of a \textbf{later} (most typically the next) layer. Usually, the fine level would already be a coarsened level, in which case the skip layer can connect a \textbf{coarse} input activation vector (e.g., $a^c = Qa$) to a \textbf{coarse} output activation vector (e.g., $\tilde{a}^c = \tilde{Q}_2^T \tilde{a}_2$) of the later layer (see Fig.~\ref{fig:skip_architecture_coarse}). The skip layers should be \textbf{disjoint} from each other (traversing disjoint segments of the fine network), to enable independent coarse-level iterations.

\textbf{A learning iteration} with skip layers is performed on a batch of samples. Each sample is fed forward through all connections (skip-layer contributions added to the fine level ones) and then back-propagated through all of them, computing gradients for both fine-level and skip-layer connections. All the gradients are averaged over the batch. After averaging over a large enough batch (cf.~App.~\ref{sec:control}), one can use the gradients, multiplied by suitable learning rates, to do either a fine-level iteration (updating the fine-level connections) or a \textbf{coarse-level iteration} (updating all  skip connections plus all fine-level connections of any layer not traversed by a skip layer).
\begin{figure}[htbp]
\centering
\begin{subfigure}[b]{0.55\textwidth}
  \includegraphics[width=3in]{skip_architecture.png}
   \caption{}
   \label{fig:skip_architecture} 
\end{subfigure}

\begin{subfigure}[b]{0.55\textwidth}
  \includegraphics[width=3in]{skip_architecture_coarse.png}
   \caption{}
   \label{fig:skip_architecture_coarse}
\end{subfigure}
\caption[Skip layer architecture.]{(a) Skip layer architecture. Layer $1$'s input is connected to layer $2$'s output via the path $W_1-{\mbox{ReLU}}-W_2$ and the skip connection with parameters $W_{12}$. (b) Once layers $1$ and $2$ have been coarsened (we assume for illustrative purposes that only inputs were coarsened in layer $1$, and only outputs were coarsened in layer $2$), the skip connection connects the coarse activations instead.}
\end{figure}

At each coarse-level round, starting with all skip connections set to $0$, they subsequently grow through the coarse-level iterations. At some point one would want to translate all the skip connections into corrections to the fine level connections, and then start another coarse-level round. This translation can be done by \textbf{comparative learning} (see App.~\ref{sec:tunning}), with each segment of fine-level layers traversed by a skip layer being guided by the \textbf{total coarsened segment} (the original segment plus the skip layer). Thanks to its local nature, this learning should converge relatively quickly, and it need not be converged much, since inaccuracies will be corrected as part of the next coarse-level round.

Instead of so translating the skip connections (or when this cannot be done accurately enough), another option is to simply leave them as permanent parts of the network. This would at most double the overall number of parameters, but may be useful in various types of applications, like the one discussed next.

Conversely, for various applications (such as network compression or fast first-response networks; see Sec.~\ref{sec:network_compression}), one would actually want to approximate the fine level by a skip-layer level. For example, layers $l-1$ and $l$ with weights $W^{l-1}$ and $W^l$ and biases $b^{l-1}$ and $b^l$, respectively, can be approximated by the skip layer $(l-1,l)$ with weights
$$ \hat{W}^{(l-1,l)} = W^l W^{l-1} $$
and biases
$$ b^{(l,l-1)} = \underset{m}{\mathrm{mean}} \left[ W^l \sigma(W^{l-1}(a^m)^{l-1} + b^{l-1}) + b^l - \hat{W}^{(l-1,l)} \right]\,,$$
averaged over a sufficiently long batch of samples. Here $\sigma$ is the activation function of layer $l-1$ and $(a^m)^{l-1}$ is its input activation vector for sample $m$. Using a larger batch o samples, a much better approximation (e.g., for the purpose of upscaling, cf. Sec.~\ref{sec:upscaling}) can be obtained by comparative learning (App.~\ref{sec:tunning}).

Similarly to Sec.~\ref{sec:fas} above, this approximation can be used for an \textbf{FAS type} operation of the skip-layer, making it autonomous and allowing it to be accelerated by visits to still coarser levels. The interpolation of the correction $ W^{(l-1,l)}  - \hat{W}^{(l-1,l)}$ to the pair of fine-level layers can be done by local comparative learning like the translation described above.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{13. Increased Convolutional Kernel Width}
\label{sec:extended_region}
In convolutional layers, the input kernel (for 2D inputs) has dimension $N_c \times k \times k$ (see Sec.~\ref{sec:8}). Most often the smallest reasonable width $k = 3$ is used. Wider (larger-$k$) kernels, when optimized, should always give better results (sine a narrow kernel is a special case of a wider one), but they are much more expensive to both train and use. In addition  to increased complexity due to the $O(k^2)$ number of parameters, training with larger $k$ also suffers from more severe slowness due to the larger-scale motions that need to be performed.

In contrast, with a multilevel optimizer the use of a larger $k$ seems more attractive, for the following reasons.

First, the slowness due to required larger-scale motions is eliminated by performing them at coarser levels.

Secondly, it is exactly those larger-scale motions that make for higher accuracy. To obtain the same accuracy with a narrower kernel, many more layers need to be employed to account for the larger scale corrections, which introduces many more parameters as well as inter-level slowness (see Sec.~\ref{sec:12}).

A wider kernel would allow taking sooner into account wider and/or more remote correlations. It will strengthen the network's ability to account for feature positional relations and make earlier detection of extended-scale features, boundary ownership, etc., making the network more resilient to occlusion, corruption, and noise (see for example discussion of lateral connections in \cite{montobbio2019kercnns, spoerer}). The wider/remote correlations can be detected as soon as they occur by applying an efficient multi-level mini-clustering (like RMC, App.~\ref{sec:x1} to the entire input of a wide kernel, basically because they do not directly exploit feature correlations common to different samples (relying on learning instead).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Upscaling}
\label{sec:upscaling}
Generally, ``\textbf{upscaling}'' is the process of replacing a computationally expensive fine-level solver with a much-less-expensive coarse-level one, which omits fine level details but retains the accuracy in the features of interest (see \cite{systematic_upscaling}).

upscaling is relatively straightforward in systems already equipped with a good multilevel (e.g., multigrid) fast solver. The upscaling process can then be based on the same coarsening scheme that was first developed just for the purpose of accelerating the fine-level solver. Casting the coarse-level system of such a solver in the Full Approximation Scheme (FAS - see \cite{brandt77}), we already have not only a fine-to-coarse projection of the variables, but also a ``fine-to-coarse defect correction'', a term added to each coarse-level equation (see, e.g., \cite[Sec.~8.2]{guide}). During the multi-level \textbf{fast-solving process}, this term, usually denoted $\tau$, is defined so that the corrected coarse-level equation is exactly satisfied by the \textbf{current} fine-level approximate solution (projected to the coarse variables space). In \textbf{upscaling}, we use many windows of fine-level approximate solutions to produce (by projection) many local examples of the coarse-level solution, each with its own $\tau$ corrections. From these examples, we can learn how each $\tau$ term depends on the \textbf{coarse-level} variables in the neighborhood. Once this has been learned in sufficient generality, the fine level is no longer needed. 

This learning is local and therefore requires few examples. In fact, one or more levels of upscaling can often naturally occur as part of coarsening. That should happen when the optimization problem is overparameterized. In such cases, by performing the CGD relaxation (see Sec.~\ref{sec:7.1}) or the low-noise relaxation (see Sec.~\ref{sec:8.3}) one would find that the relaxed combinations are already converged (so the relaxation has negligible effect), so one can switch to the next coarser level and never return to the current fine level.

Another case should happen upon fine-tuning for a downstream task, which should normally depend on much fewer degrees of freedom (compared to the huge number of pretrained parameters). One should then only update some coarse levels at each layer. Using FAS, the fine level should then only be visited infrequently, to update the fine-to-coarse defect correction. Using upscaling (i.e., learning the defect correction's dependence on the coarse variables) one may avoid further fine-level visits altogether.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Potential Benefits}
\label{sec:benefits}

\section{Fast And Energy-Efficient Training}
\label{sec:fast_training}
\begin{itemize}
    \item By systematically separating in each level at each layer the higher-noise from the low-noise and the lower-RER from the high-RER components, the multilevel cycles can eliminate the main resources of slowness inherent in current SVD optimizers. The cycles are highly parallelizable. Even though the gain in wall clock speed may be partly offset due to hardware considerations, the energy savings are likely to still fully reflect the bug reduction in operation count.
    \item Per sample, at higher training levels, using FAS coarsening (Sec.~\ref{sec:fas}), far less operations are needed for fast forward and back propagation.
    \item Fast information transmission is possible in deep networks owing to hierarchical skip-layer connections (Sec.~\ref{sec:skip_layers}).
    \item Less layers may be required if wider convolutional kernels are implemented (Sec.~\ref{sec:extended_region}).
\end{itemize}

\subsection{Fast Stable Outer Iterations. Task Adaptation}
\label{sec:out_iteration}
In many problems, the iterative solver is actually built on inner iterations (such as SGD) that serve outer iterations (such as generator/discriminator iterations in GAN algorithms or Q iterations in neural reinforcement learning). Typically in such situations, many outer iterations can be confined to the coarse levels of the inner solver, making those iterations both faster and more stable.

Similarly, the fine-tuning of a pre-trained model to a new task may be mostly confined to the coarse levels of the pre-trained model.

\subsection{High Accuracy}
\label{sec:high_accuracy}
\begin{itemize}
\item The multi-level cycles enable the networks to continue efficient learning beyond the point where current training processes stagnate, thereby achieve higher accuracy.
\item Accuracy can be much enhanced through the use of much larger (wider) activation vectors and much larger multi-scale receptive fields (see Sec.~\ref{sec:extended_region}) without substantially slowing down inference (since the coarse activation vectors such as $a^c$ in Fig.~\ref{fig:layer_coarsening}) need not be widened).
\item The adaptable correlation-based receptive fields in convolutional layers (see Sec.~\ref{sec:extended_region}) can detect more positional relations and extended-scale features.
  \item Attention by fast first-response operation (Sec.~\ref{sec:network_compression}), network bifurcations (Sec.~\ref{sec:bifurcation}) and more effective use of data (Sec.~\ref{sec:effective_data}) -- can all enhance accuracy.
\end{itemize}

\subsection{Network Compression}
\label{sec:network_compression}
\begin{itemize}
\item Our coarse levels retain much of the discriminatory power of the full network, since they retain all the ReLU steps between layers and only distill the far-too-numerous, redundant parameters within each layer.A very good approximate network can therefore be constructed by \textbf{upscaling} (Sec.\ref{sec:upscaling}), possibly followed by fine-tuning. This can reduce both the network size and inference time by a large factor.
\item In particular, for \textbf{highly specialized tasks} very high compression can be obtained thanks to especially strong correlations at all layers. This is particularly important for implementation on limited-capacity end devices.
\item Collapsing more fine levels would produce a less accurate but even faster network. This can be used as an extra-fast \textbf{first-response} network, important for making fast rejection decisions (e.g., deciding which images, or what regions in an image, can be excluded from further scrutinizing), or for quickly choosing a sub-network (see Sec.~\ref{sec:bifurcation}), etc.
\end{itemize}

\subsection{Data Efficiency}
\label{sec:effective_data}
\begin{itemize}
	\item \textbf{Much less training data} is needed for multilevel training, as explained in Secs.~\ref{sec:5.5}. In other words, correlation-based networks can use much less data since they learn hidden similarities between samples.
	\item \textbf{Unlabeled samples}, only fed-forwarded into the network, can enhance the accuracy of activation correlations. More importantly, one can choose to label and back-propagate only samples showing marked disagreement with the established correlations. This can be used for \textbf{lifelong learning} of deployed networks. Out-of-distribution detection can also be used to expose corrupted input.
\end{itemize}

\subsection{Memory Efficiency}
\label{sec:memory_efficiency}
The organization of input activations and weights in miniclusters allows storing only their differences from cluster centers, which can be done in lower relatively accuracy, enabling large storage savings by quantization and low-dimensional projections (See App.~\ref{sec:G.3}).

\subsection{Network Bifurcation}
\label{sec:bifurcation}
The narrower the network task, the stronger the correlations at all layers, enhancing all the benefits listed above. Hence a lot can be gained by bifurcating a network at some layer into different sub-networks to deal with different sample domains or different input regions, with special attentions where needed. The multi-level framework may offer some tools to this end:
\begin{itemize}
	\item Fast first-response coarse networks (see Sec.~\ref{sec:network_compression}) can be employed to detect the need and places for attention and bifurcations.
	\item A potential indicator for the need to split the data into different routes can appear in the formation of neighborhoods (Sec.~\ref{sec:neighborhoods}), when splitting the input samples into classes highly enhances the intra-neighborhood correlations within each class.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage
% TODO(oren): remove part number of the titke "Appendices" from TOC.
\part{Appendices}

\appendix

\section{SGD Eigenmode Analysis}
\label{sec:sgd_eigenmode}
We analyze \SGD~iterations, looking at one particular error eigen component of the Hessian, with eigen value $\lambda$.

\textbf{Notation:} $\mu_k$= amplitude of true average of the error component after the k-th iteration. $\mu_k + r_k$ its average over the minibatch employed in the k-th iteration. (By ``minibatch" here we mean the batch of samples averaged over between successive \SGD~updates; this is \textbf{not} necessarily related to the batch of samples processed simultaneously based on hardware or parallelization considerations.) We assume $\mathbf{E}[r_k]=0$, $\mathbf{E}[r_k^2]=\sigma^2$, $\mathbf{E}[r_k,r_{k'}]=0$ for $k \neq k'$, where $\mathbf E[\cdot]$ denotes the expected value.
$\rho$ = learning rate, $\beta=\lambda \rho \ll 1$, 

In the k-th iteration 
\begin{equation}
\mu_k = \mu_{k-1} - \lambda \rho (\mu_{k-1} + r_{k-1})
\label{eqn:muupdate}
\end{equation}
or
\begin{equation}
	\mu_k = \mu_{k-1} (1- \beta)  - \beta r_{k-1}
	\label{eqn:muupdate2}
\end{equation}
Hence we can recursively derive:
\begin{equation}
  \mu_k = (1-\beta)^k \mu_0 - \beta \sum_{i=0}^{k-1} (1-\beta)^i r_{k-i-1}
\end{equation}

%\begin{equation}
%\mu_k = (1-\beta)^k \mu_0 + \sum_{i=1}^k (-\beta)^i A_{ki}
%\end{equation}
%where
%\begin{equation}
%A_{k,k} = A_{k-1,k-1} = r_1, \quad A_{k,1} = A_{k-1,1} + r_k = \sum_{i=1}^k r_i
%\end{equation}
%and
%\begin{equation}
%A_{k,i} = A_{k-1,i} + A_{k-1,i-1}, \quad (2 \le i \le k-1).
%\end{equation}
%In some simple steps it follows that 
%\begin{equation}
%\mu_k = (1-\beta)^k \mu_0 - \beta \sum_{i=1}^k r_i  + \beta^2\sum_{i=1}^{k-1}(k-i)r_i + O(\beta^3)
%\end{equation}
Therefore:
\begin{equation}
\mathbf E [\mu_k] = (1-\beta)^k \mu_0 
\end{equation}

\begin{equation}
\mathbf E \left [\left(\mu_k - \mathbf E [\mu_k] \right)^2 \right ] =
\beta^2\sigma^2 \sum_{i=0}^{k-1} (1-\beta)^{2i} \\ =
\beta^2\sigma^2\frac{1-(1-\beta)^{2k}}{1-(1-\beta)^2}\,.
\label{eqn:noise1}
\end{equation}
The usual \SGD~practice is to employ a fixed minibatch size. This means that the variance $\sigma^2$ is nearly fixed throughout the iterations for each eigen component, while $\mu$ should converge to $0$. For any component that converges reasonably fast, an especially for high-$\lambda$ components, $(1 - \beta)^{2k}$ is small, hence the variance is
\begin{equation}
\mathbf E \left [\left(\mu_k - \mathbf E [\mu_k] \right)^2 \right ] \approx \frac{\beta}{2} \sigma^2\,.
\end{equation}
For stability, this variance should be smaller than $\mu_0^2$, or $\lambda \rho < 2 \mu^2/\sigma^2$. Hence,
$$
	\rho < \min \left( \frac{2 \mu_0^2}{\lambda \sigma^2} \right) \sim
	\lmax^{-1} \min_* \left( \frac{\mu^2}{\sigma^2} \right)\,,
$$
where $\min_*$ is taken over all components we do not wish to explode at this point and whose eigenvalue is comparable with $\lmax$, where $\lmax$ is the highest $\lambda$ of the components we do not wish to explode. Here $\mu$ refers to the value at the current stage of training, which does not change much in the analysis interval, i.e. between $\mu_0$ and $\mu_k$, where $k$ is just sufficiently large to make $(1 - \beta)^{2k}$ significantly smaller than $1$.

Hence, the convergence factor per iteration for a component with eigenvalue $\lambda$ is $1 - \epsilon$, where
\begin{equation}
  \epsilon = \lambda \rho < \frac{\lambda}{\lmax} O\left( \min_* \frac{\mu^2}{\sigma^2} \right)\,.
  \label{eqn:convrate1}
\end{equation}
Convergence quickly becomes extremely slow, particularly for small-$\lambda$ components, not only due to the smallness of the deterministic factor $\lambda/\lmax$, but much more so because for components with $\lambda \approx \lmax$ the values of $\mu / \sigma$ decrease fast.

Attempting to counter this slowness, \SGD~practice found it better not to insist on strict convergence of all components: Continuing to use the same $\rho$, the magnitude of the noise (\ref{eqn:noise1}) grows, increasing the value of $\mathbf E[|\mu_k|]$ until it becomes $O(\beta^{\frac12}\sigma)$. Note that this is a re-growth of the \textbf{true} gradients, i.e., gradients averaged over all data, not just over a minibatch (i.e., not the kind reported for instance in \cite[Fig.~8.1]{Goodfellow-et-al-2016}). After sharply dropping the learning rate, for relatively short training period the gradients will sharply decrease in size, but will then start climbing back up to $O(\beta^{\frac12} \sigma)$.

The interesting fact is that \textbf{during this re-growth of the true gradient sizes, the loss function continues its decline, and the accuracy continues to improve}, i.e., the system continues its positive learning. This clearly shows that the learning mainly depends not on the diverging high-$\lambda$ components, but on the lower $\lambda$ components that continue to converge (albeit slowly). For them, the longer use of high $\rho$ is actually beneficial (which is exactly why practitioners find doing that useful).

The multi-level training aims at treating the two problematic sides of the stochastic ill conditioning: First, it can apply only a small number of updates at the fine level, thus avoiding the large accumulation of noise at the high-$\lambda$ components. Second, and more importantly, by converging the low-$\lambda$ components in their lower dimensional subspace, both factors of slowness in (\ref{eqn:convrate1}) are largely mitigated: first, the lower dimensional subspace does not include components with eigenvalues larger than some $\llow$, allowing for $O(1/\llow)$ instead of $O(1/\lmax)$ learning rates. Secondly, the $\min_*$ minimization is over components with $O(\llow)$ instead of over $O(\lmax)$ eigenvalues, for which the ratios $\sigma/\mu$ may be  smaller. In contrast, in the usual (single-level) SGD algorithm, as the averages $\mu$ decrease, becoming mostly averages of lower-$\lambda$ components, the variances continue to include the variances typical of the highest-$\lambda$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{SGD/GD Control in the Multilevel Framework}
\label{sec:control}

As noted in App.~\ref{sec:sgd_eigenmode}, in each multilevel cycle and at each coarsening level, the fine-level \SGD~should be terminated as soon as the gradient norm starts to grow, and at that point we switch to the next coarser level. But how should the \SGD~processing itself be controlled (both in this framework and generally)?

To best control \SGD, it is very useful to calculate gradient \textbf{variances} along with gradient \textbf{averages}, which incurs only a small extra cost.

Not to be confused with the per-eigenmode notation of App.~\ref{sec:sgd_eigenmode}, we denote here the true average of the gradient $g_i$ of the weight $w_i$ by $\mu_i$, and its true variance \textbf{per sample} by $\sigma_i^2$ (i.e., the average of $(g_i-\mu_i)^2$ over all input instances). We again denote by \textbf{minibatch} a batch of $K$ samples, and by $\tilde{\mu}_i$ the gradient average over the batch, which is an estimate for $\mu_i$, and can be used to update $w_i$. Estimating $\sigma_i^2$ during minibatch processing adds relatively little work to estimating $\mu_i$, since it can be based on averages of $g_i$ over \textbf{microbatches} of size $N_{\mu b}$. The variance per sample estimate is the per-microbatch variance estimate times $N_{\mu b}$.

Barring other considerations, we adopt the general approach of \textbf{steepest descent}, namely, the correction we want to make is
\begin{equation}
  w_i \longleftarrow w_i - \rho \mu_i\,,
  \label{gd_step}
\end{equation}
where $\rho$ is the \textbf{learning rate} for the level (and layer) being trained. $\rho$ can be selected  experimentally (e.g., as half of the value at which training becomes unstable) and verified to provide effective error narrowing by the predictors described in Sec.~\ref{sec:predictors}).

Since the minibatch yields only estimates $\tilde{\mu}_i$ and $\tilde{\sigma}_i^2$ to $\mu_i$ and $\sigma_i^2$, respectively, one needs to replace (\ref{gd_step}) by
\begin{equation}
  w_i \longleftarrow w_i - \rho \mu_i \min\left\{ 1, \frac{K \tilde{\mu}_i^2}{\theta \tilde{\sigma}_i^2} \right\}\,.
  \label{gd_step_k}
\end{equation}
where $\theta$ is some $O(1)$ constant, say, between $4--10$. Indeed, for $\tilde{\mu}_i$ to be a reasonable approximation to $\mu_i$, it should be an average over $k_i = \theta \tilde{\sigma}_i^2/\tilde{\mu}_i^2$ samples; if $K > k_i$, then the minibatch already includes long enough averaging; but if $K < k_i$ then we have to average over $k_i/K$ minibatches, so we have to multiply the minibatch average by $K/k_i$.

What should the minibatch size $K$ be? A reasonable choice is a weighted average of all $k_i$, with weight reflecting each gradient's importance (or size), e.g.,
\begin{equation}
  K_* = \frac{\sum_i \tilde{\mu}_i^2 k_i}{\sum_i \tilde{\mu}_i^2}\,,
  \label{k_star}
\end{equation}
where $i$ runs over all parameters (or all parameters of a layer, if different layers are updated separately). Choosing $K$ smaller than $K_*$ is fine, but if it is \textbf{much smaller} then too many steps (\ref{gd_step_k}) are made to accumulate one reliable correction, which adds unnecessary overhead and noise to the process. On the other hand, $K$ should not be \textbf{much larger} than $K_*$, lest too long averaging is done per correction step.

We use minibatches of size $K = K_*$, in which case one correction step is (roughly) equivalent to one \textbf{deterministic} Gradient Descent (GD) step, i.e., it employs (approximately) the \textbf{true} gradients. We call the iteration (\ref{gd_step_k}) with $K = K_*$ \textbf{quasi GD iteration}.

\subsection{Sampling noise. Needed Averaging Length (NAL).}
\label{sec:sampleing_noise}
Suppose we have random variable $X$ with mean $\mu$ and variance $\sigma^2$. 
Lets assume we need to estimate this mean $\mu$. The standard method would be to sample and calculate the {\it sample mean} $S_K$ using $K$ samples $x_i$ of $X$:
\begin{equation}
S_K = \frac{1}{K}\sum_{i=1}^K X_i
\label{eqn:estimate}
\end{equation}
Why the sample mean is a good predictor for $\mu$? Because the law of large numbers shows that  our estimator can get as close as needed to the true value as we increase the number of samples:
\begin{equation}
\lim_{K \rightarrow \infty} S_K = \mathbb{E} X = \mu
\end{equation}
But, how many samples do we need to use for an accurate estimation?  In other words how far could our estimator $S_K$ deviate from the true mean $\mu$? We definitely do not want the deviation to be larger than the quantity we are trying to estimate ($\mu$): 
\begin{equation}
\mathbb{E}(S_k - \mu)^2 \le \theta \mu^2
\label{eqn:snr_req}
\end{equation}
The parameter $0 < \theta < 1$ determines the needed accuracy. Note that for any $K>0$ the expected value of $S_K$ is exactly the mean $\mu$ we are aiming to estimate and therefore the equation above represents (by definition) the variance of $S_K$:
\begin{equation}
\mathbb{E}(S_K - \mu)^2 = \var(S_K) 
\end{equation}
Calculating this expectation and assuming that our samples are independent we get:
\begin{equation}
\var(S_K) = \frac{\sigma^2}{K}
\end{equation}

The dependency on $K$ is a basic property of our estimator. If we somehow know or can get a descent estimate of the magnitude of $\mu$ and $\sigma$ then we can determine the \textbf{Needed Averaging Length (NAL)}: the minimum number of samples we will need to take in order to get to the accuracy we defined in (\ref{eqn:snr_req}), which is
\begin{equation}
\frac{\sigma^2}{K \mu^2} \le \theta \quad \Longrightarrow \quad K = 
\ceil{\frac{\sigma^2}{\theta \mu^2}}\,.
\label{eqn:snri}
\end{equation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Production and Use of Low-GER Examples}
\label{sec:low_ger}

We describe here how the production and use of low-GER examples can be done at all levels throughout the solution process, without needing to store then. We only store the (sparse) interpolation $P$ at each level, which is continuously updated and improved to an increasingly higher accuracy as additional, increasingly-lower GER examples are generated on-the-fly.

Generally, the multi-level cycles \textbf{increasingly} contain more levels: starting with two-level cycles, increasingly coarser levels are added at subsequent cycles. At each level, the interpolation can have low accuracy at first, but should become increasingly more accurate as the number of coarser levels grows. This is because at higher (coarser) levels the GER is much lower, so much larger learning rates are employed, which in turn cause much greater amplification of high-GER interpolation errors.

For clarity, we first describe the \textbf{deterministic} optimization case, then the modifications for the general \textbf{stochastic} case and the special case of neural network training.

\subsection{The Deterministic Case}
At every level, the GD iterations efficiently liquidate the highest-GER error components, and then start to struggle with progressively reducing lower GER components. So the differences between successive approximations tends to have progressively lower GER. Hence, the parameter correlations of low GER components, especially the stronger correlations, will increasingly surface. This will signal the point at which transition should be made to the next coarser level. Also, the strong correlations can be used to miniclusters the parameters into small neighborhoods and build from them a sparse and crude projection $Q$, and interpolation $P$. Higher accuracy interpolation would require additional low-GER examples, which unfortunately cannot be efficiently obtained from the GD iteration, as these iterates are slow to change low-GER components.

For the first visit to the next coarser level, the low accuracy of $Q$ and $P$ should suffice to create a coarse level that efficiently reduces the initial low-GER errors. As a result, upon interpolating the changes to the fine level and applying GD there, the slowing down of the GD iterations now reveals a \textbf{new} low-GER example, much different from the ones that were used to calculate the low-accuracy $P$. This new low-GER example can thus be used to improve $P$.

A new example $e_{NEW}$ can change an existing interpolation $P_{OLD}$ to a new, improved one $P_{NEW}$ through the (trivial to solve) minimization
\begin{equation}
  P_* = \argmin_{P \in Z} \left\{ \left\| P - P_{OLD} \right\| \,\, | \,\, e_{NEW} = P Q e_{NEW} \right\}
\label{p_star}
\end{equation}
and the under-correction
$$
  P_{NEW} = P_{OLD} + \theta \left( P_* - P_{OLD} \right)\,,
$$
  with some $0 < \theta \leq 1$. More and more examples can in this way gradually improve the interpolation \textbf{weights}. To increase the interpolation accuracy one can gradually also expand the interpolation \textbf{stencils} $Z$ (to neighboring neighborhoods, as mentioned above). A new expansion is due when the last expansion is shown to have been highly beneficial, as measured on new examples.

Thus, at any level, before switching to a coarser level the interpolation $P$ can be improved, effectively by producing and using an error example for which the algorithm has so far been inefficient. As cycles progress to high levels producing lower GER examples, more accurate interpolations will emerge as needed.

Note that this gradual change of interpolation is particularly fitting in solving highly \textbf{nonlinear} problems, where the Hessian gradually changes during the solution process.

\subsection{The Stochastic Case}
The same algorithm can basically be used with quasi GD iterations (see App.~\ref{sec:control}) instead of GD iterations. Here it would be important to make sure that the low-GER differences between iterates are not masked by the accumulation of noise (as described in App.~\ref{sec:sgd_eigenmode}). The quasi GD iterations should therefore be terminated as soon as the gradient starts to grow, and supplemented with a quasi-GD iterations with a much smaller learning rate, before extracting a low-GER example.

\subsection{Activation-based Neural Network Coarsening}
\label{sec:act-based-coarsening}
As explained in Sec.~\ref{sec:nn_coarse1}, the first, low-accuracy $Q$ and $P$ are based on activation correlations over different samples (instead of different low-GER examples). $P$ should then be \textbf{improved} by low-GER examples in the way described above.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Comparative Learning}
\label{sec:tunning}
\textbf{The comparative learning of a network $A$ guided by a network $B$ over a segment $C$ in } ($C$ containing one or multiple layers of $A$ as well as one or more layers of $B$) is defined as iteratively correcting the parameters of segment $C$ in $A$ to minimize the average over all data of the $l_2$ norm of the difference between the outputs of network $A$ and $B$, after applying both to the same input (i.e. a feed-forward pass) over segment $C$.

Each iteration uses a batch of samples. Each sample is fed forward through the network common to $A$ and $B$ until the start of segment $C$, and then separately through the $A$ version and $B$ version of the segment. Regarding the $l_2$ norm of the difference between the two versions at the end of segment $C$ as a loss function, one calculates by back propagation the gradients of segment $C$ in network $A$. Averages of these gradients over the batch, multiplied by a suitable learning rate, are used to correct the \textbf{learned} layers (not the projection/interpolation layers) of version $A$ of segment $C$.

Comparative learning is usually local (since a larger segment $C$ can usually be broken into a sequence of shorter segments), so it is expected to converge relatively fast, as only a relatively small number of parameters are updated. Importantly, comparative learning requires no annotated samples.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Representative Sample Ordering}
\label{sec:ordering}
The noise in training a network stems from the variance between batch averages. This variance can be significantly lowered by having the batches more similar to each other in their composition; in other words, having the sample composition of each batch as representative as possible of the composition of the entire training set. By a ``batch'' here we do not necessarily mean a minibatch, but actually a stretch of samples comparable to the \NAL~ of each gradient. 
Since different gradients have different {\NAL}s (and each \NAL~depends on the sample ordering), we are actually interested in ordering the samples so that \emph{any} extended interval in that ordering will be as representative as possible. This is expected to be particularly important for averaging higher-level gradients, whose {\NAL}s are much shorter, and for our protobatch statistics to be as representative as possible of other batches. 

Our general approach for achieving more representative sample ordering is as follows. We divide the set of training samples into disjoint subsets, different samples within the same subset being as ``similar'' to each other as possible (in a sense to be discussed below). The training sampling visits sequentially all the subsets in a random order. After visiting all the subsets, they are visited again in a new random order, and so on. In each visit to a subset only one sample is taken from it; the next sample in the subset will be taken upon the next visit to the subset. The samples within each subset are also ordered randomly, and re-ordered randomly once all of them have been taken.

A related approach for sampling is introduced in the context of embedding learning in \cite{WMSK}.

\subsection{Weighting}
The standard loss function assigns equal weights to all samples. To exactly retain this loss function, in averaging the gradients in a batch, a sample in subset $\mathbb{S}$ should be assigned the weight $|\mathbb{S}|$. Alternatively, one can change the loss function, for example by giving all \emph{subsets} equal weights (thus using equal weights to all the samples in the batch), arguing that the size of a subset does not reflects its importance. 

\subsection{Subset-Boosting}
Higher weights can be assigned to subsets that show poor performance (e.g., a larger average loss). Such subset boosting can be more robust than individual sample boosting, suffering less from mis-labeling cases. 

\subsection{Clustering Schedule}
At the beginning of training, the subsets are based on the labels; e.g., each subset is one category. Later in the training, each category will be sub-divided into subsets based on similarity of the representations (feature vectors of activations) that the samples have at some layers of the network. The more advanced the training, the more accurate are these representations, enabling increasingly more accurate and finer sub-divisions.

\subsection{Similarity Metric}
We first experimented with clustering based on the $L_2$ distance between the vectors of last-layer activations. The produced sample ordering did yield some improvements in training accuracy, but those were really significant only in the case of a toy problem, where the vector length (the number of last-layer activations) was just $10$. Very slight improvements were gained for vectors of length $100$ or $1000$. 

These results may be due to the metric we have used. Indeed, the $L_2$ distance between very long vectors can easily be dominated by the many distances between insignificant activations. We plan to experiment with a much more meaningful metric, based on bi-clustering samples/activations (See Section \ref{sec:bi-clustering}). One added advantage of this metric is that one does not have to decide which network layer to use. Using several layers, for each cluster the algorithm would automatically give preference to the activations more relevant to it. All the most noisy layers (those with the largest {\NAL}s) should be used. 

We will also try a ``higher'' metric, based on bi-clustering of samples vs. higher-level activations (Section \ref{sec:act-based-coarsening}).

\subsection{Multi-level Ordering}
The clustering (and bi-clustering in particular) can be hierarchical. The hierarchical clustering can be used for multi-level sample ordering, generating the two-level ordering described above. 

\subsection{Fuzzy Ordering}
The (bi) clustering can be fuzzy, where each sample has a certain probability of belonging to each of several subsets. In such a case, the probability of each sample to participate in each ordering or re-ordering of a subset should equal its probability of belonging to the subset. The weight $|\mathbb{S}|$ of a subset $\mathbb{S}$ should then be defined as the sum of its sample probabilities. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Bi-Clustering}
\label{sec:bi-clustering}
Various biclustering algorithms have been developed in Bioinformatics and other fields (see for example ``Biclustering'' in wikipedia). The biclustering described here is a method of clustering nodes in a bipartite graph based on the general graph coarsening approach described in \cite{RSB}. 

\underline{A bipartite graph} consists of two disjoint sets of nodes (two ``partitions''), $\{a_1,a_2,\ldots,a_M\}$ and 
$\{b_1,b_2,\ldots,b_N\}$ with no intra-partition links; the only links are from some $a_m$ to some $b_n$, having a weight $w_{mn}$. Examples:
\begin{enumerate}
\item
$\{a_m\}_m$ are customers, $\{b_n\}_n$ are movies, the link $w_{mn}$ is the number of times customer $a_m$ has watched the movie $b_n$.
\item 
$\{a_m\}_m$ are samples for training a neural network, $\{b_n\}_n$ are  hidden units in the network, $w_{mn}$ is the activation strength sample $a_m$ produces in unit $b_n$.
\end{enumerate}

Clustering in one partition is tied with clustering in the other. Two viewers can be classified as having a similar taste in watching movies not only when they have largely watched exactly the same movies, but also when the movies that they have watched largely belong to the same class of movies. And, two movies can be classified as belonging to the same class (or genre) if they have been watched by the same class of viewers. The clustering is usually fuzzy: a movie may belong to several genres; a viewer may like several different genres or movies that belong to a particular combination of genres. The following Biclustering algorithm reflects all these possibilities. 

The Biclustering algorithm starts by assigning each $a_m$ with a normalized random q-vector $\alpha_m$:
\begin{equation}
\begin{split}
\alpha_m = \left[ (\alpha_m^1,\alpha_m^1,\ldots,\alpha_m^q) \right]_{normalized} =  \\
(\alpha_m^1,\alpha_m^1,\ldots,\alpha_m^q) /
\left[ \sum_{i=1}^q (\alpha_m^i)^2 \right]^{1/2},
\end{split}
\end{equation}
where each $\alpha_m^i$ is a random number uniformly distributed in the interval $[-1,1]$. 

Next, the algorithm repeats several times the alternating pair of normalized-averaging steps:
\begin{equation}
\begin{split}
(1) \quad \beta_n = \left[ \sum_m w_{mn} \alpha_m / \sum_m w_{mn} \right]_{normalized}, \quad (n=1,\ldots,N) \\
(2) \quad \alpha_m = \left[ \sum_n w_{mn} \beta_n / \sum_n w_{mn} \right]_{normalized}, \quad (m=1,\ldots,M) 
\end{split}
\label{eqn:bistep}
\end{equation}

After such steps, two viewrs $a_m$ and $a_{m'}$ who watched nearly the same movies would clearly have small distance between them:
\begin{equation}
d_{mm'} := \|\alpha_m - \alpha_{m'} \| \ll 1\,.
\end{equation}

Moreover, $d_{mm'}$ will be small even if the movies they watch were quite different, as long as most of them have been watched by largely the same audience; and so on, back and forth between movies and viewers.

For efficiency, only a small number of normalized averaging steps (\ref{eqn:bistep}) should be made. The $d_{mm'}$ metric can thus only be used to identify small neighborhoods in each partition. In other words, it can be used for building in each partition many \underline{mini-clusters} of neighbors (using for efficiency iterative hierarchical k-means algorithm; see Appendix~\ref{sec:additional}). Larger clusters can then be constructed by clustering together neighboring mini-clusters based on a metric defined below. The new clusters can then be clustered to form the next level of still larger clusters. And so on, building a hierarchy of clusters.

\underline{Fuzzy hierarchical clustering.} Instead of the disjoint miniclusters one can use the $d_{mm'}$ metric to build in each partition \underline{fuzzy} mini-clusters, where each node has a \underline{probability} of belonging to each one of possible \underline{several} mini-clusters. The same can then be done at all levels of the hierarchical clustering. 

\underline{cluster metric.} To create a bipartite graph whose nodes are the mini-clusters, we need to define  the affinity (the weight of the link) between any mini-cluster $A_p$ in the first partition and any mini-cluster $B_q$ in the second partition. A natural definition is
\begin{equation}
W_{pq} = \frac{\sum_{n,m} P^a_{mp}P^b_{nq}{w_{mn}}^\eta}{ \sum_{n,m}P^a_{mp}P^b_{nq}}
\label{eqn:b2}
\end{equation}
where
\begin{equation}
P^a_{mp} = \prob \left[a_m \in A_p\right], \quad P^b_{nq} = \prob \left[ b_n \in B_q \right]
\end{equation}
and $\eta>0$ is a hyper-parameter that can be chosen higher or lower to emphasize or de-emphasize strong links.

As explained in \cite{RSB}, the bottom/up hierarchical way of constructing clusters is not just desired in various applications, and not just very efficient computationally, but can also create better clustering, since:

\begin{itemize}
\item
In most applications, graph distances can only accurately (or meaningfully) be defined between \underline{close} neighbors. Large-scale distances should be defined in terms of distances between clusters at some appropriate level. (Note however that for efficiency, at each level of the bottom/up clustering, our algorithm employs a top/down procedure as part of the iterative hierarchical k-means.)
\item
The clustering metric at high levels, should often take into account not just the lower-level metric (as in \eqref{eqn:b2}) but also some ``aggregative properties", i.e., cluster properties that are not at all visible (or definable) at the lower levels. These may modify $W_{pq}$ as well as introduce some \underline{intra}-partition links, creating at some levels \underline{generalized} bipartite graphs.
\end{itemize}

\underline{Generalized bipartite graph} is a bipartite graph that admits some intra-partition links, with weights $w^a_{mm'}$, linking $a_m$ and $a_{m'}$ and $w^b_{nn'}$, linking $b_n$ and $b_{n'}$. The biclustering algorithm remains the same as above, except that in the normalized-averaging algorithm \eqref{eqn:bistep}, step (1) is replaced by
\begin{equation}
\beta_n = \left[ \frac{\sum_m w_{mn} \alpha_m + \sum_{n'} w_{nn'}^b \bar{\beta_n}}
{\sum_m w_{mn} + \sum_{n'} w_{nn'}^b} \right]_{normalized} 
\end{equation}
where $\bar{\beta_n}$ is the value of $\beta_n$ in the previous iteration. Step (2) of \eqref{eqn:bistep} is similarly modified. In the first iteration $\bar{\beta_n}=0$ or random $(-1,1)$. Similarly to the 
\underline{inter}-partition averaging \eqref{eqn:b2}, \underline{intra}-partition weights at the higher level are then defined by averaging the corresponding intra-partition weights from the lower level.

\section{Sparse Linkage Clustering}
\label{sparse_linkage_clustering}
Given the set of \textbf{feature vectors} $E_1, \dots, E_n \in \Real^K$ and a distance metric $d(\cdot, \cdot)$, we would like to partition them into \textbf{small} neighborhoods, i.e., many clusters, each containing a few proximal objects. This task arises in building local neighborhoods of highly-correlated network parameters (Sec.~\ref{sec:neighborhoods}), as part of the coarsening construction. In that case $E_i=(E^1_i,\dots,E^K_i)$ is the vector of values of the $i${\it th} parameter in $K$ low-GER examples, and $d$ is defined by (\ref{dij}).

Our approach is a variant of single-linkage clustering optimized for our needs, which we describe below; see \cite{clustering_book} for a comprehensive survey of bottom-up agglomerative clustering, and in particular, standard single-linkage clustering.

The algorithm takes two inputs: the maximum cluster size $c > 1$, and distance threshold $\dmax$. Only vectors with distance $\leq \dmax$ are clustered together.

{\mbox{}}\\ \noindent
\textbf{Step A: Calculate the sparse distance matrix.} We calculate the \textbf{sparse} distance matrix $D$ containing only distances $\in (0, \dmax]$ (note that this excludes the diagonal). The naive way is to calculate the entire matrix and threshold it, which is fine for the activation-based coarsening of Sec.~\ref{sec:act-based-coarsening}, as the complexity is $O(n^2)$ where $n$ is the number of \text{activations}, but is still linear in the number of \textbf{parameters}. Furthermore, the calculation can be vectorized (as $d$ depends on dot products). More generally, quadratic complexity can be avoided by running a top-down multilevel $k$-means-like algorithm called \textbf{mini-clustering} (App.~\ref{sec:additional}), which requires only $O(n \log n)$ operations to find all strong connections. Once constructed, $D$ is stored in compressed-column format (matrix construction requires $O(n)$ with radix sort of the list of connections/non-zeros, although in practice quicksort is often faster). 

{\mbox{}}\\ \noindent
\textbf{Step B: Agglomerate.} This follows the single-linkage greedy process that clusters the closest pair at each step. The difference is that \textbf{$D$'s sparsity pattern is kept fixed}. Let $D(i, :)$ denote the $i${\it th} row of $D$ and $D(:, i)$ be the $i${\it th} column. 
\begin{enumerate}
	\item Initialize $n$ clusters $C_i$, each containing an individual object. Mark all $i=1,\dots,n$ as active. 
	\item Find the minimum distance between two clusters; suppose these are $C_i$, $C_j$ with $i \not = j$.
	\item While $d_{ij} \geq \dmax$, run steps 4-9.
	\item Set $C_i \leftarrow C_i \cup C_j$.
	\item Mark $j$ as inactive. Set the non-zero elements of the $j${\it th} row of $D$ to $\infty$.
	\item Set the non-zero elements of the $i${\it th} row and column of $D$ to $d(C_i, l)$ for all $l$ (such that $d_{il} \not = 0$).
	\item  Let $F := \left\{ l : |C_l| > c - |C_i| {\mbox{ or }} l {\mbox{ is inactive}} \right\}$. Set $d_{il} \leftarrow \infty$ for all $l \in F$.
	\item Set $D(:, i) \leftarrow D(i, :)^T, D(:, j) \leftarrow D(j, :)^T$ to keep $D$ symmetric.
	\item Find the new minimum distance $d_{ij}$ between two clusters $i$, $j$.
\end{enumerate}
Because $D$'s sparsity pattern is fixed, all operations are fast, as the CSR index and data arrays do not need to be resized or reallocated. The only non-contiguous operation is step $8$ (column updates), which can be executed quickly by also maintaining a compressed-column (CSC) format of $D$ and a lookup mapping between the CSC and CSR arrays. If $D$ has $m$ non-zeros per row, steps 4, 5, 7 and 8 require $O(m)$ operations.

In step 6, we calculate the distances between a cluster $C_i$ and points $l$. $d(C_i, l)$'s definition depends on the specific application. For our coarsening, let $E$ be the matrix whose columns are $E_1,\dots,E_n$. Let $E(:, C_i)$ be the sub-matrix comprising of the columns belonging to $C_i$. We define the cluster vector as the principal component of $E(:, C_i)$ (i.e., the right singular vector corresponding to the largest singular value), which serves as the new value of $E_i$. $d(C_i, j)$ is then conveniently defined again by (\ref{dij}). Calculating the SVD requires $O(|C_i| K^2 + K |C_i|^2)$, but the principal component can be well approximated by a few iterations of the method power method on $E(:, C_i)$, reducing the cost to $O(|C_i| K) \leq O(c K)$.

The total clustering complexity is bounded by $O((log n + (c K + m)(1-1/c)) n)$, as at most $n - n/c$ clustering steps are executed, compared with $O(n^2)$ required by standard single-clustering implementations (e.g., based on  Prim's algorithm \cite{clustering_book}).

\section{Additional Planned Sections}
\label{sec:additional}
\begin{itemize}
	\item Continuation: defining and obtaining the solution to the non-linear optimization problem as the limit of a chain of increasingly complex sub-problems.
	\item Mini-clustering: clustering $n$ vectors in a metric space into $O(n)$ coarse neighborhoods from in $O(n \log n)$ operations using a top-down multi-level algorithm. 
\end{itemize}

%\section{Continuation}
%\label{sec:continuation}
%
%section{Mini-Clustering}
%\label{sec:miniclustering}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Bibliography.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\bibliography{mlnn}
\end{document}
