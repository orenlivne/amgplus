\documentclass{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{authblk}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{booktabs}

\newcommand{\bbeta}{\boldsymbol\beta}
\newcommand{\bt}{\boldsymbol\tau}
\newcommand{\bta}{\boldsymbol\ta}
\newcommand{\bOmega}{\boldsymbol\Omega}
\newcommand{\bY}{\mathbf{Y}}
\newcommand{\cC}{\mathcal{C}}
\newcommand{\cG}{\mathcal{G}}
\newcommand{\st}{v_{\ta}}
\newcommand{\ta}{\theta}
\newcommand{\lla}{\longleftarrow}
\newcommand{\G}{\mathcal{G}}
\newcommand{\I}{\mathbb{I}}
\newcommand{\Normal}{\mathcal{N}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\bX}{\mathbf{X}}

\title{Systematic Upscaling for the 1D Helmholtz Equation}
\author{}

%\author[1]{Achi Brandt}
%\author[2]{Oren Livne}
%\affil[1]{The Weizmann Institute of Science,Department of Applied Mathematics \& Computer Science, 76100 Rehovot, Israel. Email: achibr@gmail.com}
%\affil[2]{Educational Testing Service, 660 Rosedale Road, Attn: MS-12, T-197, Princeton, NJ 08540. Email: olivne@ets.org}
%\author[3]{James Brannick} % Add affiliation
%\author[4]{Karsten Khal}} % Add affiliation
% In the ordering that makes sense.
\date{\today}

\begin{document}
\maketitle

\begin{abstract}
Multigrid solvers have been successful in solving numerous scientific and and engineering systems. Typically, a hierarchy of increasingly coarser levels are constructed from the original fine equation, such that each level reduces a certain range of error components, and combined in a multigrid cycle that has an optimal convergence rate. This approach, however, falls short in highly nonlinear cases and huge (yet repetitive) systems where even linear complexity is not good enough. Systematic upscaling is an extension of multilevel solvers to such cases: constructing coarse levels that accurately represent the full solution. Fine levels are rarely visited, and used only in selected small windows to improve the coarse level accuracy.

In this work we develop a systematic upscaling algorithm for the one-dimensional Helmholtz equation on a periodic domain. This model is an example where the coarse variables have a completely different character (rays) than fine-level variables (waves). The algorithm produces both near-null space functions, and a multilevel hierarchy of highly accurate coarse levels. The hierarchy and functions are iteratively improved together by bootstrap cycles, where the coarsening and interpolation are efficiently derived on local windows. Coarse levels employ the Full Approximation Scheme (FAS), and are then homogenized to obtain the full fine-level accuracy. Numerical results demonstrate the potential of the method for future applications, including the multi-dimensional Helmholtz equation, fluid flows and molecular dynamics.
\end{abstract}

\section{Introduction}
\label{intro}
Multigrid (MG) solvers have been very successful in solving a wide variety of scientific and engineering problems with optimal efficiency \cite{review}. Typically, a hierarchy of coarse levels is constructed from the original system, and combined in a multigrid cycle to obtain a fast iterative solver. There are two reasons for further generalizing these multigrid methods: (a) they cannot perform well for \emph{highly nonlinear cases}, where configurations cannot be decomposed into weakly-interacting local and non-local parts; and (b) for many systems, even attaining linear complexity is not good enough, since the number of variables is huge. Such systems on the other hand are typically \emph{highly repetitive}, in the sense that the same small set of governing equations (or Hamiltonian terms) keep repeating itself throughout the physical domain. Thus, the idea of \emph{systematic upscaling}  \cite{su} is to create coarse levels with  a small set of governing equations that are valid everywhere, and that can be derived from fine-level processing conducted only in some small representative “windows” (see below), so that fine levels are never or rarely visited.

In this work we implement systematic upscaling for the 1D Helmholtz equation on a periodic domain. The Helmholtz model is a simple yet instructive example where coarse-level variables have a completely different character (rays/geometric optics) than fine-level variables (waves/electromagnetics). By choosing a periodic domain, we factor out other difficulties that a numerical Helmholtz solver must treat (radiation boundary conditions are necessary; boundary effects; and diffraction effects of domain features of size comparable with the wavelength; phase error), to focus on the derivation of the interior-domain coarsening in the context of upscaling.

The Helmholtz operator has Almost Zero Modes (AZMs; i.e., near-null space components), which are not reduced by the fine-level relaxation. Unlike diffusion PDEs, these are not geometrically smooth functions. In 1D, they can be represented by two basis functions $\varphi_-$ and $\varphi_+$. That is, each AZM can be written as $A_- \varphi_+ A_+ \varphi_+$ where $A_{\pm}$ are smooth functions (``rays''). (The 2D problem introduces the further difficulty that AZMs are composed of waves in an infinite number of directions. Creating a multigrid solver that properly approximates AZMs and automatically detects the transition from waves to rays, has been a notoriously difficult task \cite{wave_ray, later_papers}.) Our upscaling algorithm finds both low-residual ``test functions'' that contain linear combinations of those AZMs, and a multilevel hierarchy that accurately represents them at all levels.

\subsection{Our Contribution}
We develop an upscaling algorithm finds low-residual test functions and an accurate multilevel hierarchy using a completely automatic Bootstrap Algebraic Multigrid (BAMG) procedure that does not exploit specific properties of the Helmholtz operator, such as the exponential form the AZMs, which is much simpler than a specifically tailored approach  \cite{wave_ray, later_papers}.

Furthermore, we focused on deriving general principles and rules so that the upscaling algorithm could be also applied to other problems, including nonlinear problems and systems of PDEs (such as the Navier Stokes equations). It can potentially be used for both a fast iterative solution, and coarse-graining in which the fine levels are never or rarely visited during the simulation.

Bootstrapping is an essential component in generating accurate coarse levels. We build on top of recent advances in multilevel methods (adaptive MG, BAMG \cite{bamg, lamg, mg_guide} and MG for neural networks \textbf{- pending publication, add citation}) to improve the various multigrid components beyond the framework outlined in \cite{su}:
\begin{itemize}
	\item Coarse variables (both their type and number) are automatically derived using a local Singular Value Decomposition (SVD), as opposed to expert knowledge (e.g., averages of fine variables in PDEs, or centers of mass in molecular dynamics). We do utilize the locality graph, though, by defining an aggregate as a small contiguous segment of gridpoints, although the approach can be generalized to non-geometric and non-local cases as well (cf. Sec.~\ref{Generalizations}).
	\item Interpolation is derived by least-square. We use a large set of samples and a validation set (as well as potentially regularization) to avoid over-fitting.
	\item Akin with the idea of local windows, the coarsening and interpolation are very efficiently derived on small aggregates, tiled to the entire domain. The domain is gradually grown as the hierarchy is expanded.
\end{itemize}

\section{Goal}
Given the Helhmholtz operator $A = \Delta + k^2 I$, find $M$ \emph{test functions}: $x_1\,dots,x_M$ of unit $l_2$ norm with small $\|A x\|$; i.e., near-null-space components, on a periodic domain, with constant $k$. 

Specifically, let the domain be $[0,L]$. The problem is discretized on a uniform grid with $n$ points (indexed $i = 0..n-1$; references to indices larger than $n - 1$ or negative are understood as periodic, i.e., $i \text{ mod } n$), and meshsize $h = L/n$. We use the 3-point finite difference discrertization stencil $A^h = [1, -2 + (kh)^2, 1]$. Given $k$, we set $h = 0.1/k$, so that $kh$ is small enough to resolve the wavelength and provide good local pointwise discretization accuracy. (We do not worry at this point about decreasing $h$ further to obtain pointwise accuracy on a large domain that does not have a phase error, as in such resolutions simple geometric coarsening can be employed. Thus they are irrelevant to the challenge of upscaling away from boundaries.) \emph{Our goal is to obtain discrete $M$ test vectors $x^h_1,\dots,x^h_M$ of increasingly smaller $\|A x\|/\|x\|$, as $L$ (and therefore $n$) is gradually increased.}

Our test functions are an approximation to the eigenvectors of $A$ corresponding to the $M$ smallest eigenvalues. We do not insist on finding the exact orthogonal basis here; we do know that for the 1D Helmholtz case, a small $M$ should be sufficient to represent the subspace of AZMs required for an accurate multilevel hierarchy. Furthermore,  \emph{multiple local windows} can be extracted from each test vector, so in fact we use just a single vector here ($M = 1$).

\section{Adaptive Multilevel Hierarchy}
The algorithm is a Bootstrap Algebraic Multigrid (BAMG) \cite{bamg} algorithm that takes advantage of the domain periodicity to reduce the cost of creating the coarsening and interpolation operators. It starts with a small $L$ and gradually doubles it. At the smallest domain, we start with relaxed vectors as our initial test vectors, construct a small coarse hierarchy, and improve the vectors using multigrid cycles. The hierarchy is then updated using the new vectors. Several such bootstrap iterations are executed. We then double $L$ and tile the test vectors over the larger domain, and both the vectors and multilevel hierarchy using bootstrap iterations. For each domain doubling, we add another coarse level to the hierarchy.

Since $A$ is indefinite, we use a robust smoother: Kaczmarz relaxation in lexicographic ordering. While for special cases (e.g. $k h = 0$), better smoothers are available (e.g., red-black Gauss-Seidel), Kaczmarz relaxation is guaranteed to smooth the error \cite{amg_theory}, so we keep the approach general.

\subsection{Initial Vectors}
Initial vectors are obtained by $\nu$ Kacmzarz relaxations on $A x = 0$, starting from $\text{rand}[-1, 1]$. $x$ is normalized to unit $l_2$ norm at the end. There is no need to normalize after each relaxation sweep, since we are solving a homogeneous equation (and anyway,the norm doesn't change much unless a huge number of relaxations is performed). 

{\bf What $\nu$ should we use to get a good enough initial guess for two-level boostrapping?}

\subsection{Coarse-level Construction}
We omit the $h$ superscripts in this section and use quantities without superscripts (e.g., $A$, $x$) to denote fine-level quantities and $c$-superscripted quantities (e.g., $A^c, x^c$) to denote coarse-level quantities.

\subsubsection{Coarsening}
Given a fixed-size discrete domain with $n$ and test functions $x_1,\dots,x_M$ on this domain, we derive the coarsening matrix $R_w: w_c \times w$ of an \emph{aggregate} (consisting of $w = 2-4$ consecutive gridpoints), and then tile (copy) these coarse variables to obtain an $n_c \times n$ coarsening matrix $R$, which maps a fine-level vector $x$ to its coarse representation $x^c$. That is, $R$ is the block diagonal matrix
\begin{equation}
	R := \text{diag} \left\{ \underbrace{R_w, R_w, \dots, ... R_w }_{\frac{n}{w}}  \right\} \,.
\end{equation}

$R_w$ depends on an accuracy threshold $\varepsilon$ and minimum coarsening ratio $0 < \rho < 1$. Start with $w = 2$. From each function we can derive \emph{many test samples}: $n$ windows of size $w$: $x^k_i = (x_{k,i},\dots,x_{k,i+w})$, a total of $Kw$ windows. We pick $m = 4 w$ windows, and compute the SVD decomposition $X = U \Sigma V^T$ of the $m \times w$ matrix $X$ whose rows are the windows, where $\Sigma = \text{diag}\left\{\sigma_1, \dots, \sigma_m \right\}$ and $\sigma_1 \geq \sigma_2 \geq \cdots$. Let $w_c$ be the smallest number such that
\begin{equation}
	\left(\frac{\sum_{j=n_c+1}^m \sigma_j^2}{\sum_{j=1}^m \sigma_j^2 } \right)^{\frac12} < \varepsilon.
	\label{r_accuracy}
\end{equation}
Then $R_w := V(1:w_c,:)$ is the sub-matrix of $V$ consisting of the first $w_c$ rows of $V$. (\ref{r_accuracy}) guarantees an $\varepsilon$-relative reconstruction error of $X$ in the Frobenius norm \cite{svd}.

If $w_c/w > \rho$, we double the window size $w$ and recalculate $R$ until a large enough window is reached such that $w_c/w \leq \rho$. If the relaxation method is a good smoother that quickly reduces the error (and thus our test functions, since we relax on $A x = 0$) into a lower-dimensional space, such a value of $w$ can be found.

The SVD yields coarse variables that are less sensitive to relaxation than pointwise coarsening (i.e., selecting one or several fine gridpoints as coarse variables, which corresponds to $R_w = [1,\underbrace{0,\dots,0}_{w-1}]$), as shown by comparing mock cycle rates Sec.~\ref{mock_cycle}. It also automatically selects the number of coarse variables $w_c$ per aggregate.

{\bf Add explanation on the particular nature of the coarse vars for the 1D Helmholtz: two waves, i.e., combination of sines and cosines. Add figure.}

\subsubsection{Least-squares Interpolation}
As for $R$, we derive an interpolation matrix $P_w$ on an aggregate of size $w$, and then tile it over the entire domain to obtain the full $P_{n \times n_c}$ interpolation matrix.

The \emph{interpolatory set} (the set of coarse variables used to interpolate to a target fine gridpoint) can be the set of coarse variables of its aggregate. In this case, the optimal interpolation that minimizes the interpolation error of the test functions is $P_w = R_w^T$ (as the truncated SVD is the best low-rank approximation in the Frobenius norm).

While this choice may be sufficient in some problems, even in the Laplace case ($kh=0$), $R_w^T$ is a piecewise-constant geometric interpolation (with the window size $w = 2, w_c = 1$, where only one principal component already explains most of the energy -- the constant vector), which only yields marginal multilevel efficiency (two-level, but but not multilevel $h$-independent convergence). Linear interpolation is required to obtain the full multilevel efficiency.

Thus, we increase $P_w$'s accuracy by including in the interpolatory set as many coarse variables as possible without increasing the coarse operator's density. A general rule is to use the union of all coarse variables in all aggregates of the fine gridpoints appearing in the target gridpoint's stencil.

The \emph{interpolation weights} are determined by least-square fitting to test samples.

{\bf include a description of the LS process: fitting set, fit with regularization parameter $\alpha$. Validation set: use to determine $\alpha$. $\alpha$ is global across all fine gridpoints in an aggregate. Test set: test the interpolation accuracy on samples we haven't used to ensure no overfitting.
}

\begin{equation}
	\min_p \sum_m \left( x_{m,i} - \sum_j p_{ij} x_{m,j} \right)^2 + \alpha \left( \sum_m (x_{m,i})^2 \right) \left(\sum_j p_{ij}^2\right) 
\end{equation}

\subsection{Multilevel Bootstrapping}

\subsection{Domain Growing}
{\bf When do we grow the domain? Do we add more levels on a fixed-size domain of size $n$ gridpoints until we get to $O(\log n)$ levels and a coarsest grid with only a few points, before growing the domain, so that we get a full multilevel cycle that reduces the error by a fixed factor in $O(n)$ operations; or, do we start with level 1 on $n$ points, create level $2$ with $n/2$ points, improve the hierarchy with $2$-level cycles, then consider level $2$ on $n$ points as our finest level, and coarsen it using $2$-level cycles, and so on, i.e., recursive homogenization with $2$ level-cycles only, but increasingly larger domains?}

\section{Coarse-level Homogenization}
The FAS coarse-level equations depend on the fine level only through the fine-to-coarse $\tau$ correction:
\begin{equation}
	A^c x^c = P^T b + \tau\,,\qquad \tau :=  A^c \overline{x}^c - P^T A x = (A^c R - P^T A) x\,,
	\label{fas}
\end{equation}
where $\overline{x}^c := R x$ is the restricted fine-level approximate solution (and initial coarse level approximation). Thus, $\tau_I$ depends on a local neighborhood of fine-level values near the coarse gridpoint $I$, and thus in turn on neighboring coarse-level values $x^c_J$. We can thus attempt to \emph{homogenize} (\ref{fas}) to only depend on $x^c$  by fitting a rule $\tau \approx Q x^c$ with some interpolation matrix $Q$. $Q$ can be found using a least-squares fit, just like for the BAMG interpolation $P$, namely,
\begin{equation}
	\min_p \sum_m \left( \tau_{m,I} - \sum_j q_{IJ} x^c_{m,J} \right)^2 + \alpha \left( \sum_m (\tau_{mI})^2 \right) \left(\sum_j q_{IJ}^2\right) \,.
\end{equation}

\section{Numerical Results}
We test two discretizations of the Helmholtz operator $A = -\Delta + k^2 I$: a $3$-point second order stencil $A^h = [1, -2 + (kh)^2, 1]$, and a $5$-point $4th$ order finite-differences $A^h = [-1, 16, -30 + 12 (kh)^2, 16, -1]$. The $5$-point stencil avoids any properties particular to 1D tridiagonal systems that lead to easier coarsening (e.g., as in total reduction). Since this is a homogeneous equation on a periodic domain, we use normalized stencils without the $h^{-2}$ scale factor.

\subsection{Quantitative Performance Prediction}

\subsection{Smoothing Factor}
The Kaczmarz smoothing factor is $0.75$ for this problem. {\bf TODO: add explanation of how we calculated it. Can do it by running Fourier components through Kaczmarz to numerically calculate its amplification matrix, or via a generalized eigenvalue problem that James and Karsten employ.}

\subsection{Mock Cycle}
\label{mock_cycle}
To gauge the quality of the coarse variable set, we compared the mock cycle convergence factors with SVD coarsening, $1:2$ pointwise coarsening ($R_w = [1, 0]$) and averaging coarsening with $1:2$ coarsening ratio ($R_w = [1, 1]$). for different $kh$ values and $\nu$ relaxations per cycle. Kaczmarz relaxation was used in all cases; for $kh=0$ only, we also tested Gauss-Seidel relaxation for comparison. The SVD coarsening was based on test vectors obtained by $100$ relaxations of $\text{random}[-1,1]$ starting vectors, a $w=4$-point aggregate to calculate the SVD, and accuracy threshold of  $\varepsilon=0.1$. Two principal components were selected as coarse variables for all $kh$, so all three coarsening schemes have coarsening ratio $\rho = 0.5$.
 
\begin{table}
{\small
\begin{tabular}{l|cccc|cccc|cccc}
\toprule
& \multicolumn{4}{c|}{SVD Coarsening} & \multicolumn{4}{|c|}{Pointwise Coarsening} & \multicolumn{4}{|c}{Averaging Coarsening} \\
$kh$ &  $\nu=1$ &  $2$ &  $3$ &  $4$ &  $1$ &  $2$ &  $3$ &  $4$ &  $1$ &  $2$ &  $3$ &  $4$ \\
\midrule
GS &       0.30 &       0.11 &       0.07 &       0.02 &      0.65 &      0.60 &      0.57 &      0.54 &       0.32 &       0.12 &       0.07 &       0.02 \\
0.0   &       0.42 &       0.22 &       0.15 &       0.12 &      0.49 &      0.55 &      0.49 &      0.50 &       0.45 &       0.21 &       0.19 &       0.15 \\
0.1   &       0.46 &       0.22 &       0.15 &       0.13 &      0.50 &      0.55 &      0.48 &      0.48 &       0.42 &       0.20 &       0.19 &       0.15 \\
0.2   &       0.45 &       0.24 &       0.15 &       0.12 &      0.51 &      0.58 &      0.50 &      0.48 &       0.45 &       0.21 &       0.19 &       0.16 \\
0.3   &       0.46 &       0.23 &       0.15 &       0.12 &      0.49 &      0.56 &      0.50 &      0.49 &       0.42 &       0.20 &       0.20 &       0.15 \\
0.4   &       0.49 &       0.24 &       0.16 &       0.12 &      0.50 &      0.58 &      0.53 &      0.49 &       0.49 &       0.20 &       0.20 &       0.18 \\
0.5   &       0.51 &       0.25 &       0.16 &       0.13 &      0.53 &      0.55 &      0.49 &      0.47 &       0.47 &       0.22 &       0.21 &       0.20 \\
0.6   &       0.52 &       0.30 &       0.13 &       0.14 &      0.56 &      0.52 &      0.53 &      0.48 &       0.52 &       0.22 &       0.20 &       0.21 \\
0.7   &       0.57 &       0.25 &       0.15 &       0.13 &      0.61 &      0.51 &      0.53 &      0.46 &       0.54 &       0.25 &       0.22 &       0.24 \\
0.8   &       0.60 &       0.30 &       0.15 &       0.11 &      0.64 &      0.44 &      0.50 &      0.52 &       0.49 &       0.24 &       0.23 &       0.26 \\
0.9   &       0.63 &       0.35 &       0.17 &       0.15 &      0.67 &      0.39 &      0.49 &      0.51 &       0.63 &       0.34 &       0.20 &       0.27 \\
1.0   &       0.60 &       0.29 &       0.14 &       0.10 &      0.74 &      0.46 &      0.43 &      0.50 &       0.69 &       0.48 &       0.25 &       0.25 \\
\bottomrule
\end{tabular}
}
\caption{Mock cycle convergence factors for a grid of size $n=32$. The first row corresponds to the Laplace operator and Gauss-Seidel. All other rows refer to Kaczmarz relaxation. $\nu$ is the number of relaxations per cycle.}
\end{table}

The pointwise coarsening convergence factors is bounded by $0.5$ as the error function $1,0,1,\dots$ is reduced only by a meager $0.5$ factor by relaxation. In practice, a multigrid cycle with pointwise coarsening converges faster. Generally, mock cycle predictions can be pessimistic, and cases where their prediction is quantitatively accurate should be identified based on numerical experimentation. Here, it is more accurate when the coarse variables are averages. As expected, the SVD coarsening is equivalent to averaging for the Laplace case, but trumps simple averaging for $kh = O(1)$, which is substantiates its use for upscaling. The convergence improves with $\nu$ up to $3$ relaxation sweeps.

Finally, in line with geometric multigrid theory, Gauss-Seidel is a better smoother for the Laplace case, but Kaczmarz relaxation provides adequate convergence as well in that case, while its efficiency is maintained for all values of $0 \leq kh \leq 1$. All results are independent of $n$, for $n \geq 16$.

\subsection{2-level}
{\bf 
\begin{itemize}
\item Start with $n=96$ at level 1. 
\item Show 2-level rates of solving $A x = 0$ for $n=96$ as a function of number of bootstrap cycles. Test function residuals.
\item  Then show convergence for increasingly larger $n$ with the same coarsening.
\item How do the SVD components look like.
\item Performance vs. window size. Number of components.
\end{itemize}
}

\subsection{3-level}

\subsection{Wave to Ray Transition}
{\bf Show limit of stencil coefficients of coarse equations after many coarsening levels and afer few. Is it elliptic / Ray?}

\subsection{$\tau$-Homogenization}

\section{Future Directions}
\begin{itemize}
	\item The multi-dimensional Helmholtz equation.
	\item Non-linear problems (e.g., fluid flows): use FAS; fine-level windows for different coarse solution values.
	\item Molecular dynamics: $\tau$ homogenization may lead to discovering different physical laws at different scales.
	\item Non-geometric and non-local problems (e.g., NNs): use correlations instead of locality graph to define aggregates.
\end{itemize}

\bibliographystyle{plain}
\bibliography{mg.bib}

\end{document}